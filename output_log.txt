Starting training with different noise multipliers...
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
[34m[1mtrain_dp: [0mweights=yolov5s.pt, cfg=, data=/mnt/bst/hxu10/hxu10/chanti/dataset/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, device=6, multi_scale=False, single_cls=False, optimizer=SGD, workers=8, project=runs/train/train_dp, name=noise_0.1, exist_ok=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, dp=True, noise_multiplier=0.1, max_grad_norm=5.0, delta=1e-05
[34m[1mgithub: [0mup to date with https://github.com/ultralytics/yolov5 ✅
YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.5 torch-2.6.0+cu124 CUDA:6 (NVIDIA A100-SXM4-80GB, 81154MiB)

[34m[1mhyperparameters: [0mlr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0005, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
Overriding model.yaml nc=80 with nc=3

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs

Transferred 342/349 layers from yolov5s.pt
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
[34m[1mAMP: [0mchecks passed ✅
[34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s][34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s]
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
Privacy Engine attached: Noise Multiplier=0.1, Max Grad Norm=5.0
/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=amp and not opt.dp)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to [1mruns/train/train_dp/noise_0.13[0m
Starting training for 100 epochs...
✅ Model is now private: False
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1095 0.03723 0.03838 62 640:   0%|          | 0/19 [00:06<?, ?it/s]0/99 7.18G 0.1095 0.03723 0.03838 62 640:   5%|▌         | 1/19 [00:06<02:04,  6.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1088 0.03829 0.03809 61 640:   5%|▌         | 1/19 [00:07<02:04,  6.90s/it]0/99 7.18G 0.1088 0.03829 0.03809 61 640:  11%|█         | 2/19 [00:07<00:50,  3.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1092 0.03638 0.03857 52 640:  11%|█         | 2/19 [00:07<00:50,  3.00s/it]0/99 7.18G 0.1092 0.03638 0.03857 52 640:  16%|█▌        | 3/19 [00:07<00:29,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1088 0.03681 0.03856 61 640:  16%|█▌        | 3/19 [00:08<00:29,  1.86s/it]0/99 7.18G 0.1088 0.03681 0.03856 61 640:  21%|██        | 4/19 [00:08<00:18,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1085 0.03743 0.03852 66 640:  21%|██        | 4/19 [00:08<00:18,  1.27s/it]0/99 7.18G 0.1085 0.03743 0.03852 66 640:  26%|██▋       | 5/19 [00:08<00:12,  1.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1087 0.04251 0.03868 175 640:  26%|██▋       | 5/19 [00:09<00:12,  1.11it/s]0/99 7.18G 0.1087 0.04251 0.03868 175 640:  32%|███▏      | 6/19 [00:09<00:13,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1085 0.04185 0.03868 69 640:  32%|███▏      | 6/19 [00:09<00:13,  1.03s/it] 0/99 7.18G 0.1085 0.04185 0.03868 69 640:  37%|███▋      | 7/19 [00:09<00:09,  1.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1087 0.04495 0.03872 148 640:  37%|███▋      | 7/19 [00:10<00:09,  1.24it/s]0/99 7.18G 0.1087 0.04495 0.03872 148 640:  42%|████▏     | 8/19 [00:10<00:07,  1.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1086 0.04511 0.03871 88 640:  42%|████▏     | 8/19 [00:10<00:07,  1.51it/s] 0/99 7.18G 0.1086 0.04511 0.03871 88 640:  47%|████▋     | 9/19 [00:10<00:05,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1084 0.04377 0.03865 45 640:  47%|████▋     | 9/19 [00:10<00:05,  1.78it/s]0/99 7.18G 0.1084 0.04377 0.03865 45 640:  53%|█████▎    | 10/19 [00:10<00:04,  2.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1079 0.04322 0.03862 63 640:  53%|█████▎    | 10/19 [00:11<00:04,  2.02it/s]0/99 7.18G 0.1079 0.04322 0.03862 63 640:  58%|█████▊    | 11/19 [00:11<00:03,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1079 0.04461 0.03868 134 640:  58%|█████▊    | 11/19 [00:11<00:03,  2.18it/s]0/99 7.18G 0.1079 0.04461 0.03868 134 640:  63%|██████▎   | 12/19 [00:11<00:03,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1076 0.04403 0.0386 58 640:  63%|██████▎   | 12/19 [00:12<00:03,  2.32it/s]  0/99 7.18G 0.1076 0.04403 0.0386 58 640:  68%|██████▊   | 13/19 [00:12<00:02,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1073 0.04353 0.03849 59 640:  68%|██████▊   | 13/19 [00:12<00:02,  2.43it/s]0/99 7.18G 0.1073 0.04353 0.03849 59 640:  74%|███████▎  | 14/19 [00:12<00:01,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.107 0.04334 0.03838 67 640:  74%|███████▎  | 14/19 [00:12<00:01,  2.50it/s] 0/99 7.18G 0.107 0.04334 0.03838 67 640:  79%|███████▉  | 15/19 [00:12<00:01,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1069 0.04392 0.03832 107 640:  79%|███████▉  | 15/19 [00:13<00:01,  2.56it/s]0/99 7.18G 0.1069 0.04392 0.03832 107 640:  84%|████████▍ | 16/19 [00:13<00:01,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1066 0.04381 0.03831 71 640:  84%|████████▍ | 16/19 [00:13<00:01,  2.61it/s] 0/99 7.18G 0.1066 0.04381 0.03831 71 640:  89%|████████▉ | 17/19 [00:13<00:00,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1064 0.04342 0.03829 61 640:  89%|████████▉ | 17/19 [00:14<00:00,  2.44it/s]0/99 7.18G 0.1064 0.04342 0.03829 61 640:  95%|█████████▍| 18/19 [00:14<00:00,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1062 0.04336 0.03824 72 640:  95%|█████████▍| 18/19 [00:14<00:00,  2.50it/s]0/99 7.18G 0.1062 0.04336 0.03824 72 640: 100%|██████████| 19/19 [00:14<00:00,  2.57it/s]0/99 7.18G 0.1062 0.04336 0.03824 72 640: 100%|██████████| 19/19 [00:14<00:00,  1.32it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.38s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
                   all         55        256    0.00168      0.194    0.00153   0.000462
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1029 0.04419 0.03699 74 640:   0%|          | 0/19 [00:00<?, ?it/s]1/99 7.2G 0.1029 0.04419 0.03699 74 640:   5%|▌         | 1/19 [00:00<00:14,  1.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1039 0.04516 0.0373 87 640:   5%|▌         | 1/19 [00:01<00:14,  1.20it/s] 1/99 7.2G 0.1039 0.04516 0.0373 87 640:  11%|█         | 2/19 [00:01<00:09,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1039 0.04443 0.0372 89 640:  11%|█         | 2/19 [00:01<00:09,  1.82it/s]1/99 7.2G 0.1039 0.04443 0.0372 89 640:  16%|█▌        | 3/19 [00:01<00:07,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1031 0.04261 0.03714 65 640:  16%|█▌        | 3/19 [00:02<00:07,  2.16it/s]1/99 7.2G 0.1031 0.04261 0.03714 65 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1028 0.04325 0.03707 85 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]1/99 7.2G 0.1028 0.04325 0.03707 85 640:  26%|██▋       | 5/19 [00:02<00:07,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1028 0.04226 0.03704 66 640:  26%|██▋       | 5/19 [00:03<00:07,  2.00it/s]1/99 7.2G 0.1028 0.04226 0.03704 66 640:  32%|███▏      | 6/19 [00:03<00:06,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1028 0.04162 0.03699 63 640:  32%|███▏      | 6/19 [00:03<00:06,  1.97it/s]1/99 7.2G 0.1028 0.04162 0.03699 63 640:  37%|███▋      | 7/19 [00:03<00:05,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1023 0.04099 0.03682 55 640:  37%|███▋      | 7/19 [00:03<00:05,  2.18it/s]1/99 7.2G 0.1023 0.04099 0.03682 55 640:  42%|████▏     | 8/19 [00:03<00:04,  2.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1019 0.04126 0.03687 73 640:  42%|████▏     | 8/19 [00:04<00:04,  2.34it/s]1/99 7.2G 0.1019 0.04126 0.03687 73 640:  47%|████▋     | 9/19 [00:04<00:04,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1021 0.04386 0.03691 143 640:  47%|████▋     | 9/19 [00:04<00:04,  2.47it/s]1/99 7.2G 0.1021 0.04386 0.03691 143 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1019 0.04258 0.03685 43 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.57it/s] 1/99 7.2G 0.1019 0.04258 0.03685 43 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1023 0.04526 0.03682 181 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.64it/s]1/99 7.2G 0.1023 0.04526 0.03682 181 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.102 0.04585 0.03683 95 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.15it/s]  1/99 7.2G 0.102 0.04585 0.03683 95 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1019 0.04623 0.03675 92 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.65it/s]1/99 7.2G 0.1019 0.04623 0.03675 92 640:  74%|███████▎  | 14/19 [00:05<00:01,  4.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1015 0.0459 0.03674 70 640:  74%|███████▎  | 14/19 [00:05<00:01,  4.10it/s] 1/99 7.2G 0.1015 0.0459 0.03674 70 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1014 0.04544 0.03667 63 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.49it/s]1/99 7.2G 0.1014 0.04544 0.03667 63 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1011 0.04522 0.03663 66 640:  84%|████████▍ | 16/19 [00:06<00:00,  4.55it/s]1/99 7.2G 0.1011 0.04522 0.03663 66 640:  89%|████████▉ | 17/19 [00:06<00:00,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1009 0.04478 0.03651 58 640:  89%|████████▉ | 17/19 [00:06<00:00,  3.86it/s]1/99 7.2G 0.1009 0.04478 0.03651 58 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1006 0.04446 0.03644 60 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.50it/s]1/99 7.2G 0.1006 0.04446 0.03644 60 640: 100%|██████████| 19/19 [00:06<00:00,  3.19it/s]1/99 7.2G 0.1006 0.04446 0.03644 60 640: 100%|██████████| 19/19 [00:06<00:00,  2.76it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.40s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]
                   all         55        256    0.00198      0.289    0.00245    0.00053
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1069 0.05349 0.03674 144 640:   0%|          | 0/19 [00:01<?, ?it/s]2/99 7.21G 0.1069 0.05349 0.03674 144 640:   5%|▌         | 1/19 [00:01<00:20,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1022 0.05451 0.03596 107 640:   5%|▌         | 1/19 [00:01<00:20,  1.11s/it]2/99 7.21G 0.1022 0.05451 0.03596 107 640:  11%|█         | 2/19 [00:01<00:12,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1001 0.05002 0.03581 66 640:  11%|█         | 2/19 [00:01<00:12,  1.33it/s] 2/99 7.21G 0.1001 0.05002 0.03581 66 640:  16%|█▌        | 3/19 [00:01<00:07,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09863 0.04808 0.0355 68 640:  16%|█▌        | 3/19 [00:01<00:07,  2.05it/s]2/99 7.21G 0.09863 0.04808 0.0355 68 640:  21%|██        | 4/19 [00:01<00:05,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09818 0.04792 0.03544 76 640:  21%|██        | 4/19 [00:02<00:05,  2.75it/s]2/99 7.21G 0.09818 0.04792 0.03544 76 640:  26%|██▋       | 5/19 [00:02<00:05,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09771 0.04624 0.03525 61 640:  26%|██▋       | 5/19 [00:02<00:05,  2.77it/s]2/99 7.21G 0.09771 0.04624 0.03525 61 640:  32%|███▏      | 6/19 [00:02<00:04,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09797 0.04735 0.03526 106 640:  32%|███▏      | 6/19 [00:02<00:04,  2.96it/s]2/99 7.21G 0.09797 0.04735 0.03526 106 640:  37%|███▋      | 7/19 [00:02<00:04,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09758 0.04728 0.03518 79 640:  37%|███▋      | 7/19 [00:03<00:04,  2.93it/s] 2/99 7.21G 0.09758 0.04728 0.03518 79 640:  42%|████▏     | 8/19 [00:03<00:03,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.0973 0.04673 0.0352 69 640:  42%|████▏     | 8/19 [00:03<00:03,  3.15it/s]  2/99 7.21G 0.0973 0.04673 0.0352 69 640:  47%|████▋     | 9/19 [00:03<00:02,  3.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09695 0.0456 0.03517 55 640:  47%|████▋     | 9/19 [00:03<00:02,  3.67it/s]2/99 7.21G 0.09695 0.0456 0.03517 55 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09674 0.04555 0.03509 76 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.14it/s]2/99 7.22G 0.09674 0.04555 0.03509 76 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09641 0.04381 0.035 35 640:  58%|█████▊    | 11/19 [00:04<00:01,  4.35it/s]  2/99 7.22G 0.09641 0.04381 0.035 35 640:  63%|██████▎   | 12/19 [00:04<00:01,  3.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09621 0.04324 0.03499 55 640:  63%|██████▎   | 12/19 [00:04<00:01,  3.72it/s]2/99 7.22G 0.09621 0.04324 0.03499 55 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09628 0.04429 0.03498 101 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.41it/s]2/99 7.22G 0.09628 0.04429 0.03498 101 640:  74%|███████▎  | 14/19 [00:04<00:01,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09591 0.04393 0.03493 58 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.22it/s] 2/99 7.22G 0.09591 0.04393 0.03493 58 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09565 0.04446 0.03486 78 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.11it/s]2/99 7.22G 0.09565 0.04446 0.03486 78 640:  84%|████████▍ | 16/19 [00:05<00:00,  3.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09604 0.04655 0.03485 194 640:  84%|████████▍ | 16/19 [00:05<00:00,  3.03it/s]2/99 7.22G 0.09604 0.04655 0.03485 194 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09588 0.04701 0.03483 86 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.98it/s] 2/99 7.22G 0.09588 0.04701 0.03483 86 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09601 0.04701 0.03481 82 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.90it/s]2/99 7.22G 0.09601 0.04701 0.03481 82 640: 100%|██████████| 19/19 [00:06<00:00,  2.82it/s]2/99 7.22G 0.09601 0.04701 0.03481 82 640: 100%|██████████| 19/19 [00:06<00:00,  2.87it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it]
                   all         55        256    0.00452       0.34     0.0032   0.000718
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09316 0.0401 0.03279 59 640:   0%|          | 0/19 [00:02<?, ?it/s]3/99 7.23G 0.09316 0.0401 0.03279 59 640:   5%|▌         | 1/19 [00:02<00:38,  2.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09258 0.04268 0.03389 73 640:   5%|▌         | 1/19 [00:02<00:38,  2.11s/it]3/99 7.23G 0.09258 0.04268 0.03389 73 640:  11%|█         | 2/19 [00:02<00:18,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09369 0.04347 0.03391 84 640:  11%|█         | 2/19 [00:02<00:18,  1.08s/it]3/99 7.23G 0.09369 0.04347 0.03391 84 640:  16%|█▌        | 3/19 [00:02<00:11,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09424 0.04462 0.03412 75 640:  16%|█▌        | 3/19 [00:02<00:11,  1.37it/s]3/99 7.23G 0.09424 0.04462 0.03412 75 640:  21%|██        | 4/19 [00:02<00:07,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09387 0.04445 0.034 66 640:  21%|██        | 4/19 [00:03<00:07,  1.93it/s]  3/99 7.23G 0.09387 0.04445 0.034 66 640:  26%|██▋       | 5/19 [00:03<00:05,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.0934 0.04287 0.03389 58 640:  26%|██▋       | 5/19 [00:03<00:05,  2.52it/s]3/99 7.23G 0.0934 0.04287 0.03389 58 640:  32%|███▏      | 6/19 [00:03<00:04,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09421 0.0455 0.03389 119 640:  32%|███▏      | 6/19 [00:03<00:04,  2.88it/s]3/99 7.23G 0.09421 0.0455 0.03389 119 640:  37%|███▋      | 7/19 [00:03<00:04,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09364 0.04539 0.03377 67 640:  37%|███▋      | 7/19 [00:04<00:04,  2.92it/s]3/99 7.23G 0.09364 0.04539 0.03377 67 640:  42%|████▏     | 8/19 [00:04<00:03,  2.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09369 0.0459 0.03388 80 640:  42%|████▏     | 8/19 [00:04<00:03,  2.99it/s] 3/99 7.23G 0.09369 0.0459 0.03388 80 640:  47%|████▋     | 9/19 [00:04<00:03,  3.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09342 0.0454 0.03389 59 640:  47%|████▋     | 9/19 [00:04<00:03,  3.01it/s]3/99 7.23G 0.09342 0.0454 0.03389 59 640:  53%|█████▎    | 10/19 [00:04<00:02,  3.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09347 0.04561 0.03392 91 640:  53%|█████▎    | 10/19 [00:04<00:02,  3.53it/s]3/99 7.23G 0.09347 0.04561 0.03392 91 640:  58%|█████▊    | 11/19 [00:04<00:02,  4.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.0935 0.04659 0.03386 93 640:  58%|█████▊    | 11/19 [00:04<00:02,  4.00it/s] 3/99 7.23G 0.0935 0.04659 0.03386 93 640:  63%|██████▎   | 12/19 [00:04<00:01,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09304 0.04579 0.03376 50 640:  63%|██████▎   | 12/19 [00:05<00:01,  4.37it/s]3/99 7.23G 0.09304 0.04579 0.03376 50 640:  68%|██████▊   | 13/19 [00:05<00:01,  4.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09285 0.04594 0.03373 75 640:  68%|██████▊   | 13/19 [00:05<00:01,  4.52it/s]3/99 7.23G 0.09285 0.04594 0.03373 75 640:  74%|███████▎  | 14/19 [00:05<00:01,  4.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09275 0.04648 0.03369 85 640:  74%|███████▎  | 14/19 [00:05<00:01,  4.60it/s]3/99 7.23G 0.09275 0.04648 0.03369 85 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09285 0.04675 0.03363 88 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.84it/s]3/99 7.23G 0.09285 0.04675 0.03363 88 640:  84%|████████▍ | 16/19 [00:05<00:00,  5.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09203 0.04611 0.03353 45 640:  84%|████████▍ | 16/19 [00:05<00:00,  5.08it/s]3/99 7.23G 0.09203 0.04611 0.03353 45 640:  89%|████████▉ | 17/19 [00:05<00:00,  5.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09207 0.04578 0.03355 66 640:  89%|████████▉ | 17/19 [00:06<00:00,  5.27it/s]3/99 7.23G 0.09207 0.04578 0.03355 66 640:  95%|█████████▍| 18/19 [00:06<00:00,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.092 0.04611 0.03357 83 640:  95%|█████████▍| 18/19 [00:06<00:00,  4.34it/s]  3/99 7.23G 0.092 0.04611 0.03357 83 640: 100%|██████████| 19/19 [00:06<00:00,  2.86it/s]3/99 7.23G 0.092 0.04611 0.03357 83 640: 100%|██████████| 19/19 [00:06<00:00,  2.79it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
                   all         55        256    0.00978      0.338    0.00731    0.00197
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09105 0.04384 0.03153 69 640:   0%|          | 0/19 [00:01<?, ?it/s]4/99 7.25G 0.09105 0.04384 0.03153 69 640:   5%|▌         | 1/19 [00:01<00:23,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08924 0.04329 0.03282 65 640:   5%|▌         | 1/19 [00:01<00:23,  1.31s/it]4/99 7.25G 0.08924 0.04329 0.03282 65 640:  11%|█         | 2/19 [00:01<00:12,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08891 0.04551 0.03315 77 640:  11%|█         | 2/19 [00:02<00:12,  1.34it/s]4/99 7.25G 0.08891 0.04551 0.03315 77 640:  16%|█▌        | 3/19 [00:02<00:09,  1.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08875 0.04227 0.03308 47 640:  16%|█▌        | 3/19 [00:02<00:09,  1.77it/s]4/99 7.25G 0.08875 0.04227 0.03308 47 640:  21%|██        | 4/19 [00:02<00:07,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08835 0.04228 0.03318 57 640:  21%|██        | 4/19 [00:02<00:07,  2.08it/s]4/99 7.25G 0.08835 0.04228 0.03318 57 640:  26%|██▋       | 5/19 [00:02<00:06,  2.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08763 0.04072 0.03282 46 640:  26%|██▋       | 5/19 [00:03<00:06,  2.31it/s]4/99 7.25G 0.08763 0.04072 0.03282 46 640:  32%|███▏      | 6/19 [00:03<00:05,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08853 0.04291 0.03287 102 640:  32%|███▏      | 6/19 [00:03<00:05,  2.47it/s]4/99 7.25G 0.08853 0.04291 0.03287 102 640:  37%|███▋      | 7/19 [00:03<00:04,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08814 0.04295 0.03279 65 640:  37%|███▋      | 7/19 [00:03<00:04,  2.58it/s] 4/99 7.25G 0.08814 0.04295 0.03279 65 640:  42%|████▏     | 8/19 [00:03<00:04,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08833 0.04539 0.03289 99 640:  42%|████▏     | 8/19 [00:04<00:04,  2.60it/s]4/99 7.25G 0.08833 0.04539 0.03289 99 640:  47%|████▋     | 9/19 [00:04<00:03,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08875 0.04557 0.03283 74 640:  47%|████▋     | 9/19 [00:04<00:03,  2.62it/s]4/99 7.25G 0.08875 0.04557 0.03283 74 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08887 0.04649 0.03277 87 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.65it/s]4/99 7.25G 0.08887 0.04649 0.03277 87 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08905 0.04816 0.03269 108 640:  58%|█████▊    | 11/19 [00:05<00:02,  2.68it/s]4/99 7.25G 0.08905 0.04816 0.03269 108 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08912 0.04782 0.03275 73 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.69it/s] 4/99 7.25G 0.08912 0.04782 0.03275 73 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08902 0.04677 0.03286 50 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.88it/s]4/99 7.25G 0.08902 0.04677 0.03286 50 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08951 0.04846 0.03282 146 640:  74%|███████▎  | 14/19 [00:06<00:01,  3.16it/s]4/99 7.25G 0.08951 0.04846 0.03282 146 640:  79%|███████▉  | 15/19 [00:06<00:01,  3.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08979 0.04871 0.03285 96 640:  79%|███████▉  | 15/19 [00:06<00:01,  3.03it/s] 4/99 7.25G 0.08979 0.04871 0.03285 96 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08997 0.05012 0.03283 121 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.98it/s]4/99 7.25G 0.08997 0.05012 0.03283 121 640:  89%|████████▉ | 17/19 [00:06<00:00,  3.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08981 0.05012 0.03283 75 640:  89%|████████▉ | 17/19 [00:06<00:00,  3.36it/s] 4/99 7.25G 0.08981 0.05012 0.03283 75 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08961 0.05051 0.03275 94 640:  95%|█████████▍| 18/19 [00:07<00:00,  3.70it/s]4/99 7.25G 0.08961 0.05051 0.03275 94 640: 100%|██████████| 19/19 [00:07<00:00,  3.97it/s]4/99 7.25G 0.08961 0.05051 0.03275 94 640: 100%|██████████| 19/19 [00:07<00:00,  2.66it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
                   all         55        256     0.0085      0.357     0.0096    0.00227
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08725 0.04036 0.03155 55 640:   0%|          | 0/19 [00:00<?, ?it/s]5/99 7.25G 0.08725 0.04036 0.03155 55 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08563 0.04217 0.0315 62 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s] 5/99 7.25G 0.08563 0.04217 0.0315 62 640:  11%|█         | 2/19 [00:00<00:05,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08946 0.04625 0.03183 131 640:  11%|█         | 2/19 [00:01<00:05,  2.84it/s]5/99 7.25G 0.08946 0.04625 0.03183 131 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08936 0.04774 0.03217 89 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s] 5/99 7.25G 0.08936 0.04774 0.03217 89 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08903 0.04478 0.032 48 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]  5/99 7.25G 0.08903 0.04478 0.032 48 640:  26%|██▋       | 5/19 [00:01<00:04,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08894 0.0454 0.03182 73 640:  26%|██▋       | 5/19 [00:02<00:04,  2.84it/s]5/99 7.25G 0.08894 0.0454 0.03182 73 640:  32%|███▏      | 6/19 [00:02<00:04,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09069 0.05093 0.03195 190 640:  32%|███▏      | 6/19 [00:02<00:04,  2.84it/s]5/99 7.25G 0.09069 0.05093 0.03195 190 640:  37%|███▋      | 7/19 [00:02<00:04,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09145 0.05135 0.03215 100 640:  37%|███▋      | 7/19 [00:02<00:04,  2.84it/s]5/99 7.25G 0.09145 0.05135 0.03215 100 640:  42%|████▏     | 8/19 [00:02<00:03,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09114 0.05095 0.03209 72 640:  42%|████▏     | 8/19 [00:03<00:03,  2.83it/s] 5/99 7.25G 0.09114 0.05095 0.03209 72 640:  47%|████▋     | 9/19 [00:03<00:03,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09108 0.04923 0.03218 56 640:  47%|████▋     | 9/19 [00:03<00:03,  2.84it/s]5/99 7.25G 0.09108 0.04923 0.03218 56 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09085 0.04893 0.03234 66 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.76it/s]5/99 7.25G 0.09085 0.04893 0.03234 66 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09067 0.04858 0.03225 68 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.74it/s]5/99 7.25G 0.09067 0.04858 0.03225 68 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09119 0.04984 0.03229 123 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.74it/s]5/99 7.25G 0.09119 0.04984 0.03229 123 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09121 0.05049 0.03212 101 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.74it/s]5/99 7.25G 0.09121 0.05049 0.03212 101 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09074 0.04983 0.0321 56 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.74it/s]  5/99 7.25G 0.09074 0.04983 0.0321 56 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09041 0.0496 0.03208 66 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.74it/s]5/99 7.25G 0.09041 0.0496 0.03208 66 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.0902 0.0502 0.03201 96 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.74it/s] 5/99 7.25G 0.0902 0.0502 0.03201 96 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09053 0.05089 0.03204 126 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.95it/s]5/99 7.25G 0.09053 0.05089 0.03204 126 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09045 0.05096 0.03203 87 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.30it/s] 5/99 7.25G 0.09045 0.05096 0.03203 87 640: 100%|██████████| 19/19 [00:06<00:00,  3.63it/s]5/99 7.25G 0.09045 0.05096 0.03203 87 640: 100%|██████████| 19/19 [00:06<00:00,  2.94it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.45s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.63s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]
                   all         55        256    0.00598      0.402    0.00866    0.00207
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09481 0.04836 0.03142 96 640:   0%|          | 0/19 [00:00<?, ?it/s]6/99 7.25G 0.09481 0.04836 0.03142 96 640:   5%|▌         | 1/19 [00:00<00:03,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09364 0.04814 0.03103 83 640:   5%|▌         | 1/19 [00:00<00:03,  5.31it/s]6/99 7.25G 0.09364 0.04814 0.03103 83 640:  11%|█         | 2/19 [00:00<00:03,  5.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.0925 0.05051 0.03147 90 640:  11%|█         | 2/19 [00:00<00:03,  5.54it/s] 6/99 7.25G 0.0925 0.05051 0.03147 90 640:  16%|█▌        | 3/19 [00:00<00:03,  4.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08917 0.0479 0.03165 53 640:  16%|█▌        | 3/19 [00:01<00:03,  4.65it/s]6/99 7.25G 0.08917 0.0479 0.03165 53 640:  21%|██        | 4/19 [00:01<00:04,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08906 0.04689 0.03169 67 640:  21%|██        | 4/19 [00:01<00:04,  3.38it/s]6/99 7.25G 0.08906 0.04689 0.03169 67 640:  26%|██▋       | 5/19 [00:01<00:05,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.0897 0.05284 0.03167 145 640:  26%|██▋       | 5/19 [00:02<00:05,  2.66it/s]6/99 7.25G 0.0897 0.05284 0.03167 145 640:  32%|███▏      | 6/19 [00:02<00:05,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08883 0.05174 0.03158 63 640:  32%|███▏      | 6/19 [00:02<00:05,  2.35it/s]6/99 7.25G 0.08883 0.05174 0.03158 63 640:  37%|███▋      | 7/19 [00:02<00:05,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08928 0.05328 0.03156 123 640:  37%|███▋      | 7/19 [00:02<00:05,  2.32it/s]6/99 7.25G 0.08928 0.05328 0.03156 123 640:  42%|████▏     | 8/19 [00:02<00:04,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08913 0.05388 0.03157 98 640:  42%|████▏     | 8/19 [00:03<00:04,  2.47it/s] 6/99 7.25G 0.08913 0.05388 0.03157 98 640:  47%|████▋     | 9/19 [00:03<00:03,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08938 0.05303 0.03162 73 640:  47%|████▋     | 9/19 [00:03<00:03,  2.58it/s]6/99 7.25G 0.08938 0.05303 0.03162 73 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08943 0.05304 0.03146 87 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.66it/s]6/99 7.25G 0.08943 0.05304 0.03146 87 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08907 0.05358 0.03137 97 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.72it/s]6/99 7.25G 0.08907 0.05358 0.03137 97 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08835 0.05227 0.03134 48 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.76it/s]6/99 7.25G 0.08835 0.05227 0.03134 48 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08839 0.05361 0.03128 114 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.80it/s]6/99 7.25G 0.08839 0.05361 0.03128 114 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08824 0.05382 0.03124 89 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.82it/s] 6/99 7.25G 0.08824 0.05382 0.03124 89 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08812 0.0531 0.03109 61 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.84it/s] 6/99 7.25G 0.08812 0.0531 0.03109 61 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08774 0.05222 0.03106 50 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.79it/s]6/99 7.25G 0.08774 0.05222 0.03106 50 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08764 0.0527 0.03105 90 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.76it/s] 6/99 7.25G 0.08764 0.0527 0.03105 90 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08769 0.05305 0.03106 89 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.75it/s]6/99 7.25G 0.08769 0.05305 0.03106 89 640: 100%|██████████| 19/19 [00:06<00:00,  2.75it/s]6/99 7.25G 0.08769 0.05305 0.03106 89 640: 100%|██████████| 19/19 [00:06<00:00,  2.80it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.57s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]
                   all         55        256     0.0046      0.411    0.00  0%|           0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08696 0.04284 0.03032 69 640:   0%|          | 0/19 [00:00<?, ?it/s]7/99 7.25G 0.08696 0.04284 0.03032 69 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0886 0.0549 0.03016 124 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s] 7/99 7.25G 0.0886 0.0549 0.03016 124 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08763 0.05863 0.03002 101 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]7/99 7.25G 0.08763 0.05863 0.03002 101 640:  16%|█▌        | 3/19 [00:00<00:02,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08725 0.0558 0.02998 72 640:  16%|█▌        | 3/19 [00:00<00:02,  5.50it/s]  7/99 7.25G 0.08725 0.0558 0.02998 72 640:  21%|██        | 4/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08859 0.06446 0.03006 187 640:  21%|██        | 4/19 [00:00<00:02,  5.45it/s]7/99 7.25G 0.08859 0.06446 0.03006 187 640:  26%|██▋       | 5/19 [00:00<00:02,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08729 0.05899 0.02982 46 640:  26%|██▋       | 5/19 [00:01<00:02,  5.31it/s] 7/99 7.25G 0.08729 0.05899 0.02982 46 640:  32%|███▏      | 6/19 [00:01<00:02,  4.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0873 0.05823 0.02982 85 640:  32%|███▏      | 6/19 [00:01<00:02,  4.50it/s] 7/99 7.25G 0.0873 0.05823 0.02982 85 640:  37%|███▋      | 7/19 [00:01<00:03,  3.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08759 0.05554 0.02956 59 640:  37%|███▋      | 7/19 [00:01<00:03,  3.73it/s]7/99 7.25G 0.08759 0.05554 0.02956 59 640:  42%|████▏     | 8/19 [00:01<00:03,  3.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08829 0.05706 0.02941 143 640:  42%|████▏     | 8/19 [00:02<00:03,  3.33it/s]7/99 7.25G 0.08829 0.05706 0.02941 143 640:  47%|████▋     | 9/19 [00:02<00:03,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08815 0.0542 0.02934 43 640:  47%|████▋     | 9/19 [00:02<00:03,  3.19it/s]  7/99 7.25G 0.08815 0.0542 0.02934 43 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08712 0.05226 0.02915 43 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.98it/s]7/99 7.25G 0.08712 0.05226 0.02915 43 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08663 0.05127 0.02909 56 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.55it/s]7/99 7.25G 0.08663 0.05127 0.02909 56 640:  63%|██████▎   | 12/19 [00:03<00:03,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08691 0.05342 0.0291 132 640:  63%|██████▎   | 12/19 [00:04<00:03,  2.33it/s]7/99 7.25G 0.08691 0.05342 0.0291 132 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08701 0.05264 0.02912 67 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.25it/s]7/99 7.25G 0.08701 0.05264 0.02912 67 640:  74%|███████▎  | 14/19 [00:04<00:02,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08696 0.05283 0.02923 84 640:  74%|███████▎  | 14/19 [00:04<00:02,  2.42it/s]7/99 7.25G 0.08696 0.05283 0.02923 84 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08674 0.05238 0.0294 64 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.54it/s] 7/99 7.25G 0.08674 0.05238 0.0294 64 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08704 0.05271 0.02935 113 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.63it/s]7/99 7.25G 0.08704 0.05271 0.02935 113 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08636 0.0515 0.02939 38 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.12it/s]  7/99 7.25G 0.08636 0.0515 0.02939 38 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08584 0.05123 0.02927 59 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.30it/s]7/99 7.25G 0.08584 0.05123 0.02927 59 640: 100%|██████████| 19/19 [00:06<00:00,  2.07it/s]7/99 7.25G 0.08584 0.05123 0.02927 59 640: 100%|██████████| 19/19 [00:06<00:00,  2.77it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00                             Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.48s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it]
                   all         55        256     0.0054      0.483     0.013  0%|         0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08705 0.06261 0.03002 101 640:   0%|          | 0/19 [00:00<?, ?it/s]8/99 7.25G 0.08705 0.06261 0.03002 101 640:   5%|▌         | 1/19 [00:00<00:03,  4.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.083 0.04624 0.02966 38 640:   5%|▌         | 1/19 [00:00<00:03,  4.79it/s]   8/99 7.25G 0.083 0.04624 0.02966 38 640:  11%|█         | 2/19 [00:00<00:05,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08312 0.04459 0.02971 58 640:  11%|█         | 2/19 [00:00<00:05,  3.39it/s]8/99 7.25G 0.08312 0.04459 0.02971 58 640:  16%|█▌        | 3/19 [00:00<00:04,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08261 0.04278 0.02973 53 640:  16%|█▌        | 3/19 [00:01<00:04,  3.22it/s]8/99 7.25G 0.08261 0.04278 0.02973 53 640:  21%|██        | 4/19 [00:01<00:05,  2.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08191 0.04263 0.02956 59 640:  21%|██        | 4/19 [00:01<00:05,  2.99it/s]8/99 7.25G 0.08191 0.04263 0.02956 59 640:  26%|██▋       | 5/19 [00:01<00:04,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08482 0.04571 0.02944 146 640:  26%|██▋       | 5/19 [00:01<00:04,  2.93it/s]8/99 7.25G 0.08482 0.04571 0.02944 146 640:  32%|███▏      | 6/19 [00:01<00:04,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08525 0.04595 0.02964 76 640:  32%|███▏      | 6/19 [00:02<00:04,  2.88it/s] 8/99 7.25G 0.08525 0.04595 0.02964 76 640:  37%|███▋      | 7/19 [00:02<00:05,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.0851 0.04533 0.02957 66 640:  37%|███▋      | 7/19 [00:03<00:05,  2.35it/s] 8/99 7.25G 0.0851 0.04533 0.02957 66 640:  42%|████▏     | 8/19 [00:03<00:05,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08573 0.05293 0.02955 209 640:  42%|████▏     | 8/19 [00:03<00:05,  1.95it/s]8/99 7.25G 0.08573 0.05293 0.02955 209 640:  47%|████▋     | 9/19 [00:03<00:05,  1.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08498 0.05082 0.02928 45 640:  47%|████▋     | 9/19 [00:04<00:05,  1.90it/s] 8/99 7.25G 0.08498 0.05082 0.02928 45 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.0846 0.05045 0.02916 68 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.99it/s] 8/99 7.25G 0.0846 0.05045 0.02916 68 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08518 0.05222 0.02913 153 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.95it/s]8/99 7.25G 0.08518 0.05222 0.02913 153 640:  63%|██████▎   | 12/19 [00:05<00:03,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08514 0.05139 0.02928 68 640:  63%|██████▎   | 12/19 [00:05<00:03,  1.94it/s] 8/99 7.25G 0.08514 0.05139 0.02928 68 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08522 0.05188 0.02924 88 640:  68%|██████▊   | 13/19 [00:06<00:02,  2.03it/s]8/99 7.25G 0.08522 0.05188 0.02924 88 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08541 0.0513 0.0293 69 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.18it/s]  8/99 7.25G 0.08541 0.0513 0.0293 69 640:  79%|███████▉  | 15/19 [00:06<00:01,  2.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08531 0.05114 0.02919 76 640:  79%|███████▉  | 15/19 [00:07<00:01,  2.02it/s]8/99 7.25G 0.08531 0.05114 0.02919 76 640:  84%|████████▍ | 16/19 [00:07<00:01,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08568 0.05153 0.0292 111 640:  84%|████████▍ | 16/19 [00:08<00:01,  1.98it/s]8/99 7.25G 0.08568 0.05153 0.0292 111 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08571 0.05115 0.02909 69 640:  89%|████████▉ | 17/19 [00:09<00:01,  1.41it/s]8/99 7.25G 0.08571 0.05115 0.02909 69 640:  95%|█████████▍| 18/19 [00:09<00:00,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08536 0.05009 0.02895 41 640:  95%|█████████▍| 18/19 [00:09<00:00,  1.38it/s]8/99 7.25G 0.08536 0.05009 0.02895 41 640: 100%|██████████| 19/19 [00:09<00:00,  1.64it/s]8/99 7.25G 0.08536 0.05009 0.02895 41 640: 100%|██████████| 19/19 [00:09<00:00,  1.99it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2                                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.08s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
                   all         55        256    0.00574      0.493     0.  0%|            0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08421 0.04805 0.02922 75 640:   0%|          | 0/19 [00:02<?, ?it/s]9/99 7.26G 0.08421 0.04805 0.02922 75 640:   5%|▌         | 1/19 [00:02<00:51,  2.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08421 0.06273 0.02976 121 640:   5%|▌         | 1/19 [00:03<00:51,  2.88s/it]9/99 7.26G 0.08421 0.06273 0.02976 121 640:  11%|█         | 2/19 [00:03<00:27,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08458 0.05833 0.02913 80 640:  11%|█         | 2/19 [00:04<00:27,  1.59s/it] 9/99 7.26G 0.08458 0.05833 0.02913 80 640:  16%|█▌        | 3/19 [00:04<00:18,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.084 0.05397 0.0291 58 640:  16%|█▌        | 3/19 [00:04<00:18,  1.16s/it]   9/99 7.26G 0.084 0.05397 0.0291 58 640:  21%|██        | 4/19 [00:04<00:12,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08531 0.05014 0.02933 58 640:  21%|██        | 4/19 [00:04<00:12,  1.19it/s]9/99 7.26G 0.08531 0.05014 0.02933 58 640:  26%|██▋       | 5/19 [00:04<00:09,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08341 0.04964 0.02912 62 640:  26%|██▋       | 5/19 [00:05<00:09,  1.50it/s]9/99 7.26G 0.08341 0.04964 0.02912 62 640:  32%|███▏      | 6/19 [00:05<00:08,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08403 0.05097 0.02866 100 640:  32%|███▏      | 6/19 [00:05<00:08,  1.62it/s]9/99 7.26G 0.08403 0.05097 0.02866 100 640:  37%|███▋      | 7/19 [00:05<00:06,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08462 0.05081 0.02849 99 640:  37%|███▋      | 7/19 [00:06<00:06,  1.89it/s] 9/99 7.26G 0.08462 0.05081 0.02849 99 640:  42%|████▏     | 8/19 [00:06<00:05,  2.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08384 0.04933 0.02882 53 640:  42%|████▏     | 8/19 [00:06<00:05,  2.09it/s]9/99 7.26G 0.08384 0.04933 0.02882 53 640:  47%|████▋     | 9/19 [00:06<00:04,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08309 0.04749 0.02892 42 640:  47%|████▋     | 9/19 [00:07<00:04,  2.16it/s]9/99 7.26G 0.08309 0.04749 0.02892 42 640:  53%|█████▎    | 10/19 [00:07<00:04,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08311 0.04645 0.02907 50 640:  53%|█████▎    | 10/19 [00:07<00:04,  2.03it/s]9/99 7.26G 0.08311 0.04645 0.02907 50 640:  58%|█████▊    | 11/19 [00:07<00:03,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08259 0.04702 0.02929 72 640:  58%|█████▊    | 11/19 [00:08<00:03,  2.14it/s]9/99 7.26G 0.08259 0.04702 0.02929 72 640:  63%|██████▎   | 12/19 [00:08<00:03,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08242 0.04892 0.02956 108 640:  63%|██████▎   | 12/19 [00:08<00:03,  2.14it/s]9/99 7.26G 0.08242 0.04892 0.02956 108 640:  68%|██████▊   | 13/19 [00:08<00:03,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08306 0.04876 0.0296 74 640:  68%|██████▊   | 13/19 [00:09<00:03,  1.98it/s]  9/99 7.26G 0.08306 0.04876 0.0296 74 640:  74%|███████▎  | 14/19 [00:09<00:02,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08337 0.04968 0.02942 105 640:  74%|███████▎  | 14/19 [00:09<00:02,  1.84it/s]9/99 7.26G 0.08337 0.04968 0.02942 105 640:  79%|███████▉  | 15/19 [00:09<00:02,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08366 0.04949 0.02921 81 640:  79%|███████▉  | 15/19 [00:10<00:02,  1.84it/s] 9/99 7.26G 0.08366 0.04949 0.02921 81 640:  84%|████████▍ | 16/19 [00:10<00:01,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08337 0.04885 0.02903 54 640:  84%|████████▍ | 16/19 [00:12<00:01,  1.64it/s]9/99 7.26G 0.08337 0.04885 0.02903 54 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08345 0.04822 0.02896 58 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.14s/it]9/99 7.26G 0.08345 0.04822 0.02896 58 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08352 0.0481 0.02883 70 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.39s/it] 9/99 7.26G 0.08352 0.0481 0.02883 70 640: 100%|██████████| 19/19 [00:18<00:00,  1.90s/it]9/99 7.26G 0.08352 0.0481 0.02883 70 640: 100%|██████████| 19/19 [00:18<00:00,  1.06it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|                        Class                   Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.43s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.54s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
                   all         55        256    0.00575      0.5  0%|          | 0/19 [0  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08099 0.0498 0.02723 65 640:   0%|          | 0/19 [00:00<?, ?it/s]10/99 7.28G 0.08099 0.0498 0.02723 65 640:   5%|▌         | 1/19 [00:00<00:12,  1.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08142 0.04312 0.02624 53 640:   5%|▌         | 1/19 [00:00<00:12,  1.47it/s]10/99 7.28G 0.08142 0.04312 0.02624 53 640:  11%|█         | 2/19 [00:00<00:07,  2.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08174 0.03996 0.02694 51 640:  11%|█         | 2/19 [00:01<00:07,  2.19it/s]10/99 7.28G 0.08174 0.03996 0.02694 51 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.07919 0.04113 0.02763 55 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s]10/99 7.28G 0.07919 0.04113 0.02763 55 640:  21%|██        | 4/19 [00:01<00:04,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.07981 0.04211 0.02776 74 640:  21%|██        | 4/19 [00:01<00:04,  3.43it/s]10/99 7.28G 0.07981 0.04211 0.02776 74 640:  26%|██▋       | 5/19 [00:01<00:03,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08137 0.04256 0.02793 78 640:  26%|██▋       | 5/19 [00:01<00:03,  3.97it/s]10/99 7.28G 0.08137 0.04256 0.02793 78 640:  32%|███▏      | 6/19 [00:01<00:02,  4.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08179 0.04346 0.02781 76 640:  32%|███▏      | 6/19 [00:02<00:02,  4.43it/s]10/99 7.28G 0.08179 0.04346 0.02781 76 640:  37%|███▋      | 7/19 [00:02<00:03,  3.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08127 0.04198 0.02788 43 640:  37%|███▋      | 7/19 [00:02<00:03,  3.73it/s]10/99 7.28G 0.08127 0.04198 0.02788 43 640:  42%|████▏     | 8/19 [00:02<00:02,  4.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08141 0.04185 0.02835 61 640:  42%|████▏     | 8/19 [00:03<00:02,  4.20it/s]10/99 7.28G 0.08141 0.04185 0.02835 61 640:  47%|████▋     | 9/19 [00:03<00:05,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08171 0.04189 0.02824 63 640:  47%|████▋     | 9/19 [00:03<00:05,  1.99it/s]10/99 7.28G 0.08171 0.04189 0.02824 63 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08209 0.04193 0.02819 58 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.14it/s]10/99 7.28G 0.08209 0.04193 0.02819 58 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08298 0.04355 0.02802 148 640:  58%|█████▊    | 11/19 [00:07<00:03,  2.33it/s]10/99 7.28G 0.08298 0.04355 0.02802 148 640:  63%|██████▎   | 12/19 [00:07<00:10,  1.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08234 0.04372 0.02815 61 640:  63%|██████▎   | 12/19 [00:09<00:10,  1.43s/it] 10/99 7.28G 0.08234 0.04372 0.02815 61 640:  68%|██████▊   | 13/19 [00:09<00:09,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08278 0.04433 0.02809 94 640:  68%|██████▊   | 13/19 [00:11<00:09,  1.58s/it]10/99 7.28G 0.08278 0.04433 0.02809 94 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08259 0.04324 0.02807 38 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.59s/it]10/99 7.28G 0.08259 0.04324 0.02807 38 640:  79%|███████▉  | 15/19 [00:11<00:05,  1.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08233 0.04284 0.02827 52 640:  79%|███████▉  | 15/19 [00:14<00:05,  1.26s/it]10/99 7.28G 0.08233 0.04284 0.02827 52 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08199 0.04236 0.0284 46 640:  84%|████████▍ | 16/19 [00:18<00:05,  1.72s/it] 10/99 7.28G 0.08199 0.04236 0.0284 46 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08229 0.04291 0.02834 92 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.33s/it]10/99 7.28G 0.08229 0.04291 0.02834 92 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08249 0.04387 0.02834 107 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.72s/it]10/99 7.28G 0.08249 0.04387 0.02834 107 640: 100%|██████████| 19/19 [00:19<00:00,  1.49s/it]10/99 7.28G 0.08249 0.04387 0.02834 107 640: 100%|██████████| 19/19 [00:19<00:00,  1.03s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.81s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.04s/it]
                   all         55        256    0.00609      0.518     0.0256    0.00653
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.07532 0.03984 0.0287 52 640:   0%|          | 0/19 [00:00<?, ?it/s]11/99 7.28G 0.07532 0.03984 0.0287 52 640:   5%|▌         | 1/19 [00:00<00:09,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.07561 0.03772 0.02754 48 640:   5%|▌         | 1/19 [00:01<00:09,  1.98it/s]11/99 7.28G 0.07561 0.03772 0.02754 48 640:  11%|█         | 2/19 [00:01<00:11,  1.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.07854 0.04524 0.02784 92 640:  11%|█         | 2/19 [00:02<00:11,  1.49it/s]11/99 7.28G 0.07854 0.04524 0.02784 92 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.07981 0.04102 0.02806 41 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s]11/99 7.28G 0.07981 0.04102 0.02806 41 640:  21%|██        | 4/19 [00:02<00:10,  1.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08062 0.04144 0.02854 66 640:  21%|██        | 4/19 [00:03<00:10,  1.43it/s]11/99 7.28G 0.08062 0.04144 0.02854 66 640:  26%|██▋       | 5/19 [00:03<00:08,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08085 0.04387 0.02807 91 640:  26%|██▋       | 5/19 [00:03<00:08,  1.61it/s]11/99 7.28G 0.08085 0.04387 0.02807 91 640:  32%|███▏      | 6/19 [00:03<00:06,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08118 0.04668 0.0284 95 640:  32%|███▏      | 6/19 [00:04<00:06,  1.86it/s] 11/99 7.28G 0.08118 0.04668 0.0284 95 640:  37%|███▋      | 7/19 [00:04<00:06,  1.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08159 0.04872 0.02815 106 640:  37%|███▋      | 7/19 [00:04<00:06,  1.88it/s]11/99 7.28G 0.08159 0.04872 0.02815 106 640:  42%|████▏     | 8/19 [00:04<00:05,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08162 0.04882 0.02799 73 640:  42%|████▏     | 8/19 [00:04<00:05,  2.11it/s] 11/99 7.28G 0.08162 0.04882 0.02799 73 640:  47%|████▋     | 9/19 [00:04<00:04,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08147 0.04899 0.02786 71 640:  47%|████▋     | 9/19 [00:04<00:04,  2.42it/s]11/99 7.28G 0.08147 0.04899 0.02786 71 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.0816 0.05054 0.02768 101 640:  53%|█████▎    | 10/19 [00:09<00:03,  2.92it/s]11/99 7.28G 0.0816 0.05054 0.02768 101 640:  58%|█████▊    | 11/19 [00:09<00:12,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08145 0.04936 0.02795 49 640:  58%|█████▊    | 11/19 [00:09<00:12,  1.59s/it]11/99 7.28G 0.08145 0.04936 0.02795 49 640:  63%|██████▎   | 12/19 [00:09<00:09,  1.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08153 0.0497 0.02801 77 640:  63%|██████▎   | 12/19 [00:14<00:09,  1.30s/it] 11/99 7.28G 0.08153 0.0497 0.02801 77 640:  68%|██████▊   | 13/19 [00:14<00:13,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08152 0.04984 0.02782 79 640:  68%|██████▊   | 13/19 [00:20<00:13,  2.33s/it]11/99 7.28G 0.08152 0.04984 0.02782 79 640:  74%|███████▎  | 14/19 [00:20<00:16,  3.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08159 0.04849 0.02755 42 640:  74%|███████▎  | 14/19 [00:20<00:16,  3.29s/it]11/99 7.28G 0.08159 0.04849 0.02755 42 640:  79%|███████▉  | 15/19 [00:20<00:09,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08164 0.04884 0.02744 91 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.45s/it]11/99 7.28G 0.08164 0.04884 0.02744 91 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08181 0.04897 0.02753 79 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.39s/it]11/99 7.28G 0.08181 0.04897 0.02753 79 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08144 0.04931 0.02755 80 640:  89%|████████▉ | 17/19 [00:26<00:04,  2.41s/it]11/99 7.28G 0.08144 0.04931 0.02755 80 640:  95%|█████████▍| 18/19 [00:26<00:01,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08151 0.04902 0.02744 75 640:  95%|█████████▍| 18/19 [00:31<00:01,  1.96s/it]11/99 7.28G 0.08151 0.04902 0.02744 75 640: 100%|██████████| 19/19 [00:31<00:00,  2.90s/it]11/99 7.28G 0.08151 0.04902 0.02744 75 640: 100%|██████████| 19/19 [00:31<00:00,  1.65s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.47s/it]
                   all         55        256    0.00771      0.576     0.0334    0.00847
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08283 0.05829 0.0269 100 640:   0%|          | 0/19 [00:00<?, ?it/s]12/99 7.28G 0.08283 0.05829 0.0269 100 640:   5%|▌         | 1/19 [00:00<00:04,  3.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08154 0.05355 0.02733 74 640:   5%|▌         | 1/19 [00:00<00:04,  3.63it/s]12/99 7.28G 0.08154 0.05355 0.02733 74 640:  11%|█         | 2/19 [00:00<00:03,  4.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08027 0.05406 0.0279 76 640:  11%|█         | 2/19 [00:00<00:03,  4.62it/s] 12/99 7.28G 0.08027 0.05406 0.0279 76 640:  16%|█▌        | 3/19 [00:00<00:03,  5.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.0802 0.05048 0.02721 58 640:  16%|█▌        | 3/19 [00:00<00:03,  5.06it/s]12/99 7.28G 0.0802 0.05048 0.02721 58 640:  21%|██        | 4/19 [00:00<00:02,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08017 0.0502 0.0266 75 640:  21%|██        | 4/19 [00:00<00:02,  5.31it/s] 12/99 7.28G 0.08017 0.0502 0.0266 75 640:  26%|██▋       | 5/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.07953 0.04934 0.02731 60 640:  26%|██▋       | 5/19 [00:01<00:02,  5.45it/s]12/99 7.28G 0.07953 0.04934 0.02731 60 640:  32%|███▏      | 6/19 [00:01<00:02,  4.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08032 0.05083 0.02729 93 640:  32%|███▏      | 6/19 [00:01<00:02,  4.92it/s]12/99 7.28G 0.08032 0.05083 0.02729 93 640:  37%|███▋      | 7/19 [00:01<00:03,  3.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08191 0.05242 0.0275 124 640:  37%|███▋      | 7/19 [00:02<00:03,  3.28it/s]12/99 7.28G 0.08191 0.05242 0.0275 124 640:  42%|████▏     | 8/19 [00:02<00:03,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08074 0.05021 0.02804 41 640:  42%|████▏     | 8/19 [00:02<00:03,  3.15it/s]12/99 7.28G 0.08074 0.05021 0.02804 41 640:  47%|████▋     | 9/19 [00:02<00:03,  3.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08075 0.0514 0.02773 93 640:  47%|████▋     | 9/19 [00:05<00:03,  3.06it/s] 12/99 7.28G 0.08075 0.0514 0.02773 93 640:  53%|█████▎    | 10/19 [00:05<00:11,  1.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08128 0.05447 0.02769 159 640:  53%|█████▎    | 10/19 [00:11<00:11,  1.26s/it]12/99 7.28G 0.08128 0.05447 0.02769 159 640:  58%|█████▊    | 11/19 [00:11<00:21,  2.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08185 0.05492 0.0276 105 640:  58%|█████▊    | 11/19 [00:12<00:21,  2.64s/it] 12/99 7.28G 0.08185 0.05492 0.0276 105 640:  63%|██████▎   | 12/19 [00:12<00:14,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08144 0.05511 0.02769 81 640:  63%|██████▎   | 12/19 [00:12<00:14,  2.00s/it]12/99 7.28G 0.08144 0.05511 0.02769 81 640:  68%|██████▊   | 13/19 [00:12<00:09,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08111 0.05413 0.02731 61 640:  68%|██████▊   | 13/19 [00:15<00:09,  1.58s/it]12/99 7.28G 0.08111 0.05413 0.02731 61 640:  74%|███████▎  | 14/19 [00:15<00:10,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08158 0.05555 0.02704 164 640:  74%|███████▎  | 14/19 [00:16<00:10,  2.00s/it]12/99 7.28G 0.08158 0.05555 0.02704 164 640:  79%|███████▉  | 15/19 [00:16<00:06,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08141 0.055 0.02706 68 640:  79%|███████▉  | 15/19 [00:23<00:06,  1.50s/it]   12/99 7.28G 0.08141 0.055 0.02706 68 640:  84%|████████▍ | 16/19 [00:23<00:10,  3.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08111 0.05444 0.02717 66 640:  84%|████████▍ | 16/19 [00:24<00:10,  3.45s/it]12/99 7.28G 0.08111 0.05444 0.02717 66 640:  89%|████████▉ | 17/19 [00:24<00:05,  2.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.0808 0.05366 0.02715 54 640:  89%|████████▉ | 17/19 [00:29<00:05,  2.53s/it] 12/99 7.28G 0.0808 0.05366 0.02715 54 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08077 0.05412 0.02707 94 640:  95%|█████████▍| 18/19 [00:37<00:03,  3.25s/it]12/99 7.28G 0.08077 0.05412 0.02707 94 640: 100%|██████████| 19/19 [00:37<00:00,  4.60s/it]12/99 7.28G 0.08077 0.05412 0.02707 94 640: 100%|██████████| 19/19 [00:37<00:00,  1.95s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.79s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]
                   all         55        256    0.00828      0.614     0.0433     0.0118
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08384 0.04722 0.03094 76 640:   0%|          | 0/19 [00:00<?, ?it/s]13/99 7.28G 0.08384 0.04722 0.03094 76 640:   5%|▌         | 1/19 [00:00<00:04,  4.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08423 0.05678 0.02868 113 640:   5%|▌         | 1/19 [00:00<00:04,  4.07it/s]13/99 7.28G 0.08423 0.05678 0.02868 113 640:  11%|█         | 2/19 [00:00<00:05,  3.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08428 0.05865 0.02818 111 640:  11%|█         | 2/19 [00:01<00:05,  3.36it/s]13/99 7.28G 0.08428 0.05865 0.02818 111 640:  16%|█▌        | 3/19 [00:01<00:06,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08247 0.05282 0.0271 51 640:  16%|█▌        | 3/19 [00:01<00:06,  2.35it/s]  13/99 7.28G 0.08247 0.05282 0.0271 51 640:  21%|██        | 4/19 [00:01<00:05,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.082 0.0524 0.02707 74 640:  21%|██        | 4/19 [00:01<00:05,  2.52it/s]  13/99 7.28G 0.082 0.0524 0.02707 74 640:  26%|██▋       | 5/19 [00:01<00:05,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08086 0.05047 0.02666 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.63it/s]13/99 7.28G 0.08086 0.05047 0.02666 56 640:  32%|███▏      | 6/19 [00:02<00:04,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08154 0.04997 0.02696 75 640:  32%|███▏      | 6/19 [00:02<00:04,  2.70it/s]13/99 7.28G 0.08154 0.04997 0.02696 75 640:  37%|███▋      | 7/19 [00:02<00:04,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08132 0.04801 0.0278 47 640:  37%|███▋      | 7/19 [00:03<00:04,  2.50it/s] 13/99 7.28G 0.08132 0.04801 0.0278 47 640:  42%|████▏     | 8/19 [00:03<00:04,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08087 0.04664 0.02742 53 640:  42%|████▏     | 8/19 [00:03<00:04,  2.28it/s]13/99 7.28G 0.08087 0.04664 0.02742 53 640:  47%|████▋     | 9/19 [00:03<00:04,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08034 0.04541 0.02716 48 640:  47%|████▋     | 9/19 [00:04<00:04,  2.12it/s]13/99 7.28G 0.08034 0.04541 0.02716 48 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08082 0.04773 0.02698 121 640:  53%|█████▎    | 10/19 [00:05<00:04,  1.88it/s]13/99 7.28G 0.08082 0.04773 0.02698 121 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08116 0.0475 0.02682 81 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.81it/s]  13/99 7.28G 0.08116 0.0475 0.02682 81 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08104 0.0466 0.02698 52 640:  63%|██████▎   | 12/19 [00:12<00:04,  1.70it/s]13/99 7.28G 0.08104 0.0466 0.02698 52 640:  68%|██████▊   | 13/19 [00:12<00:14,  2.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08063 0.04646 0.02692 64 640:  68%|██████▊   | 13/19 [00:12<00:14,  2.35s/it]13/99 7.28G 0.08063 0.04646 0.02692 64 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.74s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08105 0.04716 0.02695 100 640:  74%|███████▎  | 14/19 [00:18<00:08,  1.74s/it]13/99 7.28G 0.08105 0.04716 0.02695 100 640:  79%|███████▉  | 15/19 [00:18<00:11,  2.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.0811 0.0477 0.02697 83 640:  79%|███████▉  | 15/19 [00:25<00:11,  2.95s/it]   13/99 7.28G 0.0811 0.0477 0.02697 83 640:  84%|████████▍ | 16/19 [00:25<00:12,  4.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08065 0.04745 0.02705 58 640:  84%|████████▍ | 16/19 [00:25<00:12,  4.21s/it]13/99 7.28G 0.08065 0.04745 0.02705 58 640:  89%|████████▉ | 17/19 [00:25<00:06,  3.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08092 0.04834 0.02714 108 640:  89%|████████▉ | 17/19 [00:26<00:06,  3.05s/it]13/99 7.28G 0.08092 0.04834 0.02714 108 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08079 0.04884 0.02718 83 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.25s/it] 13/99 7.28G 0.08079 0.04884 0.02718 83 640: 100%|██████████| 19/19 [00:28<00:00,  2.40s/it]13/99 7.28G 0.08079 0.04884 0.02718 83 640: 100%|██████████| 19/19 [00:28<00:00,  1.51s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.14s/it]
                   all         55        256    0.00825        0.6     0.0348     0.0103
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07749 0.04568 0.02629 68 640:   0%|          | 0/19 [00:00<?, ?it/s]14/99 7.28G 0.07749 0.04568 0.02629 68 640:   5%|▌         | 1/19 [00:00<00:11,  1.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07467 0.04905 0.02439 75 640:   5%|▌         | 1/19 [00:01<00:11,  1.52it/s]14/99 7.28G 0.07467 0.04905 0.02439 75 640:  11%|█         | 2/19 [00:01<00:10,  1.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07372 0.05048 0.02485 71 640:  11%|█         | 2/19 [00:01<00:10,  1.68it/s]14/99 7.28G 0.07372 0.05048 0.02485 71 640:  16%|█▌        | 3/19 [00:01<00:09,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07546 0.04815 0.02612 64 640:  16%|█▌        | 3/19 [00:02<00:09,  1.75it/s]14/99 7.28G 0.07546 0.04815 0.02612 64 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07502 0.04697 0.02596 60 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]14/99 7.28G 0.07502 0.04697 0.02596 60 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07547 0.04704 0.02575 75 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]14/99 7.28G 0.07547 0.04704 0.02575 75 640:  32%|███▏      | 6/19 [00:02<00:05,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07651 0.04733 0.02618 72 640:  32%|███▏      | 6/19 [00:03<00:05,  2.28it/s]14/99 7.28G 0.07651 0.04733 0.02618 72 640:  37%|███▋      | 7/19 [00:03<00:04,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07729 0.04778 0.02649 81 640:  37%|███▋      | 7/19 [00:03<00:04,  2.47it/s]14/99 7.28G 0.07729 0.04778 0.02649 81 640:  42%|████▏     | 8/19 [00:03<00:04,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07816 0.04673 0.02614 67 640:  42%|████▏     | 8/19 [00:03<00:04,  2.52it/s]14/99 7.28G 0.07816 0.04673 0.02614 67 640:  47%|████▋     | 9/19 [00:03<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07825 0.04655 0.02644 64 640:  47%|████▋     | 9/19 [00:08<00:03,  2.57it/s]14/99 7.28G 0.07825 0.04655 0.02644 64 640:  53%|█████▎    | 10/19 [00:08<00:14,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07857 0.04742 0.02661 93 640:  53%|█████▎    | 10/19 [00:08<00:14,  1.62s/it]14/99 7.28G 0.07857 0.04742 0.02661 93 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.079 0.04777 0.02659 89 640:  58%|█████▊    | 11/19 [00:12<00:09,  1.23s/it]  14/99 7.28G 0.079 0.04777 0.02659 89 640:  63%|██████▎   | 12/19 [00:12<00:14,  2.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07945 0.04734 0.0266 72 640:  63%|██████▎   | 12/19 [00:19<00:14,  2.09s/it]14/99 7.28G 0.07945 0.04734 0.0266 72 640:  68%|██████▊   | 13/19 [00:19<00:20,  3.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07984 0.04914 0.02665 117 640:  68%|██████▊   | 13/19 [00:19<00:20,  3.46s/it]14/99 7.28G 0.07984 0.04914 0.02665 117 640:  74%|███████▎  | 14/19 [00:19<00:12,  2.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07954 0.04857 0.02669 60 640:  74%|███████▎  | 14/19 [00:20<00:12,  2.47s/it] 14/99 7.28G 0.07954 0.04857 0.02669 60 640:  79%|███████▉  | 15/19 [00:20<00:08,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07941 0.0487 0.02661 73 640:  79%|███████▉  | 15/19 [00:24<00:08,  2.01s/it] 14/99 7.28G 0.07941 0.0487 0.02661 73 640:  84%|████████▍ | 16/19 [00:24<00:07,  2.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07888 0.04787 0.02658 47 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.61s/it]14/99 7.28G 0.07888 0.04787 0.02658 47 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07892 0.04746 0.02645 65 640:  89%|████████▉ | 17/19 [00:30<00:04,  2.04s/it]14/99 7.28G 0.07892 0.04746 0.02645 65 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07875 0.04669 0.02666 43 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.93s/it]14/99 7.28G 0.07875 0.04669 0.02666 43 640: 100%|██████████| 19/19 [00:30<00:00,  2.10s/it]14/99 7.28G 0.07875 0.04669 0.02666 43 640: 100%|██████████| 19/19 [00:30<00:00,  1.60s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.40s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.53s/it]
                   all         55        256      0.121     0.0549     0.0508     0.0139
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.0786 0.05403 0.02355 82 640:   0%|          | 0/19 [00:00<?, ?it/s]15/99 7.28G 0.0786 0.05403 0.02355 82 640:   5%|▌         | 1/19 [00:00<00:07,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07642 0.04989 0.02295 67 640:   5%|▌         | 1/19 [00:00<00:07,  2.56it/s]15/99 7.28G 0.07642 0.04989 0.02295 67 640:  11%|█         | 2/19 [00:00<00:04,  3.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07734 0.05355 0.02366 90 640:  11%|█         | 2/19 [00:00<00:04,  3.79it/s]15/99 7.28G 0.07734 0.05355 0.02366 90 640:  16%|█▌        | 3/19 [00:00<00:03,  4.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07885 0.05581 0.02461 105 640:  16%|█▌        | 3/19 [00:00<00:03,  4.48it/s]15/99 7.28G 0.07885 0.05581 0.02461 105 640:  21%|██        | 4/19 [00:00<00:03,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07923 0.05477 0.02435 87 640:  21%|██        | 4/19 [00:01<00:03,  4.88it/s] 15/99 7.28G 0.07923 0.05477 0.02435 87 640:  26%|██▋       | 5/19 [00:01<00:02,  5.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07858 0.05165 0.02407 55 640:  26%|██▋       | 5/19 [00:01<00:02,  5.15it/s]15/99 7.28G 0.07858 0.05165 0.02407 55 640:  32%|███▏      | 6/19 [00:01<00:02,  5.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07732 0.04966 0.02473 58 640:  32%|███▏      | 6/19 [00:01<00:02,  5.34it/s]15/99 7.28G 0.07732 0.04966 0.02473 58 640:  37%|███▋      | 7/19 [00:01<00:02,  5.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07687 0.04815 0.02507 51 640:  37%|███▋      | 7/19 [00:01<00:02,  5.37it/s]15/99 7.28G 0.07687 0.04815 0.02507 51 640:  42%|████▏     | 8/19 [00:01<00:02,  4.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07829 0.0488 0.02494 118 640:  42%|████▏     | 8/19 [00:05<00:02,  4.28it/s]15/99 7.28G 0.07829 0.0488 0.02494 118 640:  47%|████▋     | 9/19 [00:05<00:13,  1.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07837 0.04818 0.02504 64 640:  47%|████▋     | 9/19 [00:10<00:13,  1.30s/it]15/99 7.28G 0.07837 0.04818 0.02504 64 640:  53%|█████▎    | 10/19 [00:10<00:22,  2.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07857 0.05041 0.02509 113 640:  53%|█████▎    | 10/19 [00:11<00:22,  2.52s/it]15/99 7.28G 0.07857 0.05041 0.02509 113 640:  58%|█████▊    | 11/19 [00:11<00:15,  1.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07834 0.05035 0.02538 70 640:  58%|█████▊    | 11/19 [00:13<00:15,  1.95s/it] 15/99 7.28G 0.07834 0.05035 0.02538 70 640:  63%|██████▎   | 12/19 [00:13<00:13,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07918 0.05173 0.02533 139 640:  63%|██████▎   | 12/19 [00:15<00:13,  1.98s/it]15/99 7.28G 0.07918 0.05173 0.02533 139 640:  68%|██████▊   | 13/19 [00:15<00:12,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.0795 0.05161 0.02536 83 640:  68%|██████▊   | 13/19 [00:16<00:12,  2.15s/it]  15/99 7.28G 0.0795 0.05161 0.02536 83 640:  74%|███████▎  | 14/19 [00:16<00:07,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07937 0.05186 0.02538 85 640:  74%|███████▎  | 14/19 [00:23<00:07,  1.56s/it]15/99 7.28G 0.07937 0.05186 0.02538 85 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07909 0.05045 0.02522 39 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.28s/it]15/99 7.28G 0.07909 0.05045 0.02522 39 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07889 0.05057 0.02532 75 640:  84%|████████▍ | 16/19 [00:29<00:07,  2.34s/it]15/99 7.28G 0.07889 0.05057 0.02532 75 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07876 0.04999 0.02534 58 640:  89%|████████▉ | 17/19 [00:35<00:06,  3.47s/it]15/99 7.28G 0.07876 0.04999 0.02534 58 640:  95%|█████████▍| 18/19 [00:35<00:04,  4.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.0786 0.04853 0.02516 31 640:  95%|█████████▍| 18/19 [00:36<00:04,  4.18s/it] 15/99 7.28G 0.0786 0.04853 0.02516 31 640: 100%|██████████| 19/19 [00:36<00:00,  3.21s/it]15/99 7.28G 0.0786 0.04853 0.02516 31 640: 100%|██████████| 19/19 [00:36<00:00,  1.92s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09, 10.00s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.19s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]
                   all         55        256      0.474     0.0353     0.0482     0.0138
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07473 0.03065 0.0273 42 640:   0%|          | 0/19 [00:00<?, ?it/s]16/99 7.28G 0.07473 0.03065 0.0273 42 640:   5%|▌         | 1/19 [00:00<00:13,  1.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07982 0.04914 0.02655 125 640:   5%|▌         | 1/19 [00:01<00:13,  1.29it/s]16/99 7.28G 0.07982 0.04914 0.02655 125 640:  11%|█         | 2/19 [00:01<00:12,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08121 0.04958 0.02576 90 640:  11%|█         | 2/19 [00:02<00:12,  1.37it/s] 16/99 7.28G 0.08121 0.04958 0.02576 90 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08102 0.05237 0.02541 103 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s]16/99 7.28G 0.08102 0.05237 0.02541 103 640:  21%|██        | 4/19 [00:02<00:10,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07962 0.05537 0.02466 101 640:  21%|██        | 4/19 [00:03<00:10,  1.40it/s]16/99 7.28G 0.07962 0.05537 0.02466 101 640:  26%|██▋       | 5/19 [00:03<00:09,  1.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07857 0.05344 0.02526 58 640:  26%|██▋       | 5/19 [00:03<00:09,  1.49it/s] 16/99 7.28G 0.07857 0.05344 0.02526 58 640:  32%|███▏      | 6/19 [00:03<00:08,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07938 0.05545 0.02504 117 640:  32%|███▏      | 6/19 [00:04<00:08,  1.61it/s]16/99 7.28G 0.07938 0.05545 0.02504 117 640:  37%|███▋      | 7/19 [00:04<00:06,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.0788 0.05245 0.02508 44 640:  37%|███▋      | 7/19 [00:04<00:06,  1.73it/s]  16/99 7.28G 0.0788 0.05245 0.02508 44 640:  42%|████▏     | 8/19 [00:04<00:05,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07896 0.05142 0.025 67 640:  42%|████▏     | 8/19 [00:05<00:05,  2.00it/s] 16/99 7.28G 0.07896 0.05142 0.025 67 640:  47%|████▋     | 9/19 [00:05<00:04,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07893 0.04927 0.02491 46 640:  47%|████▋     | 9/19 [00:05<00:04,  2.21it/s]16/99 7.28G 0.07893 0.04927 0.02491 46 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07887 0.05225 0.02475 124 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.38it/s]16/99 7.28G 0.07887 0.05225 0.02475 124 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.0789 0.05195 0.02448 85 640:  58%|█████▊    | 11/19 [00:10<00:03,  2.52it/s]  16/99 7.28G 0.0789 0.05195 0.02448 85 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07885 0.05155 0.02439 76 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.70s/it]16/99 7.28G 0.07885 0.05155 0.02439 76 640:  68%|██████▊   | 13/19 [00:10<00:07,  1.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07919 0.05031 0.02448 59 640:  68%|██████▊   | 13/19 [00:18<00:07,  1.30s/it]16/99 7.28G 0.07919 0.05031 0.02448 59 640:  74%|███████▎  | 14/19 [00:18<00:16,  3.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07879 0.04931 0.02482 52 640:  74%|███████▎  | 14/19 [00:26<00:16,  3.27s/it]16/99 7.28G 0.07879 0.04931 0.02482 52 640:  79%|███████▉  | 15/19 [00:26<00:17,  4.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07868 0.04931 0.02463 85 640:  79%|███████▉  | 15/19 [00:26<00:17,  4.48s/it]16/99 7.28G 0.07868 0.04931 0.02463 85 640:  84%|████████▍ | 16/19 [00:26<00:09,  3.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.0786 0.04939 0.02471 78 640:  84%|████████▍ | 16/19 [00:27<00:09,  3.29s/it] 16/99 7.28G 0.0786 0.04939 0.02471 78 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07885 0.04994 0.02463 104 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.50s/it]16/99 7.28G 0.07885 0.04994 0.02463 104 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07911 0.05005 0.02452 93 640:  95%|█████████▍| 18/19 [00:31<00:02,  2.87s/it] 16/99 7.28G 0.07911 0.05005 0.02452 93 640: 100%|██████████| 19/19 [00:31<00:00,  2.22s/it]16/99 7.28G 0.07911 0.05005 0.02452 93 640: 100%|██████████| 19/19 [00:31<00:00,  1.67s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.26s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.29s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]
                   all         55        256      0.456     0.0506     0.0529     0.0149
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08074 0.04228 0.02837 71 640:   0%|          | 0/19 [00:00<?, ?it/s]17/99 7.28G 0.08074 0.04228 0.02837 71 640:   5%|▌         | 1/19 [00:00<00:09,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.07867 0.03845 0.02455 49 640:   5%|▌         | 1/19 [00:00<00:09,  1.93it/s]17/99 7.28G 0.07867 0.03845 0.02455 49 640:  11%|█         | 2/19 [00:00<00:07,  2.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.0811 0.04673 0.02349 132 640:  11%|█         | 2/19 [00:01<00:07,  2.37it/s]17/99 7.28G 0.0811 0.04673 0.02349 132 640:  16%|█▌        | 3/19 [00:01<00:04,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.0816 0.05111 0.02505 102 640:  16%|█▌        | 3/19 [00:01<00:04,  3.21it/s]17/99 7.28G 0.0816 0.05111 0.02505 102 640:  21%|██        | 4/19 [00:01<00:03,  3.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08262 0.05023 0.02598 85 640:  21%|██        | 4/19 [00:01<00:03,  3.89it/s]17/99 7.28G 0.08262 0.05023 0.02598 85 640:  26%|██▋       | 5/19 [00:01<00:03,  4.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08285 0.05085 0.02584 93 640:  26%|██▋       | 5/19 [00:01<00:03,  4.38it/s]17/99 7.28G 0.08285 0.05085 0.02584 93 640:  32%|███▏      | 6/19 [00:01<00:02,  4.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08366 0.04983 0.02573 80 640:  32%|███▏      | 6/19 [00:01<00:02,  4.77it/s]17/99 7.28G 0.08366 0.04983 0.02573 80 640:  37%|███▋      | 7/19 [00:01<00:02,  5.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08261 0.04962 0.02562 74 640:  37%|███▋      | 7/19 [00:01<00:02,  5.05it/s]17/99 7.28G 0.08261 0.04962 0.02562 74 640:  42%|████▏     | 8/19 [00:01<00:02,  5.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08193 0.04876 0.02555 62 640:  42%|████▏     | 8/19 [00:05<00:02,  5.16it/s]17/99 7.28G 0.08193 0.04876 0.02555 62 640:  47%|████▋     | 9/19 [00:05<00:10,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08282 0.04969 0.02535 137 640:  47%|████▋     | 9/19 [00:05<00:10,  1.10s/it]17/99 7.28G 0.08282 0.04969 0.02535 137 640:  53%|█████▎    | 10/19 [00:05<00:08,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08194 0.04787 0.02597 45 640:  53%|█████▎    | 10/19 [00:09<00:08,  1.08it/s] 17/99 7.28G 0.08194 0.04787 0.02597 45 640:  58%|█████▊    | 11/19 [00:09<00:15,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08169 0.04811 0.02582 79 640:  58%|█████▊    | 11/19 [00:17<00:15,  1.93s/it]17/99 7.28G 0.08169 0.04811 0.02582 79 640:  63%|██████▎   | 12/19 [00:17<00:26,  3.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08166 0.04759 0.02569 70 640:  63%|██████▎   | 12/19 [00:18<00:26,  3.84s/it]17/99 7.28G 0.08166 0.04759 0.02569 70 640:  68%|██████▊   | 13/19 [00:18<00:16,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08129 0.04729 0.02544 69 640:  68%|██████▊   | 13/19 [00:18<00:16,  2.79s/it]17/99 7.28G 0.08129 0.04729 0.02544 69 640:  74%|███████▎  | 14/19 [00:18<00:10,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08187 0.04874 0.02538 142 640:  74%|███████▎  | 14/19 [00:22<00:10,  2.07s/it]17/99 7.28G 0.08187 0.04874 0.02538 142 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08189 0.04874 0.0254 82 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.45s/it]  17/99 7.28G 0.08189 0.04874 0.0254 82 640:  84%|████████▍ | 16/19 [00:22<00:05,  1.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.0813 0.04818 0.0253 58 640:  84%|████████▍ | 16/19 [00:26<00:05,  1.82s/it] 17/99 7.28G 0.0813 0.04818 0.0253 58 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08141 0.04908 0.0253 115 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.61s/it]17/99 7.28G 0.08141 0.04908 0.0253 115 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.0811 0.04879 0.0255 65 640:  95%|█████████▍| 18/19 [00:33<00:01,  1.89s/it]  17/99 7.28G 0.0811 0.04879 0.0255 65 640: 100%|██████████| 19/19 [00:33<00:00,  3.31s/it]17/99 7.28G 0.0811 0.04879 0.0255 65 640: 100%|██████████| 19/19 [00:33<00:00,  1.78s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  6.05s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.32s/it]
                   all         55        256      0.425     0.0415      0.052      0.016
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07559 0.03318 0.02576 50 640:   0%|          | 0/19 [00:00<?, ?it/s]18/99 7.28G 0.07559 0.03318 0.02576 50 640:   5%|▌         | 1/19 [00:00<00:09,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07824 0.03236 0.02546 48 640:   5%|▌         | 1/19 [00:00<00:09,  1.94it/s]18/99 7.28G 0.07824 0.03236 0.02546 48 640:  11%|█         | 2/19 [00:00<00:07,  2.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07811 0.03789 0.02566 79 640:  11%|█         | 2/19 [00:01<00:07,  2.34it/s]18/99 7.28G 0.07811 0.03789 0.02566 79 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07672 0.03788 0.02456 53 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]18/99 7.28G 0.07672 0.03788 0.02456 53 640:  21%|██        | 4/19 [00:01<00:06,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07785 0.04033 0.02426 83 640:  21%|██        | 4/19 [00:02<00:06,  2.42it/s]18/99 7.28G 0.07785 0.04033 0.02426 83 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07696 0.04243 0.02443 74 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]18/99 7.28G 0.07696 0.04243 0.02443 74 640:  32%|███▏      | 6/19 [00:02<00:06,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07766 0.04525 0.02445 109 640:  32%|███▏      | 6/19 [00:03<00:06,  2.08it/s]18/99 7.28G 0.07766 0.04525 0.02445 109 640:  37%|███▋      | 7/19 [00:03<00:06,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07712 0.04567 0.02456 71 640:  37%|███▋      | 7/19 [00:03<00:06,  2.00it/s] 18/99 7.28G 0.07712 0.04567 0.02456 71 640:  42%|████▏     | 8/19 [00:03<00:05,  1.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07684 0.04453 0.02464 52 640:  42%|████▏     | 8/19 [00:05<00:05,  1.96it/s]18/99 7.28G 0.07684 0.04453 0.02464 52 640:  47%|████▋     | 9/19 [00:05<00:08,  1.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07632 0.04383 0.0247 51 640:  47%|████▋     | 9/19 [00:06<00:08,  1.12it/s] 18/99 7.28G 0.07632 0.04383 0.0247 51 640:  53%|█████▎    | 10/19 [00:06<00:07,  1.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07711 0.04388 0.02484 79 640:  53%|█████▎    | 10/19 [00:08<00:07,  1.28it/s]18/99 7.28G 0.07711 0.04388 0.02484 79 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07654 0.04318 0.02466 48 640:  58%|█████▊    | 11/19 [00:10<00:09,  1.16s/it]18/99 7.28G 0.07654 0.04318 0.02466 48 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07621 0.04338 0.02447 74 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.44s/it]18/99 7.28G 0.07621 0.04338 0.02447 74 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07708 0.04434 0.02439 128 640:  68%|██████▊   | 13/19 [00:15<00:06,  1.06s/it]18/99 7.28G 0.07708 0.04434 0.02439 128 640:  74%|███████▎  | 14/19 [00:15<00:11,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07688 0.0437 0.02431 51 640:  74%|███████▎  | 14/19 [00:15<00:11,  2.27s/it]  18/99 7.28G 0.07688 0.0437 0.02431 51 640:  79%|███████▉  | 15/19 [00:15<00:06,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.0774 0.04571 0.02402 149 640:  79%|███████▉  | 15/19 [00:19<00:06,  1.71s/it]18/99 7.28G 0.0774 0.04571 0.02402 149 640:  84%|████████▍ | 16/19 [00:19<00:07,  2.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07763 0.04574 0.02404 75 640:  84%|████████▍ | 16/19 [00:27<00:07,  2.44s/it]18/99 7.28G 0.07763 0.04574 0.02404 75 640:  89%|████████▉ | 17/19 [00:27<00:07,  3.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07736 0.04634 0.02382 89 640:  89%|████████▉ | 17/19 [00:27<00:07,  3.97s/it]18/99 7.28G 0.07736 0.04634 0.02382 89 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.07714 0.04619 0.02375 63 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.83s/it]18/99 7.28G 0.07714 0.04619 0.02375 63 640: 100%|██████████| 19/19 [00:30<00:00,  2.91s/it]18/99 7.28G 0.07714 0.04619 0.02375 63 640: 100%|██████████| 19/19 [00:30<00:00,  1.62s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.11s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.06s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]
                   all         55        256      0.214     0.0601     0.0669      0.021
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07498 0.03793 0.02698 53 640:   0%|          | 0/19 [00:00<?, ?it/s]19/99 7.28G 0.07498 0.03793 0.02698 53 640:   5%|▌         | 1/19 [00:00<00:06,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.0758 0.03839 0.02348 61 640:   5%|▌         | 1/19 [00:00<00:06,  2.82it/s] 19/99 7.28G 0.0758 0.03839 0.02348 61 640:  11%|█         | 2/19 [00:00<00:05,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07449 0.03531 0.02324 46 640:  11%|█         | 2/19 [00:00<00:05,  3.38it/s]19/99 7.28G 0.07449 0.03531 0.02324 46 640:  16%|█▌        | 3/19 [00:00<00:03,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.0761 0.04729 0.02402 135 640:  16%|█▌        | 3/19 [00:00<00:03,  4.12it/s]19/99 7.28G 0.0761 0.04729 0.02402 135 640:  21%|██        | 4/19 [00:00<00:03,  4.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07625 0.04597 0.0237 62 640:  21%|██        | 4/19 [00:01<00:03,  4.42it/s] 19/99 7.28G 0.07625 0.04597 0.0237 62 640:  26%|██▋       | 5/19 [00:01<00:04,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07526 0.04489 0.02402 56 640:  26%|██▋       | 5/19 [00:01<00:04,  3.22it/s]19/99 7.28G 0.07526 0.04489 0.02402 56 640:  32%|███▏      | 6/19 [00:01<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07661 0.04627 0.02353 118 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s]19/99 7.28G 0.07661 0.04627 0.02353 118 640:  37%|███▋      | 7/19 [00:02<00:04,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07782 0.04807 0.02331 121 640:  37%|███▋      | 7/19 [00:02<00:04,  2.52it/s]19/99 7.28G 0.07782 0.04807 0.02331 121 640:  42%|████▏     | 8/19 [00:02<00:04,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07818 0.04838 0.0236 83 640:  42%|████▏     | 8/19 [00:03<00:04,  2.55it/s]  19/99 7.28G 0.07818 0.04838 0.0236 83 640:  47%|████▋     | 9/19 [00:03<00:04,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07892 0.04808 0.02399 82 640:  47%|████▋     | 9/19 [00:03<00:04,  2.17it/s]19/99 7.28G 0.07892 0.04808 0.02399 82 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07888 0.04704 0.0241 55 640:  53%|█████▎    | 10/19 [00:07<00:03,  2.41it/s] 19/99 7.28G 0.07888 0.04704 0.0241 55 640:  58%|█████▊    | 11/19 [00:07<00:12,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07883 0.04712 0.02389 78 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.52s/it]19/99 7.28G 0.07883 0.04712 0.02389 78 640:  63%|██████▎   | 12/19 [00:08<00:08,  1.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07898 0.04612 0.02384 55 640:  63%|██████▎   | 12/19 [00:13<00:08,  1.20s/it]19/99 7.28G 0.07898 0.04612 0.02384 55 640:  68%|██████▊   | 13/19 [00:13<00:14,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07933 0.04623 0.02379 89 640:  68%|██████▊   | 13/19 [00:19<00:14,  2.40s/it]19/99 7.28G 0.07933 0.04623 0.02379 89 640:  74%|███████▎  | 14/19 [00:19<00:18,  3.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07965 0.04621 0.02384 79 640:  74%|███████▎  | 14/19 [00:20<00:18,  3.64s/it]19/99 7.28G 0.07965 0.04621 0.02384 79 640:  79%|███████▉  | 15/19 [00:20<00:10,  2.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07967 0.04569 0.02413 64 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.65s/it]19/99 7.28G 0.07967 0.04569 0.02413 64 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07946 0.04532 0.02412 61 640:  84%|████████▍ | 16/19 [00:27<00:09,  3.08s/it]19/99 7.28G 0.07946 0.04532 0.02412 61 640:  89%|████████▉ | 17/19 [00:27<00:06,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07957 0.04631 0.0241 101 640:  89%|████████▉ | 17/19 [00:27<00:06,  3.07s/it]19/99 7.28G 0.07957 0.04631 0.0241 101 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.07912 0.04598 0.02408 57 640:  95%|█████████▍| 18/19 [00:32<00:02,  2.25s/it]19/99 7.28G 0.07912 0.04598 0.02408 57 640: 100%|██████████| 19/19 [00:32<00:00,  2.91s/it]19/99 7.28G 0.07912 0.04598 0.02408 57 640: 100%|██████████| 19/19 [00:32<00:00,  1.69s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.05s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.64s/it]
                   all         55        256      0.129     0.0743     0.0707     0.0241
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0794 0.04673 0.0224 75 640:   0%|          | 0/19 [00:00<?, ?it/s]20/99 7.28G 0.0794 0.04673 0.0224 75 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0764 0.04717 0.02441 69 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]20/99 7.28G 0.0764 0.04717 0.02441 69 640:  11%|█         | 2/19 [00:00<00:03,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0722 0.04433 0.02305 51 640:  11%|█         | 2/19 [00:00<00:03,  5.51it/s]20/99 7.28G 0.0722 0.04433 0.02305 51 640:  16%|█▌        | 3/19 [00:00<00:03,  5.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07602 0.05177 0.02259 141 640:  16%|█▌        | 3/19 [00:00<00:03,  5.29it/s]20/99 7.28G 0.07602 0.05177 0.02259 141 640:  21%|██        | 4/19 [00:00<00:03,  4.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07654 0.05254 0.02232 85 640:  21%|██        | 4/19 [00:01<00:03,  4.63it/s] 20/99 7.28G 0.07654 0.05254 0.02232 85 640:  26%|██▋       | 5/19 [00:01<00:04,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07716 0.05051 0.02273 57 640:  26%|██▋       | 5/19 [00:01<00:04,  3.41it/s]20/99 7.28G 0.07716 0.05051 0.02273 57 640:  32%|███▏      | 6/19 [00:01<00:05,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07852 0.05303 0.02277 115 640:  32%|███▏      | 6/19 [00:02<00:05,  2.60it/s]20/99 7.28G 0.07852 0.05303 0.02277 115 640:  37%|███▋      | 7/19 [00:02<00:05,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07812 0.05221 0.02273 72 640:  37%|███▋      | 7/19 [00:02<00:05,  2.29it/s] 20/99 7.28G 0.07812 0.05221 0.02273 72 640:  42%|████▏     | 8/19 [00:02<00:04,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07926 0.05163 0.02279 107 640:  42%|████▏     | 8/19 [00:03<00:04,  2.39it/s]20/99 7.28G 0.07926 0.05163 0.02279 107 640:  47%|████▋     | 9/19 [00:03<00:04,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07956 0.05092 0.02277 80 640:  47%|████▋     | 9/19 [00:05<00:04,  2.33it/s] 20/99 7.28G 0.07956 0.05092 0.02277 80 640:  53%|█████▎    | 10/19 [00:05<00:08,  1.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07925 0.04921 0.02251 48 640:  53%|█████▎    | 10/19 [00:10<00:08,  1.12it/s]20/99 7.28G 0.07925 0.04921 0.02251 48 640:  58%|█████▊    | 11/19 [00:10<00:16,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0786 0.04753 0.02316 43 640:  58%|█████▊    | 11/19 [00:10<00:16,  2.12s/it] 20/99 7.28G 0.0786 0.04753 0.02316 43 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07824 0.04683 0.02344 60 640:  63%|██████▎   | 12/19 [00:13<00:10,  1.53s/it]20/99 7.28G 0.07824 0.04683 0.02344 60 640:  68%|██████▊   | 13/19 [00:13<00:12,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0786 0.04745 0.02354 93 640:  68%|██████▊   | 13/19 [00:16<00:12,  2.12s/it] 20/99 7.28G 0.0786 0.04745 0.02354 93 640:  74%|███████▎  | 14/19 [00:16<00:11,  2.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07858 0.04666 0.02341 53 640:  74%|███████▎  | 14/19 [00:16<00:11,  2.30s/it]20/99 7.28G 0.07858 0.04666 0.02341 53 640:  79%|███████▉  | 15/19 [00:16<00:06,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07885 0.04676 0.02352 78 640:  79%|███████▉  | 15/19 [00:20<00:06,  1.67s/it]20/99 7.28G 0.07885 0.04676 0.02352 78 640:  84%|████████▍ | 16/19 [00:20<00:07,  2.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07974 0.04781 0.02356 159 640:  84%|████████▍ | 16/19 [00:21<00:07,  2.47s/it]20/99 7.28G 0.07974 0.04781 0.02356 159 640:  89%|████████▉ | 17/19 [00:21<00:04,  2.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07971 0.04851 0.02367 103 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.03s/it]20/99 7.28G 0.07971 0.04851 0.02367 103 640:  95%|█████████▍| 18/19 [00:27<00:03,  3.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07988 0.0474 0.02378 45 640:  95%|█████████▍| 18/19 [00:33<00:03,  3.23s/it]  20/99 7.28G 0.07988 0.0474 0.02378 45 640: 100%|██████████| 19/19 [00:33<00:00,  4.05s/it]20/99 7.28G 0.07988 0.0474 0.02378 45 640: 100%|██████████| 19/19 [00:33<00:00,  1.79s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.74s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.73s/it]
                   all         55        256      0.152     0.0896     0.0991     0.0284
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07976 0.07644 0.02489 123 640:   0%|          | 0/19 [00:00<?, ?it/s]21/99 7.28G 0.07976 0.07644 0.02489 123 640:   5%|▌         | 1/19 [00:00<00:11,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07797 0.0677 0.02718 92 640:   5%|▌         | 1/19 [00:01<00:11,  1.53it/s]  21/99 7.28G 0.07797 0.0677 0.02718 92 640:  11%|█         | 2/19 [00:01<00:09,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07817 0.059 0.02706 66 640:  11%|█         | 2/19 [00:01<00:09,  1.75it/s] 21/99 7.28G 0.07817 0.059 0.02706 66 640:  16%|█▌        | 3/19 [00:01<00:09,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07631 0.05273 0.02634 49 640:  16%|█▌        | 3/19 [00:02<00:09,  1.66it/s]21/99 7.28G 0.07631 0.05273 0.02634 49 640:  21%|██        | 4/19 [00:02<00:09,  1.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07758 0.05576 0.02533 120 640:  21%|██        | 4/19 [00:03<00:09,  1.54it/s]21/99 7.28G 0.07758 0.05576 0.02533 120 640:  26%|██▋       | 5/19 [00:03<00:08,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07788 0.0543 0.02558 70 640:  26%|██▋       | 5/19 [00:03<00:08,  1.64it/s]  21/99 7.28G 0.07788 0.0543 0.02558 70 640:  32%|███▏      | 6/19 [00:03<00:05,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07827 0.05244 0.02511 67 640:  32%|███▏      | 6/19 [00:03<00:05,  2.17it/s]21/99 7.28G 0.07827 0.05244 0.02511 67 640:  37%|███▋      | 7/19 [00:03<00:04,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07834 0.05111 0.02444 78 640:  37%|███▋      | 7/19 [00:03<00:04,  2.72it/s]21/99 7.28G 0.07834 0.05111 0.02444 78 640:  42%|████▏     | 8/19 [00:03<00:03,  3.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07823 0.04908 0.02405 47 640:  42%|████▏     | 8/19 [00:03<00:03,  3.26it/s]21/99 7.28G 0.07823 0.04908 0.02405 47 640:  47%|████▋     | 9/19 [00:03<00:02,  3.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07803 0.04943 0.02354 95 640:  47%|████▋     | 9/19 [00:03<00:02,  3.77it/s]21/99 7.28G 0.07803 0.04943 0.02354 95 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07825 0.05117 0.02342 113 640:  53%|█████▎    | 10/19 [00:05<00:02,  4.22it/s]21/99 7.28G 0.07825 0.05117 0.02342 113 640:  58%|█████▊    | 11/19 [00:05<00:05,  1.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07811 0.05124 0.02342 82 640:  58%|█████▊    | 11/19 [00:06<00:05,  1.49it/s] 21/99 7.28G 0.07811 0.05124 0.02342 82 640:  63%|██████▎   | 12/19 [00:06<00:04,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07818 0.05096 0.02352 81 640:  63%|██████▎   | 12/19 [00:12<00:04,  1.61it/s]21/99 7.28G 0.07818 0.05096 0.02352 81 640:  68%|██████▊   | 13/19 [00:12<00:13,  2.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07722 0.04921 0.02316 38 640:  68%|██████▊   | 13/19 [00:12<00:13,  2.32s/it]21/99 7.28G 0.07722 0.04921 0.02316 38 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.74s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07808 0.05147 0.02306 197 640:  74%|███████▎  | 14/19 [00:19<00:08,  1.74s/it]21/99 7.28G 0.07808 0.05147 0.02306 197 640:  79%|███████▉  | 15/19 [00:19<00:12,  3.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07851 0.05258 0.02322 142 640:  79%|███████▉  | 15/19 [00:24<00:12,  3.17s/it]21/99 7.28G 0.07851 0.05258 0.02322 142 640:  84%|████████▍ | 16/19 [00:24<00:11,  3.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07865 0.05219 0.02321 86 640:  84%|████████▍ | 16/19 [00:24<00:11,  3.85s/it] 21/99 7.28G 0.07865 0.05219 0.02321 86 640:  89%|████████▉ | 17/19 [00:24<00:05,  2.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07866 0.05143 0.02304 66 640:  89%|████████▉ | 17/19 [00:29<00:05,  2.75s/it]21/99 7.28G 0.07866 0.05143 0.02304 66 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.07853 0.05115 0.02312 75 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.44s/it]21/99 7.28G 0.07853 0.05115 0.02312 75 640: 100%|██████████| 19/19 [00:32<00:00,  3.07s/it]21/99 7.28G 0.07853 0.05115 0.02312 75 640: 100%|██████████| 19/19 [00:32<00:00,  1.69s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.75s/it]
                   all         55        256      0.149      0.101      0.105     0.0331
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07382 0.03177 0.02034 49 640:   0%|          | 0/19 [00:00<?, ?it/s]22/99 7.28G 0.07382 0.03177 0.02034 49 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07659 0.04319 0.02135 92 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]22/99 7.28G 0.07659 0.04319 0.02135 92 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07643 0.03974 0.02158 51 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]22/99 7.28G 0.07643 0.03974 0.02158 51 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07536 0.03806 0.02102 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]22/99 7.28G 0.07536 0.03806 0.02102 52 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07588 0.03891 0.02123 63 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]22/99 7.28G 0.07588 0.03891 0.02123 63 640:  26%|██▋       | 5/19 [00:00<00:02,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07561 0.03786 0.02152 49 640:  26%|██▋       | 5/19 [00:01<00:02,  5.64it/s]22/99 7.28G 0.07561 0.03786 0.02152 49 640:  32%|███▏      | 6/19 [00:01<00:03,  4.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07638 0.03911 0.02211 74 640:  32%|███▏      | 6/19 [00:01<00:03,  4.29it/s]22/99 7.28G 0.07638 0.03911 0.02211 74 640:  37%|███▋      | 7/19 [00:01<00:02,  4.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07715 0.04067 0.02172 93 640:  37%|███▋      | 7/19 [00:01<00:02,  4.68it/s]22/99 7.28G 0.07715 0.04067 0.02172 93 640:  42%|████▏     | 8/19 [00:01<00:02,  4.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0764 0.03985 0.02152 50 640:  42%|████▏     | 8/19 [00:01<00:02,  4.35it/s] 22/99 7.28G 0.0764 0.03985 0.02152 50 640:  47%|████▋     | 9/19 [00:01<00:02,  4.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07549 0.03881 0.02143 47 640:  47%|████▋     | 9/19 [00:05<00:02,  4.16it/s]22/99 7.28G 0.07549 0.03881 0.02143 47 640:  53%|█████▎    | 10/19 [00:05<00:12,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07631 0.04051 0.02197 106 640:  53%|█████▎    | 10/19 [00:06<00:12,  1.35s/it]22/99 7.28G 0.07631 0.04051 0.02197 106 640:  58%|█████▊    | 11/19 [00:06<00:09,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0769 0.04045 0.0218 88 640:  58%|█████▊    | 11/19 [00:12<00:09,  1.16s/it]   22/99 7.28G 0.0769 0.04045 0.0218 88 640:  63%|██████▎   | 12/19 [00:12<00:17,  2.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0766 0.03987 0.02222 48 640:  63%|██████▎   | 12/19 [00:16<00:17,  2.56s/it]22/99 7.28G 0.0766 0.03987 0.02222 48 640:  68%|██████▊   | 13/19 [00:16<00:17,  2.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07702 0.03963 0.02203 80 640:  68%|██████▊   | 13/19 [00:16<00:17,  2.92s/it]22/99 7.28G 0.07702 0.03963 0.02203 80 640:  74%|███████▎  | 14/19 [00:16<00:10,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07666 0.04009 0.02178 70 640:  74%|███████▎  | 14/19 [00:19<00:10,  2.15s/it]22/99 7.28G 0.07666 0.04009 0.02178 70 640:  79%|███████▉  | 15/19 [00:19<00:09,  2.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0766 0.04036 0.02177 66 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.42s/it] 22/99 7.28G 0.0766 0.04036 0.02177 66 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07686 0.04068 0.02203 71 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.65s/it]22/99 7.28G 0.07686 0.04068 0.02203 71 640:  89%|████████▉ | 17/19 [00:23<00:04,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0768 0.04111 0.02229 69 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.07s/it] 22/99 7.28G 0.0768 0.04111 0.02229 69 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07668 0.04071 0.02249 48 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.83s/it]22/99 7.28G 0.07668 0.04071 0.02249 48 640: 100%|██████████| 19/19 [00:28<00:00,  2.03s/it]22/99 7.28G 0.07668 0.04071 0.02249 48 640: 100%|██████████| 19/19 [00:28<00:00,  1.48s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.22s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.31s/it]
                   all         55        256      0.177      0.116      0.107     0.0375
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08205 0.06687 0.02337 109 640:   0%|          | 0/19 [00:00<?, ?it/s]23/99 7.28G 0.08205 0.06687 0.02337 109 640:   5%|▌         | 1/19 [00:00<00:03,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07709 0.04965 0.0237 48 640:   5%|▌         | 1/19 [00:00<00:03,  5.35it/s]  23/99 7.28G 0.07709 0.04965 0.0237 48 640:  11%|█         | 2/19 [00:00<00:04,  3.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07711 0.04339 0.02323 49 640:  11%|█         | 2/19 [00:00<00:04,  3.48it/s]23/99 7.28G 0.07711 0.04339 0.02323 49 640:  16%|█▌        | 3/19 [00:00<00:05,  3.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07879 0.04604 0.02355 92 640:  16%|█▌        | 3/19 [00:01<00:05,  3.02it/s]23/99 7.28G 0.07879 0.04604 0.02355 92 640:  21%|██        | 4/19 [00:01<00:06,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07725 0.04508 0.02313 60 640:  21%|██        | 4/19 [00:02<00:06,  2.35it/s]23/99 7.28G 0.07725 0.04508 0.02313 60 640:  26%|██▋       | 5/19 [00:02<00:07,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07786 0.0483 0.02249 120 640:  26%|██▋       | 5/19 [00:02<00:07,  1.86it/s]23/99 7.28G 0.07786 0.0483 0.02249 120 640:  32%|███▏      | 6/19 [00:02<00:07,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07787 0.04896 0.02264 82 640:  32%|███▏      | 6/19 [00:03<00:07,  1.75it/s]23/99 7.28G 0.07787 0.04896 0.02264 82 640:  37%|███▋      | 7/19 [00:03<00:07,  1.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07758 0.05009 0.02296 94 640:  37%|███▋      | 7/19 [00:04<00:07,  1.60it/s]23/99 7.28G 0.07758 0.05009 0.02296 94 640:  42%|████▏     | 8/19 [00:04<00:07,  1.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07696 0.04879 0.02338 57 640:  42%|████▏     | 8/19 [00:05<00:07,  1.52it/s]23/99 7.28G 0.07696 0.04879 0.02338 57 640:  47%|████▋     | 9/19 [00:05<00:07,  1.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07749 0.04847 0.02349 86 640:  47%|████▋     | 9/19 [00:10<00:07,  1.43it/s]23/99 7.28G 0.07749 0.04847 0.02349 86 640:  53%|█████▎    | 10/19 [00:10<00:18,  2.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.0776 0.04767 0.02358 74 640:  53%|█████▎    | 10/19 [00:10<00:18,  2.04s/it] 23/99 7.28G 0.0776 0.04767 0.02358 74 640:  58%|█████▊    | 11/19 [00:10<00:13,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07809 0.04799 0.02348 98 640:  58%|█████▊    | 11/19 [00:13<00:13,  1.64s/it]23/99 7.28G 0.07809 0.04799 0.02348 98 640:  63%|██████▎   | 12/19 [00:13<00:12,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07794 0.04802 0.02311 81 640:  63%|██████▎   | 12/19 [00:13<00:12,  1.79s/it]23/99 7.28G 0.07794 0.04802 0.02311 81 640:  68%|██████▊   | 13/19 [00:13<00:08,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07761 0.04761 0.023 67 640:  68%|██████▊   | 13/19 [00:15<00:08,  1.49s/it]  23/99 7.28G 0.07761 0.04761 0.023 67 640:  74%|███████▎  | 14/19 [00:15<00:07,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07728 0.04652 0.02299 45 640:  74%|███████▎  | 14/19 [00:21<00:07,  1.53s/it]23/99 7.28G 0.07728 0.04652 0.02299 45 640:  79%|███████▉  | 15/19 [00:21<00:12,  3.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.077 0.04581 0.02328 50 640:  79%|███████▉  | 15/19 [00:22<00:12,  3.03s/it]  23/99 7.28G 0.077 0.04581 0.02328 50 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07726 0.04659 0.02354 105 640:  84%|████████▍ | 16/19 [00:26<00:07,  2.41s/it]23/99 7.28G 0.07726 0.04659 0.02354 105 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.07699 0.0456 0.02335 47 640:  89%|████████▉ | 17/19 [00:32<00:05,  2.64s/it]  23/99 7.28G 0.07699 0.0456 0.02335 47 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.0773 0.0463 0.02317 118 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.66s/it]23/99 7.28G 0.0773 0.0463 0.02317 118 640: 100%|██████████| 19/19 [00:32<00:00,  2.61s/it]23/99 7.28G 0.0773 0.0463 0.02317 118 640: 100%|██████████| 19/19 [00:32<00:00,  1.70s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.76s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]
                   all         55        256      0.136      0.107      0.112     0.0376
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08407 0.05246 0.01992 111 640:   0%|          | 0/19 [00:00<?, ?it/s]24/99 7.28G 0.08407 0.05246 0.01992 111 640:   5%|▌         | 1/19 [00:00<00:09,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07754 0.04874 0.0206 72 640:   5%|▌         | 1/19 [00:01<00:09,  1.97it/s]  24/99 7.28G 0.07754 0.04874 0.0206 72 640:  11%|█         | 2/19 [00:01<00:08,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08033 0.05122 0.02126 115 640:  11%|█         | 2/19 [00:01<00:08,  1.97it/s]24/99 7.28G 0.08033 0.05122 0.02126 115 640:  16%|█▌        | 3/19 [00:01<00:08,  1.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08057 0.05002 0.02287 72 640:  16%|█▌        | 3/19 [00:02<00:08,  1.87it/s] 24/99 7.28G 0.08057 0.05002 0.02287 72 640:  21%|██        | 4/19 [00:02<00:10,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07943 0.04965 0.02212 76 640:  21%|██        | 4/19 [00:03<00:10,  1.46it/s]24/99 7.28G 0.07943 0.04965 0.02212 76 640:  26%|██▋       | 5/19 [00:03<00:09,  1.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07774 0.04793 0.02235 53 640:  26%|██▋       | 5/19 [00:03<00:09,  1.42it/s]24/99 7.28G 0.07774 0.04793 0.02235 53 640:  32%|███▏      | 6/19 [00:03<00:07,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07691 0.04645 0.02278 58 640:  32%|███▏      | 6/19 [00:03<00:07,  1.70it/s]24/99 7.28G 0.07691 0.04645 0.02278 58 640:  37%|███▋      | 7/19 [00:03<00:05,  2.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07719 0.04487 0.02269 56 640:  37%|███▋      | 7/19 [00:04<00:05,  2.15it/s]24/99 7.28G 0.07719 0.04487 0.02269 56 640:  42%|████▏     | 8/19 [00:04<00:04,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07757 0.04591 0.02253 95 640:  42%|████▏     | 8/19 [00:04<00:04,  2.60it/s]24/99 7.28G 0.07757 0.04591 0.02253 95 640:  47%|████▋     | 9/19 [00:04<00:03,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07809 0.0461 0.02224 84 640:  47%|████▋     | 9/19 [00:04<00:03,  2.98it/s] 24/99 7.28G 0.07809 0.0461 0.02224 84 640:  53%|█████▎    | 10/19 [00:04<00:02,  3.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07835 0.0474 0.02202 104 640:  53%|█████▎    | 10/19 [00:04<00:02,  3.35it/s]24/99 7.28G 0.07835 0.0474 0.02202 104 640:  58%|█████▊    | 11/19 [00:04<00:02,  3.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07759 0.04662 0.0222 55 640:  58%|█████▊    | 11/19 [00:11<00:02,  3.00it/s] 24/99 7.28G 0.07759 0.04662 0.0222 55 640:  63%|██████▎   | 12/19 [00:11<00:15,  2.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07764 0.04651 0.02228 71 640:  63%|██████▎   | 12/19 [00:11<00:15,  2.18s/it]24/99 7.28G 0.07764 0.04651 0.02228 71 640:  68%|██████▊   | 13/19 [00:11<00:09,  1.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07731 0.04642 0.02226 74 640:  68%|██████▊   | 13/19 [00:17<00:09,  1.57s/it]24/99 7.28G 0.07731 0.04642 0.02226 74 640:  74%|███████▎  | 14/19 [00:17<00:14,  2.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07725 0.04713 0.02219 92 640:  74%|███████▎  | 14/19 [00:22<00:14,  2.80s/it]24/99 7.28G 0.07725 0.04713 0.02219 92 640:  79%|███████▉  | 15/19 [00:22<00:14,  3.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07719 0.04775 0.02219 97 640:  79%|███████▉  | 15/19 [00:22<00:14,  3.54s/it]24/99 7.28G 0.07719 0.04775 0.02219 97 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07693 0.0477 0.02206 72 640:  84%|████████▍ | 16/19 [00:26<00:07,  2.63s/it] 24/99 7.28G 0.07693 0.0477 0.02206 72 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07683 0.04744 0.02206 73 640:  89%|████████▉ | 17/19 [00:28<00:05,  2.93s/it]24/99 7.28G 0.07683 0.04744 0.02206 73 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.07688 0.04716 0.02195 70 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.63s/it]24/99 7.28G 0.07688 0.04716 0.02195 70 640: 100%|██████████| 19/19 [00:29<00:00,  2.16s/it]24/99 7.28G 0.07688 0.04716 0.02195 70 640: 100%|██████████| 19/19 [00:29<00:00,  1.55s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:16<00:16, 16.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:18<00:00,  7.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:18<00:00,  9.10s/it]
                   all         55        256      0.162      0.118      0.102     0.0338
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07595 0.04676 0.02511 76 640:   0%|          | 0/19 [00:00<?, ?it/s]25/99 7.28G 0.07595 0.04676 0.02511 76 640:   5%|▌         | 1/19 [00:00<00:10,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07451 0.04125 0.02453 53 640:   5%|▌         | 1/19 [00:01<00:10,  1.66it/s]25/99 7.28G 0.07451 0.04125 0.02453 53 640:  11%|█         | 2/19 [00:01<00:11,  1.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07574 0.04292 0.0242 83 640:  11%|█         | 2/19 [00:01<00:11,  1.52it/s] 25/99 7.28G 0.07574 0.04292 0.0242 83 640:  16%|█▌        | 3/19 [00:01<00:10,  1.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07737 0.05005 0.02522 112 640:  16%|█▌        | 3/19 [00:02<00:10,  1.48it/s]25/99 7.28G 0.07737 0.05005 0.02522 112 640:  21%|██        | 4/19 [00:02<00:09,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07762 0.0483 0.02452 74 640:  21%|██        | 4/19 [00:03<00:09,  1.61it/s]  25/99 7.28G 0.07762 0.0483 0.02452 74 640:  26%|██▋       | 5/19 [00:03<00:08,  1.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07773 0.04794 0.02475 74 640:  26%|██▋       | 5/19 [00:03<00:08,  1.69it/s]25/99 7.28G 0.07773 0.04794 0.02475 74 640:  32%|███▏      | 6/19 [00:03<00:08,  1.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07688 0.04738 0.02478 71 640:  32%|███▏      | 6/19 [00:04<00:08,  1.59it/s]25/99 7.28G 0.07688 0.04738 0.02478 71 640:  37%|███▋      | 7/19 [00:04<00:07,  1.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07736 0.0478 0.0243 89 640:  37%|███▋      | 7/19 [00:05<00:07,  1.51it/s]  25/99 7.28G 0.07736 0.0478 0.0243 89 640:  42%|████▏     | 8/19 [00:05<00:08,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07706 0.04734 0.02418 74 640:  42%|████▏     | 8/19 [00:06<00:08,  1.26it/s]25/99 7.28G 0.07706 0.04734 0.02418 74 640:  47%|████▋     | 9/19 [00:06<00:09,  1.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07756 0.04763 0.02382 96 640:  47%|████▋     | 9/19 [00:07<00:09,  1.11it/s]25/99 7.28G 0.07756 0.04763 0.02382 96 640:  53%|█████▎    | 10/19 [00:07<00:08,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07725 0.04645 0.02346 61 640:  53%|█████▎    | 10/19 [00:08<00:08,  1.05it/s]25/99 7.28G 0.07725 0.04645 0.02346 61 640:  58%|█████▊    | 11/19 [00:08<00:07,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07747 0.04795 0.02345 115 640:  58%|█████▊    | 11/19 [00:11<00:07,  1.00it/s]25/99 7.28G 0.07747 0.04795 0.02345 115 640:  63%|██████▎   | 12/19 [00:11<00:11,  1.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07752 0.04886 0.02326 108 640:  63%|██████▎   | 12/19 [00:12<00:11,  1.60s/it]25/99 7.28G 0.07752 0.04886 0.02326 108 640:  68%|██████▊   | 13/19 [00:12<00:07,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07698 0.04789 0.02294 50 640:  68%|██████▊   | 13/19 [00:13<00:07,  1.23s/it] 25/99 7.28G 0.07698 0.04789 0.02294 50 640:  74%|███████▎  | 14/19 [00:13<00:06,  1.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07685 0.04838 0.02274 89 640:  74%|███████▎  | 14/19 [00:16<00:06,  1.26s/it]25/99 7.28G 0.07685 0.04838 0.02274 89 640:  79%|███████▉  | 15/19 [00:16<00:06,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07705 0.04763 0.02273 65 640:  79%|███████▉  | 15/19 [00:16<00:06,  1.67s/it]25/99 7.28G 0.07705 0.04763 0.02273 65 640:  84%|████████▍ | 16/19 [00:16<00:03,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07671 0.04705 0.02234 61 640:  84%|████████▍ | 16/19 [00:22<00:03,  1.25s/it]25/99 7.28G 0.07671 0.04705 0.02234 61 640:  89%|████████▉ | 17/19 [00:22<00:05,  2.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07708 0.04816 0.02269 118 640:  89%|████████▉ | 17/19 [00:22<00:05,  2.57s/it]25/99 7.28G 0.07708 0.04816 0.02269 118 640:  95%|█████████▍| 18/19 [00:22<00:01,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.07713 0.04759 0.0227 63 640:  95%|█████████▍| 18/19 [00:29<00:01,  1.99s/it]  25/99 7.28G 0.07713 0.04759 0.0227 63 640: 100%|██████████| 19/19 [00:29<00:00,  3.45s/it]25/99 7.28G 0.07713 0.04759 0.0227 63 640: 100%|██████████| 19/19 [00:29<00:00,  1.56s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.22s/it]
                   all         55        256      0.117      0.109     0.0996     0.0288
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07999 0.06012 0.02349 128 640:   0%|          | 0/19 [00:00<?, ?it/s]26/99 7.28G 0.07999 0.06012 0.02349 128 640:   5%|▌         | 1/19 [00:00<00:12,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.0753 0.05067 0.02405 59 640:   5%|▌         | 1/19 [00:01<00:12,  1.39it/s]  26/99 7.28G 0.0753 0.05067 0.02405 59 640:  11%|█         | 2/19 [00:01<00:11,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07578 0.05059 0.0237 83 640:  11%|█         | 2/19 [00:02<00:11,  1.53it/s]26/99 7.28G 0.07578 0.05059 0.0237 83 640:  16%|█▌        | 3/19 [00:02<00:10,  1.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07425 0.04782 0.02347 61 640:  16%|█▌        | 3/19 [00:02<00:10,  1.49it/s]26/99 7.28G 0.07425 0.04782 0.02347 61 640:  21%|██        | 4/19 [00:02<00:09,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07607 0.04621 0.02276 75 640:  21%|██        | 4/19 [00:03<00:09,  1.62it/s]26/99 7.28G 0.07607 0.04621 0.02276 75 640:  26%|██▋       | 5/19 [00:03<00:08,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07621 0.04755 0.02266 87 640:  26%|██▋       | 5/19 [00:03<00:08,  1.62it/s]26/99 7.28G 0.07621 0.04755 0.02266 87 640:  32%|███▏      | 6/19 [00:03<00:08,  1.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07651 0.04838 0.02333 82 640:  32%|███▏      | 6/19 [00:04<00:08,  1.55it/s]26/99 7.28G 0.07651 0.04838 0.02333 82 640:  37%|███▋      | 7/19 [00:04<00:07,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07754 0.05468 0.02337 176 640:  37%|███▋      | 7/19 [00:05<00:07,  1.65it/s]26/99 7.28G 0.07754 0.05468 0.02337 176 640:  42%|████▏     | 8/19 [00:05<00:06,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07733 0.05376 0.02371 74 640:  42%|████▏     | 8/19 [00:05<00:06,  1.65it/s] 26/99 7.28G 0.07733 0.05376 0.02371 74 640:  47%|████▋     | 9/19 [00:05<00:07,  1.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07724 0.05362 0.02356 87 640:  47%|████▋     | 9/19 [00:06<00:07,  1.43it/s]26/99 7.28G 0.07724 0.05362 0.02356 87 640:  53%|█████▎    | 10/19 [00:06<00:05,  1.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07694 0.05186 0.02382 52 640:  53%|█████▎    | 10/19 [00:06<00:05,  1.79it/s]26/99 7.28G 0.07694 0.05186 0.02382 52 640:  58%|█████▊    | 11/19 [00:06<00:03,  2.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07667 0.05201 0.02359 86 640:  58%|█████▊    | 11/19 [00:09<00:03,  2.19it/s]26/99 7.28G 0.07667 0.05201 0.02359 86 640:  63%|██████▎   | 12/19 [00:09<00:08,  1.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07624 0.05068 0.02343 55 640:  63%|██████▎   | 12/19 [00:09<00:08,  1.22s/it]26/99 7.28G 0.07624 0.05068 0.02343 55 640:  68%|██████▊   | 13/19 [00:09<00:05,  1.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07641 0.04988 0.02352 68 640:  68%|██████▊   | 13/19 [00:15<00:05,  1.10it/s]26/99 7.28G 0.07641 0.04988 0.02352 68 640:  74%|███████▎  | 14/19 [00:15<00:11,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07668 0.05005 0.02352 103 640:  74%|███████▎  | 14/19 [00:16<00:11,  2.40s/it]26/99 7.28G 0.07668 0.05005 0.02352 103 640:  79%|███████▉  | 15/19 [00:16<00:07,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07688 0.05002 0.02329 88 640:  79%|███████▉  | 15/19 [00:21<00:07,  1.99s/it] 26/99 7.28G 0.07688 0.05002 0.02329 88 640:  84%|████████▍ | 16/19 [00:21<00:08,  2.94s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07706 0.05009 0.02298 97 640:  84%|████████▍ | 16/19 [00:27<00:08,  2.94s/it]26/99 7.28G 0.07706 0.05009 0.02298 97 640:  89%|████████▉ | 17/19 [00:27<00:07,  3.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07733 0.05029 0.02282 101 640:  89%|████████▉ | 17/19 [00:27<00:07,  3.72s/it]26/99 7.28G 0.07733 0.05029 0.02282 101 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.07731 0.0506 0.02252 94 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.65s/it]  26/99 7.28G 0.07731 0.0506 0.02252 94 640: 100%|██████████| 19/19 [00:27<00:00,  1.91s/it]26/99 7.28G 0.07731 0.0506 0.02252 94 640: 100%|██████████| 19/19 [00:27<00:00,  1.44s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  7.00s/it]
                   all         55        256      0.187      0.132      0.111      0.038
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07456 0.03407 0.025 57 640:   0%|          | 0/19 [00:00<?, ?it/s]27/99 7.28G 0.07456 0.03407 0.025 57 640:   5%|▌         | 1/19 [00:00<00:06,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07347 0.03639 0.02545 64 640:   5%|▌         | 1/19 [00:00<00:06,  2.83it/s]27/99 7.28G 0.07347 0.03639 0.02545 64 640:  11%|█         | 2/19 [00:00<00:05,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07571 0.04365 0.02455 117 640:  11%|█         | 2/19 [00:01<00:05,  2.88it/s]27/99 7.28G 0.07571 0.04365 0.02455 117 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07581 0.04248 0.02463 63 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s] 27/99 7.28G 0.07581 0.04248 0.02463 63 640:  21%|██        | 4/19 [00:01<00:06,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07557 0.04109 0.02413 56 640:  21%|██        | 4/19 [00:02<00:06,  2.33it/s]27/99 7.28G 0.07557 0.04109 0.02413 56 640:  26%|██▋       | 5/19 [00:02<00:06,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07515 0.04104 0.02424 64 640:  26%|██▋       | 5/19 [00:02<00:06,  2.11it/s]27/99 7.28G 0.07515 0.04104 0.02424 64 640:  32%|███▏      | 6/19 [00:02<00:06,  2.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07512 0.04241 0.02423 81 640:  32%|███▏      | 6/19 [00:03<00:06,  2.01it/s]27/99 7.28G 0.07512 0.04241 0.02423 81 640:  37%|███▋      | 7/19 [00:03<00:06,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07504 0.04206 0.02464 61 640:  37%|███▋      | 7/19 [00:03<00:06,  1.95it/s]27/99 7.28G 0.07504 0.04206 0.02464 61 640:  42%|████▏     | 8/19 [00:03<00:05,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07567 0.04485 0.0246 122 640:  42%|████▏     | 8/19 [00:04<00:05,  1.89it/s]27/99 7.28G 0.07567 0.04485 0.0246 122 640:  47%|████▋     | 9/19 [00:04<00:05,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07555 0.04363 0.02459 48 640:  47%|████▋     | 9/19 [00:04<00:05,  1.78it/s]27/99 7.28G 0.07555 0.04363 0.02459 48 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07603 0.04445 0.02428 97 640:  53%|█████▎    | 10/19 [00:08<00:04,  1.99it/s]27/99 7.28G 0.07603 0.04445 0.02428 97 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07644 0.04505 0.0244 82 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.52s/it] 27/99 7.28G 0.07644 0.04505 0.0244 82 640:  63%|██████▎   | 12/19 [00:08<00:07,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07665 0.04569 0.024 95 640:  63%|██████▎   | 12/19 [00:15<00:07,  1.11s/it] 27/99 7.28G 0.07665 0.04569 0.024 95 640:  68%|██████▊   | 13/19 [00:15<00:17,  2.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07658 0.04582 0.02404 72 640:  68%|██████▊   | 13/19 [00:21<00:17,  2.91s/it]27/99 7.28G 0.07658 0.04582 0.02404 72 640:  74%|███████▎  | 14/19 [00:21<00:19,  3.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07633 0.04482 0.02405 47 640:  74%|███████▎  | 14/19 [00:21<00:19,  3.81s/it]27/99 7.28G 0.07633 0.04482 0.02405 47 640:  79%|███████▉  | 15/19 [00:21<00:10,  2.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07618 0.04456 0.02441 61 640:  79%|███████▉  | 15/19 [00:22<00:10,  2.71s/it]27/99 7.28G 0.07618 0.04456 0.02441 61 640:  84%|████████▍ | 16/19 [00:22<00:05,  1.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07635 0.04476 0.02417 86 640:  84%|████████▍ | 16/19 [00:25<00:05,  1.95s/it]27/99 7.28G 0.07635 0.04476 0.02417 86 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07628 0.04478 0.02402 73 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.34s/it]27/99 7.28G 0.07628 0.04478 0.02402 73 640:  95%|█████████▍| 18/19 [00:25<00:01,  1.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07671 0.04547 0.02393 131 640:  95%|█████████▍| 18/19 [00:31<00:01,  1.80s/it]27/99 7.28G 0.07671 0.04547 0.02393 131 640: 100%|██████████| 19/19 [00:31<00:00,  2.99s/it]27/99 7.28G 0.07671 0.04547 0.02393 131 640: 100%|██████████| 19/19 [00:31<00:00,  1.67s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.23s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.17s/it]
                   all         55        256      0.184      0.168      0.112      0.036
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.06841 0.03541 0.02052 52 640:   0%|          | 0/19 [00:00<?, ?it/s]28/99 7.28G 0.06841 0.03541 0.02052 52 640:   5%|▌         | 1/19 [00:00<00:06,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07578 0.05371 0.02137 120 640:   5%|▌         | 1/19 [00:00<00:06,  2.87it/s]28/99 7.28G 0.07578 0.05371 0.02137 120 640:  11%|█         | 2/19 [00:00<00:07,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07616 0.05104 0.02139 68 640:  11%|█         | 2/19 [00:01<00:07,  2.16it/s] 28/99 7.28G 0.07616 0.05104 0.02139 68 640:  16%|█▌        | 3/19 [00:01<00:08,  1.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07752 0.05006 0.02146 82 640:  16%|█▌        | 3/19 [00:02<00:08,  1.79it/s]28/99 7.28G 0.07752 0.05006 0.02146 82 640:  21%|██        | 4/19 [00:02<00:08,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07638 0.04741 0.02092 58 640:  21%|██        | 4/19 [00:02<00:08,  1.78it/s]28/99 7.28G 0.07638 0.04741 0.02092 58 640:  26%|██▋       | 5/19 [00:02<00:08,  1.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07523 0.04684 0.0202 72 640:  26%|██▋       | 5/19 [00:03<00:08,  1.63it/s] 28/99 7.28G 0.07523 0.04684 0.0202 72 640:  32%|███▏      | 6/19 [00:03<00:08,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07512 0.04641 0.0203 76 640:  32%|███▏      | 6/19 [00:04<00:08,  1.46it/s]28/99 7.28G 0.07512 0.04641 0.0203 76 640:  37%|███▋      | 7/19 [00:04<00:08,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07357 0.04441 0.021 44 640:  37%|███▋      | 7/19 [00:05<00:08,  1.34it/s] 28/99 7.28G 0.07357 0.04441 0.021 44 640:  42%|████▏     | 8/19 [00:05<00:08,  1.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07342 0.0442 0.02096 65 640:  42%|████▏     | 8/19 [00:05<00:08,  1.32it/s]28/99 7.28G 0.07342 0.0442 0.02096 65 640:  47%|████▋     | 9/19 [00:05<00:07,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07339 0.046 0.0211 100 640:  47%|████▋     | 9/19 [00:08<00:07,  1.39it/s] 28/99 7.28G 0.07339 0.046 0.0211 100 640:  53%|█████▎    | 10/19 [00:08<00:10,  1.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07465 0.04864 0.02121 154 640:  53%|█████▎    | 10/19 [00:12<00:10,  1.20s/it]28/99 7.28G 0.07465 0.04864 0.02121 154 640:  58%|█████▊    | 11/19 [00:12<00:18,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07494 0.04783 0.02105 67 640:  58%|█████▊    | 11/19 [00:13<00:18,  2.26s/it] 28/99 7.28G 0.07494 0.04783 0.02105 67 640:  63%|██████▎   | 12/19 [00:13<00:11,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07502 0.04718 0.02125 66 640:  63%|██████▎   | 12/19 [00:13<00:11,  1.62s/it]28/99 7.28G 0.07502 0.04718 0.02125 66 640:  68%|██████▊   | 13/19 [00:13<00:07,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07504 0.04646 0.02127 63 640:  68%|██████▊   | 13/19 [00:16<00:07,  1.18s/it]28/99 7.28G 0.07504 0.04646 0.02127 63 640:  74%|███████▎  | 14/19 [00:16<00:08,  1.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07519 0.04579 0.0213 64 640:  74%|███████▎  | 14/19 [00:16<00:08,  1.78s/it] 28/99 7.28G 0.07519 0.04579 0.0213 64 640:  79%|███████▉  | 15/19 [00:16<00:05,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07505 0.04495 0.02136 52 640:  79%|███████▉  | 15/19 [00:19<00:05,  1.29s/it]28/99 7.28G 0.07505 0.04495 0.02136 52 640:  84%|████████▍ | 16/19 [00:19<00:05,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07474 0.04445 0.02122 60 640:  84%|████████▍ | 16/19 [00:20<00:05,  1.89s/it]28/99 7.28G 0.07474 0.04445 0.02122 60 640:  89%|████████▉ | 17/19 [00:20<00:02,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07528 0.04462 0.02136 106 640:  89%|████████▉ | 17/19 [00:30<00:02,  1.39s/it]28/99 7.28G 0.07528 0.04462 0.02136 106 640:  95%|█████████▍| 18/19 [00:30<00:04,  4.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07549 0.04472 0.02133 89 640:  95%|█████████▍| 18/19 [00:36<00:04,  4.08s/it] 28/99 7.28G 0.07549 0.04472 0.02133 89 640: 100%|██████████| 19/19 [00:36<00:00,  4.81s/it]28/99 7.28G 0.07549 0.04472 0.02133 89 640: 100%|██████████| 19/19 [00:36<00:00,  1.94s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  7.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.32s/it]
                   all         55        256      0.194      0.219      0.147      0.047
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07413 0.0465 0.02645 77 640:   0%|          | 0/19 [00:00<?, ?it/s]29/99 7.28G 0.07413 0.0465 0.02645 77 640:   5%|▌         | 1/19 [00:00<00:07,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07671 0.04291 0.02435 57 640:   5%|▌         | 1/19 [00:00<00:07,  2.39it/s]29/99 7.28G 0.07671 0.04291 0.02435 57 640:  11%|█         | 2/19 [00:00<00:04,  3.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08009 0.04804 0.02344 118 640:  11%|█         | 2/19 [00:00<00:04,  3.59it/s]29/99 7.28G 0.08009 0.04804 0.02344 118 640:  16%|█▌        | 3/19 [00:00<00:03,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07916 0.04629 0.02304 61 640:  16%|█▌        | 3/19 [00:00<00:03,  4.27it/s] 29/99 7.28G 0.07916 0.04629 0.02304 61 640:  21%|██        | 4/19 [00:00<00:03,  4.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07876 0.04628 0.02266 82 640:  21%|██        | 4/19 [00:01<00:03,  4.59it/s]29/99 7.28G 0.07876 0.04628 0.02266 82 640:  26%|██▋       | 5/19 [00:01<00:02,  4.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07901 0.04753 0.02287 93 640:  26%|██▋       | 5/19 [00:01<00:02,  4.90it/s]29/99 7.28G 0.07901 0.04753 0.02287 93 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07851 0.04841 0.02259 87 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]29/99 7.28G 0.07851 0.04841 0.02259 87 640:  37%|███▋      | 7/19 [00:01<00:02,  4.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07816 0.05066 0.0225 116 640:  37%|███▋      | 7/19 [00:01<00:02,  4.09it/s]29/99 7.28G 0.07816 0.05066 0.0225 116 640:  42%|████▏     | 8/19 [00:01<00:02,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07843 0.0491 0.02266 65 640:  42%|████▏     | 8/19 [00:02<00:02,  3.97it/s] 29/99 7.28G 0.07843 0.0491 0.02266 65 640:  47%|████▋     | 9/19 [00:02<00:02,  3.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07757 0.04716 0.02273 46 640:  47%|████▋     | 9/19 [00:02<00:02,  3.47it/s]29/99 7.28G 0.07757 0.04716 0.02273 46 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07679 0.04625 0.02282 54 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.66it/s]29/99 7.28G 0.07679 0.04625 0.02282 54 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07711 0.04792 0.02276 118 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.12it/s]29/99 7.28G 0.07711 0.04792 0.02276 118 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07693 0.04808 0.0227 84 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.44it/s]  29/99 7.28G 0.07693 0.04808 0.0227 84 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07677 0.04824 0.02277 81 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.20it/s]29/99 7.28G 0.07677 0.04824 0.02277 81 640:  74%|███████▎  | 14/19 [00:03<00:01,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07683 0.0488 0.02266 94 640:  74%|███████▎  | 14/19 [00:13<00:01,  2.66it/s] 29/99 7.28G 0.07683 0.0488 0.02266 94 640:  79%|███████▉  | 15/19 [00:13<00:12,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07704 0.04824 0.02254 72 640:  79%|███████▉  | 15/19 [00:18<00:12,  3.07s/it]29/99 7.28G 0.07704 0.04824 0.02254 72 640:  84%|████████▍ | 16/19 [00:18<00:11,  3.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07686 0.04749 0.02257 58 640:  84%|████████▍ | 16/19 [00:24<00:11,  3.72s/it]29/99 7.28G 0.07686 0.04749 0.02257 58 640:  89%|████████▉ | 17/19 [00:24<00:08,  4.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07663 0.04744 0.02243 73 640:  89%|████████▉ | 17/19 [00:24<00:08,  4.34s/it]29/99 7.28G 0.07663 0.04744 0.02243 73 640:  95%|█████████▍| 18/19 [00:24<00:03,  3.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07641 0.04726 0.02224 77 640:  95%|█████████▍| 18/19 [00:25<00:03,  3.27s/it]29/99 7.28G 0.07641 0.04726 0.02224 77 640: 100%|██████████| 19/19 [00:25<00:00,  2.42s/it]29/99 7.28G 0.07641 0.04726 0.02224 77 640: 100%|██████████| 19/19 [00:25<00:00,  1.34s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.81s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.34s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.46s/it]
                   all         55        256      0.208      0.205      0.135     0.0405
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07633 0.04164 0.01808 69 640:   0%|          | 0/19 [00:00<?, ?it/s]30/99 7.28G 0.07633 0.04164 0.01808 69 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07404 0.04247 0.01984 71 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]30/99 7.28G 0.07404 0.04247 0.01984 71 640:  11%|█         | 2/19 [00:00<00:07,  2.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07614 0.0524 0.02052 121 640:  11%|█         | 2/19 [00:01<00:07,  2.15it/s]30/99 7.28G 0.07614 0.0524 0.02052 121 640:  16%|█▌        | 3/19 [00:01<00:06,  2.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07772 0.05461 0.02031 119 640:  16%|█▌        | 3/19 [00:01<00:06,  2.34it/s]30/99 7.28G 0.07772 0.05461 0.02031 119 640:  21%|██        | 4/19 [00:01<00:06,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.0786 0.05208 0.02071 80 640:  21%|██        | 4/19 [00:01<00:06,  2.38it/s]  30/99 7.28G 0.0786 0.05208 0.02071 80 640:  26%|██▋       | 5/19 [00:01<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07628 0.04962 0.02097 54 640:  26%|██▋       | 5/19 [00:02<00:04,  2.86it/s]30/99 7.28G 0.07628 0.04962 0.02097 54 640:  32%|███▏      | 6/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07574 0.04686 0.02082 48 640:  32%|███▏      | 6/19 [00:02<00:04,  2.86it/s]30/99 7.28G 0.07574 0.04686 0.02082 48 640:  37%|███▋      | 7/19 [00:02<00:04,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07459 0.0456 0.02072 53 640:  37%|███▋      | 7/19 [00:03<00:04,  2.77it/s] 30/99 7.28G 0.07459 0.0456 0.02072 53 640:  42%|████▏     | 8/19 [00:03<00:04,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07504 0.04734 0.02075 113 640:  42%|████▏     | 8/19 [00:03<00:04,  2.50it/s]30/99 7.28G 0.07504 0.04734 0.02075 113 640:  47%|████▋     | 9/19 [00:03<00:03,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07511 0.04721 0.02078 77 640:  47%|████▋     | 9/19 [00:03<00:03,  2.61it/s] 30/99 7.28G 0.07511 0.04721 0.02078 77 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07655 0.05096 0.02111 214 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.79it/s]30/99 7.28G 0.07655 0.05096 0.02111 214 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07572 0.04959 0.0209 55 640:  58%|█████▊    | 11/19 [00:12<00:02,  3.31it/s]  30/99 7.28G 0.07572 0.04959 0.0209 55 640:  63%|██████▎   | 12/19 [00:12<00:19,  2.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07579 0.04917 0.02097 83 640:  63%|██████▎   | 12/19 [00:15<00:19,  2.85s/it]30/99 7.28G 0.07579 0.04917 0.02097 83 640:  68%|██████▊   | 13/19 [00:15<00:17,  2.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07574 0.04956 0.02122 80 640:  68%|██████▊   | 13/19 [00:19<00:17,  2.88s/it]30/99 7.28G 0.07574 0.04956 0.02122 80 640:  74%|███████▎  | 14/19 [00:19<00:16,  3.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07518 0.04848 0.02163 47 640:  74%|███████▎  | 14/19 [00:19<00:16,  3.22s/it]30/99 7.28G 0.07518 0.04848 0.02163 47 640:  79%|███████▉  | 15/19 [00:19<00:09,  2.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07459 0.04676 0.02157 33 640:  79%|███████▉  | 15/19 [00:20<00:09,  2.36s/it]30/99 7.28G 0.07459 0.04676 0.02157 33 640:  84%|████████▍ | 16/19 [00:20<00:05,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07406 0.04582 0.0215 46 640:  84%|████████▍ | 16/19 [00:20<00:05,  1.76s/it] 30/99 7.28G 0.07406 0.04582 0.0215 46 640:  89%|████████▉ | 17/19 [00:20<00:02,  1.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07441 0.04655 0.02147 113 640:  89%|████████▉ | 17/19 [00:23<00:02,  1.34s/it]30/99 7.28G 0.07441 0.04655 0.02147 113 640:  95%|█████████▍| 18/19 [00:23<00:01,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07463 0.04621 0.02159 87 640:  95%|█████████▍| 18/19 [00:23<00:01,  1.64s/it] 30/99 7.28G 0.07463 0.04621 0.02159 87 640: 100%|██████████| 19/19 [00:23<00:00,  1.31s/it]30/99 7.28G 0.07463 0.04621 0.02159 87 640: 100%|██████████| 19/19 [00:23<00:00,  1.24s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.83s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.94s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
                   all         55        256      0.147      0.117       0.14      0.044
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08052 0.05072 0.02381 92 640:   0%|          | 0/19 [00:00<?, ?it/s]31/99 7.28G 0.08052 0.05072 0.02381 92 640:   5%|▌         | 1/19 [00:00<00:04,  3.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07512 0.04135 0.02239 47 640:   5%|▌         | 1/19 [00:00<00:04,  3.79it/s]31/99 7.28G 0.07512 0.04135 0.02239 47 640:  11%|█         | 2/19 [00:00<00:03,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07589 0.05102 0.02195 121 640:  11%|█         | 2/19 [00:00<00:03,  4.74it/s]31/99 7.28G 0.07589 0.05102 0.02195 121 640:  16%|█▌        | 3/19 [00:00<00:03,  5.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07773 0.05169 0.02236 123 640:  16%|█▌        | 3/19 [00:00<00:03,  5.15it/s]31/99 7.28G 0.07773 0.05169 0.02236 123 640:  21%|██        | 4/19 [00:00<00:02,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07811 0.05308 0.02291 105 640:  21%|██        | 4/19 [00:00<00:02,  5.35it/s]31/99 7.28G 0.07811 0.05308 0.02291 105 640:  26%|██▋       | 5/19 [00:00<00:02,  5.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07735 0.05196 0.02244 76 640:  26%|██▋       | 5/19 [00:01<00:02,  5.49it/s] 31/99 7.28G 0.07735 0.05196 0.02244 76 640:  32%|███▏      | 6/19 [00:01<00:02,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07802 0.05149 0.022 95 640:  32%|███▏      | 6/19 [00:01<00:02,  5.57it/s]  31/99 7.28G 0.07802 0.05149 0.022 95 640:  37%|███▋      | 7/19 [00:01<00:02,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07829 0.0502 0.02262 72 640:  37%|███▋      | 7/19 [00:01<00:02,  5.62it/s]31/99 7.28G 0.07829 0.0502 0.02262 72 640:  42%|████▏     | 8/19 [00:01<00:01,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0786 0.04945 0.02245 71 640:  42%|████▏     | 8/19 [00:10<00:01,  5.66it/s]31/99 7.28G 0.0786 0.04945 0.02245 71 640:  47%|████▋     | 9/19 [00:10<00:29,  2.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07728 0.04732 0.0218 47 640:  47%|████▋     | 9/19 [00:13<00:29,  2.98s/it]31/99 7.28G 0.07728 0.04732 0.0218 47 640:  53%|█████▎    | 10/19 [00:13<00:27,  3.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0768 0.04846 0.02169 96 640:  53%|█████▎    | 10/19 [00:19<00:27,  3.10s/it]31/99 7.28G 0.0768 0.04846 0.02169 96 640:  58%|█████▊    | 11/19 [00:19<00:29,  3.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07621 0.04775 0.02184 59 640:  58%|█████▊    | 11/19 [00:19<00:29,  3.71s/it]31/99 7.28G 0.07621 0.04775 0.02184 59 640:  63%|██████▎   | 12/19 [00:19<00:18,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07608 0.047 0.02154 63 640:  63%|██████▎   | 12/19 [00:19<00:18,  2.63s/it]  31/99 7.28G 0.07608 0.047 0.02154 63 640:  68%|██████▊   | 13/19 [00:19<00:11,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07665 0.04738 0.02176 103 640:  68%|██████▊   | 13/19 [00:20<00:11,  1.89s/it]31/99 7.28G 0.07665 0.04738 0.02176 103 640:  74%|███████▎  | 14/19 [00:20<00:08,  1.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07652 0.04819 0.0221 97 640:  74%|███████▎  | 14/19 [00:23<00:08,  1.77s/it]  31/99 7.28G 0.07652 0.04819 0.0221 97 640:  79%|███████▉  | 15/19 [00:23<00:07,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07654 0.04814 0.02206 76 640:  79%|███████▉  | 15/19 [00:24<00:07,  1.97s/it]31/99 7.28G 0.07654 0.04814 0.02206 76 640:  84%|████████▍ | 16/19 [00:24<00:04,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0764 0.0476 0.02206 64 640:  84%|████████▍ | 16/19 [00:31<00:04,  1.58s/it]  31/99 7.28G 0.0764 0.0476 0.02206 64 640:  89%|████████▉ | 17/19 [00:31<00:06,  3.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07659 0.04836 0.02206 100 640:  89%|████████▉ | 17/19 [00:37<00:06,  3.36s/it]31/99 7.28G 0.07659 0.04836 0.02206 100 640:  95%|█████████▍| 18/19 [00:37<00:04,  4.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.07658 0.04919 0.02196 107 640:  95%|█████████▍| 18/19 [00:41<00:04,  4.10s/it]31/99 7.28G 0.07658 0.04919 0.02196 107 640: 100%|██████████| 19/19 [00:41<00:00,  4.09s/it]31/99 7.28G 0.07658 0.04919 0.02196 107 640: 100%|██████████| 19/19 [00:41<00:00,  2.18s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:15<00:15, 15.46s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  7.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  8.75s/it]
                   all         55        256      0.227      0.187      0.145     0.0504
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08056 0.03168 0.02581 50 640:   0%|          | 0/19 [00:00<?, ?it/s]32/99 7.28G 0.08056 0.03168 0.02581 50 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08068 0.04944 0.02252 141 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]32/99 7.28G 0.08068 0.04944 0.02252 141 640:  11%|█         | 2/19 [00:00<00:07,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.0762 0.04156 0.0218 41 640:  11%|█         | 2/19 [00:01<00:07,  2.16it/s]   32/99 7.28G 0.0762 0.04156 0.0218 41 640:  16%|█▌        | 3/19 [00:01<00:07,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07338 0.03923 0.02021 46 640:  16%|█▌        | 3/19 [00:01<00:07,  2.05it/s]32/99 7.28G 0.07338 0.03923 0.02021 46 640:  21%|██        | 4/19 [00:01<00:07,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07444 0.04082 0.02057 96 640:  21%|██        | 4/19 [00:02<00:07,  2.00it/s]32/99 7.28G 0.07444 0.04082 0.02057 96 640:  26%|██▋       | 5/19 [00:02<00:06,  2.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07446 0.04006 0.02049 63 640:  26%|██▋       | 5/19 [00:02<00:06,  2.22it/s]32/99 7.28G 0.07446 0.04006 0.02049 63 640:  32%|███▏      | 6/19 [00:02<00:05,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07427 0.03995 0.02061 66 640:  32%|███▏      | 6/19 [00:03<00:05,  2.39it/s]32/99 7.28G 0.07427 0.03995 0.02061 66 640:  37%|███▋      | 7/19 [00:03<00:04,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07347 0.0385 0.02072 42 640:  37%|███▋      | 7/19 [00:03<00:04,  2.52it/s] 32/99 7.28G 0.07347 0.0385 0.02072 42 640:  42%|████▏     | 8/19 [00:03<00:04,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07427 0.04107 0.02059 121 640:  42%|████▏     | 8/19 [00:03<00:04,  2.61it/s]32/99 7.28G 0.07427 0.04107 0.02059 121 640:  47%|████▋     | 9/19 [00:03<00:03,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.0743 0.04065 0.02081 57 640:  47%|████▋     | 9/19 [00:04<00:03,  2.59it/s]  32/99 7.28G 0.0743 0.04065 0.02081 57 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07451 0.04238 0.02055 106 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.61it/s]32/99 7.28G 0.07451 0.04238 0.02055 106 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07398 0.042 0.02052 58 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.59it/s]   32/99 7.28G 0.07398 0.042 0.02052 58 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07463 0.0412 0.02052 68 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.60it/s]32/99 7.28G 0.07463 0.0412 0.02052 68 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07519 0.04052 0.02041 68 640:  68%|██████▊   | 13/19 [00:08<00:03,  1.95it/s]32/99 7.28G 0.07519 0.04052 0.02041 68 640:  74%|███████▎  | 14/19 [00:08<00:06,  1.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07565 0.04382 0.02057 157 640:  74%|███████▎  | 14/19 [00:16<00:06,  1.32s/it]32/99 7.28G 0.07565 0.04382 0.02057 157 640:  79%|███████▉  | 15/19 [00:16<00:12,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07612 0.04447 0.02053 115 640:  79%|███████▉  | 15/19 [00:20<00:12,  3.16s/it]32/99 7.28G 0.07612 0.04447 0.02053 115 640:  84%|████████▍ | 16/19 [00:20<00:10,  3.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07592 0.04481 0.02046 86 640:  84%|████████▍ | 16/19 [00:24<00:10,  3.33s/it] 32/99 7.28G 0.07592 0.04481 0.02046 86 640:  89%|████████▉ | 17/19 [00:24<00:07,  3.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07588 0.04524 0.02057 84 640:  89%|████████▉ | 17/19 [00:25<00:07,  3.57s/it]32/99 7.28G 0.07588 0.04524 0.02057 84 640:  95%|█████████▍| 18/19 [00:25<00:03,  3.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07606 0.0464 0.02059 127 640:  95%|█████████▍| 18/19 [00:26<00:03,  3.01s/it]32/99 7.28G 0.07606 0.0464 0.02059 127 640: 100%|██████████| 19/19 [00:26<00:00,  2.26s/it]32/99 7.28G 0.07606 0.0464 0.02059 127 640: 100%|██████████| 19/19 [00:26<00:00,  1.39s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.64s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.27s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.37s/it]
                   all         55        256      0.216      0.149       0.16     0.0519
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.06559 0.0255 0.01815 37 640:   0%|          | 0/19 [00:00<?, ?it/s]33/99 7.28G 0.06559 0.0255 0.01815 37 640:   5%|▌         | 1/19 [00:00<00:06,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.06967 0.03358 0.01844 72 640:   5%|▌         | 1/19 [00:00<00:06,  2.78it/s]33/99 7.28G 0.06967 0.03358 0.01844 72 640:  11%|█         | 2/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.06862 0.03538 0.01776 63 640:  11%|█         | 2/19 [00:01<00:06,  2.81it/s]33/99 7.28G 0.06862 0.03538 0.01776 63 640:  16%|█▌        | 3/19 [00:01<00:05,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.06951 0.03621 0.0185 59 640:  16%|█▌        | 3/19 [00:01<00:05,  2.81it/s] 33/99 7.28G 0.06951 0.03621 0.0185 59 640:  21%|██        | 4/19 [00:01<00:05,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07127 0.03873 0.01865 85 640:  21%|██        | 4/19 [00:01<00:05,  2.81it/s]33/99 7.28G 0.07127 0.03873 0.01865 85 640:  26%|██▋       | 5/19 [00:01<00:04,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07141 0.03966 0.01941 72 640:  26%|██▋       | 5/19 [00:01<00:04,  2.81it/s]33/99 7.28G 0.07141 0.03966 0.01941 72 640:  32%|███▏      | 6/19 [00:01<00:03,  3.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07132 0.04 0.01878 68 640:  32%|███▏      | 6/19 [00:02<00:03,  3.40it/s]   33/99 7.28G 0.07132 0.04 0.01878 68 640:  37%|███▋      | 7/19 [00:02<00:03,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07142 0.04068 0.01928 72 640:  37%|███▋      | 7/19 [00:02<00:03,  3.82it/s]33/99 7.28G 0.07142 0.04068 0.01928 72 640:  42%|████▏     | 8/19 [00:02<00:03,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07138 0.04129 0.01937 78 640:  42%|████▏     | 8/19 [00:02<00:03,  3.43it/s]33/99 7.28G 0.07138 0.04129 0.01937 78 640:  47%|████▋     | 9/19 [00:02<00:03,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07171 0.04047 0.01983 50 640:  47%|████▋     | 9/19 [00:03<00:03,  3.12it/s]33/99 7.28G 0.07171 0.04047 0.01983 50 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07235 0.04262 0.01998 111 640:  53%|█████▎    | 10/19 [00:06<00:03,  2.99it/s]33/99 7.28G 0.07235 0.04262 0.01998 111 640:  58%|█████▊    | 11/19 [00:06<00:10,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07252 0.04263 0.01999 72 640:  58%|█████▊    | 11/19 [00:12<00:10,  1.29s/it] 33/99 7.28G 0.07252 0.04263 0.01999 72 640:  63%|██████▎   | 12/19 [00:12<00:17,  2.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07251 0.04244 0.02029 67 640:  63%|██████▎   | 12/19 [00:15<00:17,  2.56s/it]33/99 7.28G 0.07251 0.04244 0.02029 67 640:  68%|██████▊   | 13/19 [00:15<00:16,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07319 0.04205 0.02036 69 640:  68%|██████▊   | 13/19 [00:18<00:16,  2.83s/it]33/99 7.28G 0.07319 0.04205 0.02036 69 640:  74%|███████▎  | 14/19 [00:18<00:14,  2.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07324 0.04241 0.02048 77 640:  74%|███████▎  | 14/19 [00:20<00:14,  2.90s/it]33/99 7.28G 0.07324 0.04241 0.02048 77 640:  79%|███████▉  | 15/19 [00:20<00:10,  2.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07365 0.04227 0.0205 71 640:  79%|███████▉  | 15/19 [00:21<00:10,  2.70s/it] 33/99 7.28G 0.07365 0.04227 0.0205 71 640:  84%|████████▍ | 16/19 [00:21<00:05,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07341 0.04179 0.02066 53 640:  84%|████████▍ | 16/19 [00:21<00:05,  1.98s/it]33/99 7.28G 0.07341 0.04179 0.02066 53 640:  89%|████████▉ | 17/19 [00:21<00:02,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07378 0.04368 0.02074 141 640:  89%|████████▉ | 17/19 [00:21<00:02,  1.45s/it]33/99 7.28G 0.07378 0.04368 0.02074 141 640:  95%|█████████▍| 18/19 [00:21<00:01,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07354 0.04335 0.02034 65 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.07s/it] 33/99 7.28G 0.07354 0.04335 0.02034 65 640: 100%|██████████| 19/19 [00:27<00:00,  2.45s/it]33/99 7.28G 0.07354 0.04335 0.02034 65 640: 100%|██████████| 19/19 [00:27<00:00,  1.44s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.96s/it]
                   all         55        256      0.291      0.284      0.189     0.0604
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.06997 0.02738 0.01809 41 640:   0%|          | 0/19 [00:00<?, ?it/s]34/99 7.28G 0.06997 0.02738 0.01809 41 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07588 0.0364 0.02015 78 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s] 34/99 7.28G 0.07588 0.0364 0.02015 78 640:  11%|█         | 2/19 [00:00<00:03,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07721 0.04094 0.01999 90 640:  11%|█         | 2/19 [00:00<00:03,  5.50it/s]34/99 7.28G 0.07721 0.04094 0.01999 90 640:  16%|█▌        | 3/19 [00:00<00:03,  5.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07617 0.03769 0.02071 42 640:  16%|█▌        | 3/19 [00:00<00:03,  5.26it/s]34/99 7.28G 0.07617 0.03769 0.02071 42 640:  21%|██        | 4/19 [00:00<00:03,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07731 0.04245 0.02088 112 640:  21%|██        | 4/19 [00:01<00:03,  3.92it/s]34/99 7.28G 0.07731 0.04245 0.02088 112 640:  26%|██▋       | 5/19 [00:01<00:04,  3.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07658 0.04452 0.02078 95 640:  26%|██▋       | 5/19 [00:01<00:04,  3.31it/s] 34/99 7.28G 0.07658 0.04452 0.02078 95 640:  32%|███▏      | 6/19 [00:01<00:04,  3.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07644 0.04382 0.02117 68 640:  32%|███▏      | 6/19 [00:02<00:04,  3.05it/s]34/99 7.28G 0.07644 0.04382 0.02117 68 640:  37%|███▋      | 7/19 [00:02<00:04,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07643 0.04204 0.02156 40 640:  37%|███▋      | 7/19 [00:02<00:04,  2.92it/s]34/99 7.28G 0.07643 0.04204 0.02156 40 640:  42%|████▏     | 8/19 [00:02<00:03,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.0758 0.04278 0.02153 77 640:  42%|████▏     | 8/19 [00:07<00:03,  2.80it/s] 34/99 7.28G 0.0758 0.04278 0.02153 77 640:  47%|████▋     | 9/19 [00:07<00:18,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07519 0.0425 0.02116 65 640:  47%|████▋     | 9/19 [00:09<00:18,  1.85s/it]34/99 7.28G 0.07519 0.0425 0.02116 65 640:  53%|█████▎    | 10/19 [00:09<00:16,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07501 0.04201 0.02073 63 640:  53%|█████▎    | 10/19 [00:12<00:16,  1.85s/it]34/99 7.28G 0.07501 0.04201 0.02073 63 640:  58%|█████▊    | 11/19 [00:12<00:17,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07476 0.04087 0.02145 41 640:  58%|█████▊    | 11/19 [00:14<00:17,  2.17s/it]34/99 7.28G 0.07476 0.04087 0.02145 41 640:  63%|██████▎   | 12/19 [00:14<00:14,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07552 0.04112 0.02147 92 640:  63%|██████▎   | 12/19 [00:14<00:14,  2.12s/it]34/99 7.28G 0.07552 0.04112 0.02147 92 640:  68%|██████▊   | 13/19 [00:14<00:09,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07617 0.04128 0.02146 80 640:  68%|██████▊   | 13/19 [00:14<00:09,  1.53s/it]34/99 7.28G 0.07617 0.04128 0.02146 80 640:  74%|███████▎  | 14/19 [00:14<00:05,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07625 0.04133 0.0212 77 640:  74%|███████▎  | 14/19 [00:15<00:05,  1.19s/it] 34/99 7.28G 0.07625 0.04133 0.0212 77 640:  79%|███████▉  | 15/19 [00:15<00:03,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07663 0.04108 0.02116 78 640:  79%|███████▉  | 15/19 [00:21<00:03,  1.01it/s]34/99 7.28G 0.07663 0.04108 0.02116 78 640:  84%|████████▍ | 16/19 [00:21<00:07,  2.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07655 0.04185 0.02105 92 640:  84%|████████▍ | 16/19 [00:29<00:07,  2.54s/it]34/99 7.28G 0.07655 0.04185 0.02105 92 640:  89%|████████▉ | 17/19 [00:29<00:08,  4.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.0769 0.04275 0.02112 115 640:  89%|████████▉ | 17/19 [00:33<00:08,  4.17s/it]34/99 7.28G 0.0769 0.04275 0.02112 115 640:  95%|█████████▍| 18/19 [00:33<00:04,  4.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.07702 0.04347 0.02104 99 640:  95%|█████████▍| 18/19 [00:37<00:04,  4.03s/it]34/99 7.28G 0.07702 0.04347 0.02104 99 640: 100%|██████████| 19/19 [00:37<00:00,  4.08s/it]34/99 7.28G 0.07702 0.04347 0.02104 99 640: 100%|██████████| 19/19 [00:37<00:00,  1.97s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:18<00:18, 18.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:21<00:00,  9.64s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:21<00:00, 10.94s/it]
                   all         55        256      0.271      0.242      0.181     0.0574
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07458 0.04662 0.02225 77 640:   0%|          | 0/19 [00:00<?, ?it/s]35/99 7.28G 0.07458 0.04662 0.02225 77 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07432 0.05814 0.02188 114 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s]35/99 7.28G 0.07432 0.05814 0.02188 114 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07586 0.05507 0.0218 83 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]  35/99 7.28G 0.07586 0.05507 0.0218 83 640:  16%|█▌        | 3/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07659 0.05375 0.02179 85 640:  16%|█▌        | 3/19 [00:00<00:02,  5.67it/s]35/99 7.28G 0.07659 0.05375 0.02179 85 640:  21%|██        | 4/19 [00:00<00:02,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07911 0.05701 0.02121 184 640:  21%|██        | 4/19 [00:00<00:02,  5.59it/s]35/99 7.28G 0.07911 0.05701 0.02121 184 640:  26%|██▋       | 5/19 [00:00<00:02,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07838 0.05342 0.02168 58 640:  26%|██▋       | 5/19 [00:01<00:02,  5.51it/s] 35/99 7.28G 0.07838 0.05342 0.02168 58 640:  32%|███▏      | 6/19 [00:01<00:03,  3.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07773 0.05108 0.02122 72 640:  32%|███▏      | 6/19 [00:01<00:03,  3.94it/s]35/99 7.28G 0.07773 0.05108 0.02122 72 640:  37%|███▋      | 7/19 [00:01<00:04,  2.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07723 0.05031 0.02109 73 640:  37%|███▋      | 7/19 [00:02<00:04,  2.53it/s]35/99 7.28G 0.07723 0.05031 0.02109 73 640:  42%|████▏     | 8/19 [00:02<00:04,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07698 0.05028 0.02123 78 640:  42%|████▏     | 8/19 [00:03<00:04,  2.29it/s]35/99 7.28G 0.07698 0.05028 0.02123 78 640:  47%|████▋     | 9/19 [00:03<00:04,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07695 0.0501 0.0212 97 640:  47%|████▋     | 9/19 [00:03<00:04,  2.18it/s]  35/99 7.28G 0.07695 0.0501 0.0212 97 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07748 0.04969 0.02106 85 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.06it/s]35/99 7.28G 0.07748 0.04969 0.02106 85 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07741 0.04993 0.02079 98 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.85it/s]35/99 7.28G 0.07741 0.04993 0.02079 98 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07761 0.05042 0.02078 92 640:  63%|██████▎   | 12/19 [00:06<00:04,  1.53it/s]35/99 7.28G 0.07761 0.05042 0.02078 92 640:  68%|██████▊   | 13/19 [00:06<00:04,  1.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07745 0.04926 0.02037 62 640:  68%|██████▊   | 13/19 [00:08<00:04,  1.30it/s]35/99 7.28G 0.07745 0.04926 0.02037 62 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07777 0.05091 0.02068 138 640:  74%|███████▎  | 14/19 [00:10<00:05,  1.11s/it]35/99 7.28G 0.07777 0.05091 0.02068 138 640:  79%|███████▉  | 15/19 [00:10<00:06,  1.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07748 0.04993 0.02094 53 640:  79%|███████▉  | 15/19 [00:14<00:06,  1.65s/it] 35/99 7.28G 0.07748 0.04993 0.02094 53 640:  84%|████████▍ | 16/19 [00:14<00:06,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07771 0.04994 0.02145 89 640:  84%|████████▍ | 16/19 [00:20<00:06,  2.28s/it]35/99 7.28G 0.07771 0.04994 0.02145 89 640:  89%|████████▉ | 17/19 [00:20<00:06,  3.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07737 0.04993 0.02117 82 640:  89%|████████▉ | 17/19 [00:22<00:06,  3.48s/it]35/99 7.28G 0.07737 0.04993 0.02117 82 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.07693 0.04953 0.02108 70 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.81s/it]35/99 7.28G 0.07693 0.04953 0.02108 70 640: 100%|██████████| 19/19 [00:22<00:00,  2.05s/it]35/99 7.28G 0.07693 0.04953 0.02108 70 640: 100%|██████████| 19/19 [00:22<00:00,  1.19s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:17<00:17, 17.22s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  7.50s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  8.96s/it]
                   all         55        256      0.206      0.152      0.204     0.0606
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07711 0.0492 0.02094 85 640:   0%|          | 0/19 [00:00<?, ?it/s]36/99 7.28G 0.07711 0.0492 0.02094 85 640:   5%|▌         | 1/19 [00:00<00:03,  5.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07797 0.05758 0.01941 118 640:   5%|▌         | 1/19 [00:00<00:03,  5.16it/s]36/99 7.28G 0.07797 0.05758 0.01941 118 640:  11%|█         | 2/19 [00:00<00:03,  5.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07854 0.05299 0.01824 95 640:  11%|█         | 2/19 [00:00<00:03,  5.47it/s] 36/99 7.28G 0.07854 0.05299 0.01824 95 640:  16%|█▌        | 3/19 [00:00<00:02,  5.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07678 0.04769 0.01845 59 640:  16%|█▌        | 3/19 [00:00<00:02,  5.58it/s]36/99 7.28G 0.07678 0.04769 0.01845 59 640:  21%|██        | 4/19 [00:00<00:03,  4.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07684 0.04876 0.01834 101 640:  21%|██        | 4/19 [00:01<00:03,  4.96it/s]36/99 7.28G 0.07684 0.04876 0.01834 101 640:  26%|██▋       | 5/19 [00:01<00:03,  4.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07756 0.04671 0.01882 63 640:  26%|██▋       | 5/19 [00:01<00:03,  4.31it/s] 36/99 7.28G 0.07756 0.04671 0.01882 63 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0765 0.04452 0.01856 47 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s] 36/99 7.28G 0.0765 0.04452 0.01856 47 640:  37%|███▋      | 7/19 [00:01<00:02,  5.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07635 0.04403 0.01878 73 640:  37%|███▋      | 7/19 [00:01<00:02,  5.01it/s]36/99 7.28G 0.07635 0.04403 0.01878 73 640:  42%|████▏     | 8/19 [00:01<00:02,  5.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07651 0.04418 0.01919 77 640:  42%|████▏     | 8/19 [00:01<00:02,  5.13it/s]36/99 7.28G 0.07651 0.04418 0.01919 77 640:  47%|████▋     | 9/19 [00:01<00:02,  4.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07491 0.0426 0.01893 42 640:  47%|████▋     | 9/19 [00:03<00:02,  4.83it/s] 36/99 7.28G 0.07491 0.0426 0.01893 42 640:  53%|█████▎    | 10/19 [00:03<00:06,  1.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07517 0.04331 0.01918 95 640:  53%|█████▎    | 10/19 [00:08<00:06,  1.48it/s]36/99 7.28G 0.07517 0.04331 0.01918 95 640:  58%|█████▊    | 11/19 [00:08<00:16,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07558 0.04548 0.01911 129 640:  58%|█████▊    | 11/19 [00:10<00:16,  2.10s/it]36/99 7.28G 0.07558 0.04548 0.01911 129 640:  63%|██████▎   | 12/19 [00:10<00:14,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07526 0.04443 0.01905 50 640:  63%|██████▎   | 12/19 [00:13<00:14,  2.10s/it] 36/99 7.28G 0.07526 0.04443 0.01905 50 640:  68%|██████▊   | 13/19 [00:13<00:14,  2.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07478 0.04356 0.01921 47 640:  68%|██████▊   | 13/19 [00:18<00:14,  2.36s/it]36/99 7.28G 0.07478 0.04356 0.01921 47 640:  74%|███████▎  | 14/19 [00:18<00:15,  3.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07491 0.04498 0.01924 107 640:  74%|███████▎  | 14/19 [00:19<00:15,  3.03s/it]36/99 7.28G 0.07491 0.04498 0.01924 107 640:  79%|███████▉  | 15/19 [00:19<00:09,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07481 0.04544 0.01913 86 640:  79%|███████▉  | 15/19 [00:20<00:09,  2.28s/it] 36/99 7.28G 0.07481 0.04544 0.01913 86 640:  84%|████████▍ | 16/19 [00:20<00:06,  2.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07488 0.04496 0.0191 59 640:  84%|████████▍ | 16/19 [00:20<00:06,  2.03s/it] 36/99 7.28G 0.07488 0.04496 0.0191 59 640:  89%|████████▉ | 17/19 [00:20<00:02,  1.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07471 0.04473 0.01908 63 640:  89%|████████▉ | 17/19 [00:22<00:02,  1.47s/it]36/99 7.28G 0.07471 0.04473 0.01908 63 640:  95%|█████████▍| 18/19 [00:22<00:01,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.07502 0.04718 0.01912 166 640:  95%|█████████▍| 18/19 [00:29<00:01,  1.64s/it]36/99 7.28G 0.07502 0.04718 0.01912 166 640: 100%|██████████| 19/19 [00:29<00:00,  3.14s/it]36/99 7.28G 0.07502 0.04718 0.01912 166 640: 100%|██████████| 19/19 [00:29<00:00,  1.55s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.77s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.23s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.51s/it]
                   all         55        256      0.258      0.232      0.168      0.051
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.0673 0.0353 0.01766 56 640:   0%|          | 0/19 [00:01<?, ?it/s]37/99 7.28G 0.0673 0.0353 0.01766 56 640:   5%|▌         | 1/19 [00:01<00:19,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07197 0.04087 0.01894 89 640:   5%|▌         | 1/19 [00:02<00:19,  1.08s/it]37/99 7.28G 0.07197 0.04087 0.01894 89 640:  11%|█         | 2/19 [00:02<00:19,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07076 0.03689 0.01913 49 640:  11%|█         | 2/19 [00:03<00:19,  1.16s/it]37/99 7.28G 0.07076 0.03689 0.01913 49 640:  16%|█▌        | 3/19 [00:03<00:16,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07337 0.04562 0.01873 142 640:  16%|█▌        | 3/19 [00:04<00:16,  1.03s/it]37/99 7.28G 0.07337 0.04562 0.01873 142 640:  21%|██        | 4/19 [00:04<00:15,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07474 0.04423 0.02002 68 640:  21%|██        | 4/19 [00:05<00:15,  1.03s/it] 37/99 7.28G 0.07474 0.04423 0.02002 68 640:  26%|██▋       | 5/19 [00:05<00:14,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07675 0.04552 0.02029 110 640:  26%|██▋       | 5/19 [00:06<00:14,  1.04s/it]37/99 7.28G 0.07675 0.04552 0.02029 110 640:  32%|███▏      | 6/19 [00:06<00:14,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07596 0.04639 0.02072 80 640:  32%|███▏      | 6/19 [00:07<00:14,  1.08s/it] 37/99 7.28G 0.07596 0.04639 0.02072 80 640:  37%|███▋      | 7/19 [00:07<00:12,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.0765 0.04833 0.02036 114 640:  37%|███▋      | 7/19 [00:08<00:12,  1.05s/it]37/99 7.28G 0.0765 0.04833 0.02036 114 640:  42%|████▏     | 8/19 [00:08<00:11,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07637 0.04786 0.0206 73 640:  42%|████▏     | 8/19 [00:09<00:11,  1.04s/it] 37/99 7.28G 0.07637 0.04786 0.0206 73 640:  47%|████▋     | 9/19 [00:09<00:11,  1.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07655 0.04781 0.02046 86 640:  47%|████▋     | 9/19 [00:10<00:11,  1.12s/it]37/99 7.28G 0.07655 0.04781 0.02046 86 640:  53%|█████▎    | 10/19 [00:10<00:09,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07639 0.04716 0.0204 68 640:  53%|█████▎    | 10/19 [00:11<00:09,  1.07s/it] 37/99 7.28G 0.07639 0.04716 0.0204 68 640:  58%|█████▊    | 11/19 [00:11<00:07,  1.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07677 0.04786 0.02046 114 640:  58%|█████▊    | 11/19 [00:11<00:07,  1.06it/s]37/99 7.28G 0.07677 0.04786 0.02046 114 640:  63%|██████▎   | 12/19 [00:11<00:05,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07668 0.04725 0.02031 71 640:  63%|██████▎   | 12/19 [00:12<00:05,  1.22it/s] 37/99 7.28G 0.07668 0.04725 0.02031 71 640:  68%|██████▊   | 13/19 [00:12<00:04,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07673 0.04734 0.0204 86 640:  68%|██████▊   | 13/19 [00:13<00:04,  1.22it/s] 37/99 7.28G 0.07673 0.04734 0.0204 86 640:  74%|███████▎  | 14/19 [00:13<00:03,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07689 0.04831 0.02037 104 640:  74%|███████▎  | 14/19 [00:15<00:03,  1.39it/s]37/99 7.28G 0.07689 0.04831 0.02037 104 640:  79%|███████▉  | 15/19 [00:15<00:04,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07654 0.04783 0.02038 61 640:  79%|███████▉  | 15/19 [00:21<00:04,  1.06s/it] 37/99 7.28G 0.07654 0.04783 0.02038 61 640:  84%|████████▍ | 16/19 [00:21<00:08,  2.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07648 0.04758 0.0205 84 640:  84%|████████▍ | 16/19 [00:25<00:08,  2.68s/it] 37/99 7.28G 0.07648 0.04758 0.0205 84 640:  89%|████████▉ | 17/19 [00:25<00:06,  3.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07672 0.04825 0.02026 117 640:  89%|████████▉ | 17/19 [00:30<00:06,  3.01s/it]37/99 7.28G 0.07672 0.04825 0.02026 117 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.0768 0.04787 0.02016 86 640:  95%|█████████▍| 18/19 [00:36<00:03,  3.71s/it]  37/99 7.28G 0.0768 0.04787 0.02016 86 640: 100%|██████████| 19/19 [00:36<00:00,  4.49s/it]37/99 7.28G 0.0768 0.04787 0.02016 86 640: 100%|██████████| 19/19 [00:36<00:00,  1.94s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  7.40s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.14s/it]
                   all         55        256      0.255      0.231      0.169     0.0512
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.0763 0.03869 0.02157 63 640:   0%|          | 0/19 [00:00<?, ?it/s]38/99 7.28G 0.0763 0.03869 0.02157 63 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07842 0.04465 0.02198 94 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]38/99 7.28G 0.07842 0.04465 0.02198 94 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07431 0.04187 0.02154 57 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]38/99 7.28G 0.07431 0.04187 0.02154 57 640:  16%|█▌        | 3/19 [00:00<00:04,  3.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07318 0.03968 0.02076 52 640:  16%|█▌        | 3/19 [00:01<00:04,  3.91it/s]38/99 7.28G 0.07318 0.03968 0.02076 52 640:  21%|██        | 4/19 [00:01<00:05,  2.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.0748 0.03929 0.02011 71 640:  21%|██        | 4/19 [00:01<00:05,  2.94it/s] 38/99 7.28G 0.0748 0.03929 0.02011 71 640:  26%|██▋       | 5/19 [00:01<00:04,  3.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07437 0.03931 0.02052 66 640:  26%|██▋       | 5/19 [00:01<00:04,  3.32it/s]38/99 7.28G 0.07437 0.03931 0.02052 66 640:  32%|███▏      | 6/19 [00:01<00:03,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07449 0.04019 0.02053 71 640:  32%|███▏      | 6/19 [00:01<00:03,  3.88it/s]38/99 7.28G 0.07449 0.04019 0.02053 71 640:  37%|███▋      | 7/19 [00:01<00:03,  3.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07409 0.03982 0.02034 59 640:  37%|███▋      | 7/19 [00:02<00:03,  3.47it/s]38/99 7.28G 0.07409 0.03982 0.02034 59 640:  42%|████▏     | 8/19 [00:02<00:02,  3.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07476 0.04028 0.02014 83 640:  42%|████▏     | 8/19 [00:02<00:02,  3.95it/s]38/99 7.28G 0.07476 0.04028 0.02014 83 640:  47%|████▋     | 9/19 [00:02<00:02,  4.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07532 0.04217 0.02045 108 640:  47%|████▋     | 9/19 [00:02<00:02,  4.38it/s]38/99 7.28G 0.07532 0.04217 0.02045 108 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07435 0.04028 0.02036 33 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.56it/s] 38/99 7.28G 0.07435 0.04028 0.02036 33 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07361 0.03933 0.02022 46 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.97it/s]38/99 7.28G 0.07361 0.03933 0.02022 46 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07401 0.03966 0.02086 79 640:  63%|██████▎   | 12/19 [00:06<00:01,  4.08it/s]38/99 7.28G 0.07401 0.03966 0.02086 79 640:  68%|██████▊   | 13/19 [00:06<00:06,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07424 0.03964 0.02051 77 640:  68%|██████▊   | 13/19 [00:09<00:06,  1.11s/it]38/99 7.28G 0.07424 0.03964 0.02051 77 640:  74%|███████▎  | 14/19 [00:09<00:09,  1.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.0746 0.03979 0.02041 88 640:  74%|███████▎  | 14/19 [00:13<00:09,  1.83s/it] 38/99 7.28G 0.0746 0.03979 0.02041 88 640:  79%|███████▉  | 15/19 [00:13<00:09,  2.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07404 0.03915 0.02005 45 640:  79%|███████▉  | 15/19 [00:18<00:09,  2.47s/it]38/99 7.28G 0.07404 0.03915 0.02005 45 640:  84%|████████▍ | 16/19 [00:18<00:09,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07426 0.03901 0.02014 64 640:  84%|████████▍ | 16/19 [00:20<00:09,  3.16s/it]38/99 7.28G 0.07426 0.03901 0.02014 64 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07382 0.0386 0.0202 47 640:  89%|████████▉ | 17/19 [00:21<00:05,  2.98s/it]  38/99 7.28G 0.07382 0.0386 0.0202 47 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07363 0.03796 0.02012 40 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.14s/it]38/99 7.28G 0.07363 0.03796 0.02012 40 640: 100%|██████████| 19/19 [00:21<00:00,  1.55s/it]38/99 7.28G 0.07363 0.03796 0.02012 40 640: 100%|██████████| 19/19 [00:21<00:00,  1.12s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  5.85s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]
                   all         55        256      0.262      0.252       0.17     0.0508
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07416 0.03611 0.01831 62 640:   0%|          | 0/19 [00:00<?, ?it/s]39/99 7.3G 0.07416 0.03611 0.01831 62 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07302 0.03593 0.01804 60 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]39/99 7.3G 0.07302 0.03593 0.01804 60 640:  11%|█         | 2/19 [00:00<00:03,  5.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07554 0.04341 0.01989 102 640:  11%|█         | 2/19 [00:00<00:03,  5.24it/s]39/99 7.3G 0.07554 0.04341 0.01989 102 640:  16%|█▌        | 3/19 [00:00<00:05,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07442 0.03992 0.0194 49 640:  16%|█▌        | 3/19 [00:01<00:05,  2.93it/s]  39/99 7.3G 0.07442 0.03992 0.0194 49 640:  21%|██        | 4/19 [00:01<00:04,  3.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07433 0.04078 0.01949 73 640:  21%|██        | 4/19 [00:01<00:04,  3.63it/s]39/99 7.3G 0.07433 0.04078 0.01949 73 640:  26%|██▋       | 5/19 [00:01<00:03,  4.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07478 0.04421 0.0196 112 640:  26%|██▋       | 5/19 [00:01<00:03,  4.18it/s]39/99 7.3G 0.07478 0.04421 0.0196 112 640:  32%|███▏      | 6/19 [00:01<00:03,  4.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07457 0.04365 0.01957 67 640:  32%|███▏      | 6/19 [00:01<00:03,  4.09it/s]39/99 7.3G 0.07457 0.04365 0.01957 67 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07478 0.04278 0.01982 64 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]39/99 7.3G 0.07478 0.04278 0.01982 64 640:  42%|████▏     | 8/19 [00:01<00:02,  4.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07441 0.04327 0.01972 82 640:  42%|████▏     | 8/19 [00:02<00:02,  4.39it/s]39/99 7.3G 0.07441 0.04327 0.01972 82 640:  47%|████▋     | 9/19 [00:02<00:03,  3.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07502 0.04305 0.02002 79 640:  47%|████▋     | 9/19 [00:09<00:03,  3.06it/s]39/99 7.3G 0.07502 0.04305 0.02002 79 640:  53%|█████▎    | 10/19 [00:09<00:20,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07539 0.04393 0.02011 99 640:  53%|█████▎    | 10/19 [00:11<00:20,  2.28s/it]39/99 7.3G 0.07539 0.04393 0.02011 99 640:  58%|█████▊    | 11/19 [00:11<00:18,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07473 0.04396 0.02105 68 640:  58%|█████▊    | 11/19 [00:15<00:18,  2.37s/it]39/99 7.3G 0.07473 0.04396 0.02105 68 640:  63%|██████▎   | 12/19 [00:15<00:18,  2.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07386 0.04337 0.02056 52 640:  63%|██████▎   | 12/19 [00:19<00:18,  2.71s/it]39/99 7.3G 0.07386 0.04337 0.02056 52 640:  68%|██████▊   | 13/19 [00:19<00:19,  3.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07483 0.04547 0.02057 161 640:  68%|██████▊   | 13/19 [00:21<00:19,  3.28s/it]39/99 7.3G 0.07483 0.04547 0.02057 161 640:  74%|███████▎  | 14/19 [00:21<00:13,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07456 0.04645 0.02053 97 640:  74%|███████▎  | 14/19 [00:21<00:13,  2.69s/it] 39/99 7.3G 0.07456 0.04645 0.02053 97 640:  79%|███████▉  | 15/19 [00:21<00:07,  1.94s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07412 0.04526 0.0203 41 640:  79%|███████▉  | 15/19 [00:21<00:07,  1.94s/it] 39/99 7.3G 0.07412 0.04526 0.0203 41 640:  84%|████████▍ | 16/19 [00:21<00:04,  1.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07398 0.04582 0.02005 91 640:  84%|████████▍ | 16/19 [00:23<00:04,  1.46s/it]39/99 7.3G 0.07398 0.04582 0.02005 91 640:  89%|████████▉ | 17/19 [00:23<00:02,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07392 0.04571 0.01991 80 640:  89%|████████▉ | 17/19 [00:30<00:02,  1.44s/it]39/99 7.3G 0.07392 0.04571 0.01991 80 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.3G 0.07352 0.04555 0.01983 72 640:  95%|█████████▍| 18/19 [00:33<00:03,  3.22s/it]39/99 7.3G 0.07352 0.04555 0.01983 72 640: 100%|██████████| 19/19 [00:33<00:00,  3.28s/it]39/99 7.3G 0.07352 0.04555 0.01983 72 640: 100%|██████████| 19/19 [00:33<00:00,  1.78s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]
                   all         55        256      0.211      0.238      0.143     0.0526
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07333 0.05253 0.02267 93 640:   0%|          | 0/19 [00:00<?, ?it/s]40/99 7.3G 0.07333 0.05253 0.02267 93 640:   5%|▌         | 1/19 [00:00<00:10,  1.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07332 0.04638 0.02289 64 640:   5%|▌         | 1/19 [00:01<00:10,  1.77it/s]40/99 7.3G 0.07332 0.04638 0.02289 64 640:  11%|█         | 2/19 [00:01<00:14,  1.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.0755 0.04934 0.02187 106 640:  11%|█         | 2/19 [00:02<00:14,  1.20it/s]40/99 7.3G 0.0755 0.04934 0.02187 106 640:  16%|█▌        | 3/19 [00:02<00:16,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07452 0.04542 0.02148 56 640:  16%|█▌        | 3/19 [00:03<00:16,  1.04s/it]40/99 7.3G 0.07452 0.04542 0.02148 56 640:  21%|██        | 4/19 [00:03<00:15,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07446 0.046 0.02236 73 640:  21%|██        | 4/19 [00:05<00:15,  1.05s/it]  40/99 7.3G 0.07446 0.046 0.02236 73 640:  26%|██▋       | 5/19 [00:05<00:14,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07523 0.04899 0.02331 103 640:  26%|██▋       | 5/19 [00:06<00:14,  1.06s/it]40/99 7.3G 0.07523 0.04899 0.02331 103 640:  32%|███▏      | 6/19 [00:06<00:13,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07571 0.04725 0.02261 72 640:  32%|███▏      | 6/19 [00:06<00:13,  1.04s/it] 40/99 7.3G 0.07571 0.04725 0.02261 72 640:  37%|███▋      | 7/19 [00:06<00:11,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07496 0.04584 0.02219 52 640:  37%|███▋      | 7/19 [00:07<00:11,  1.07it/s]40/99 7.3G 0.07496 0.04584 0.02219 52 640:  42%|████▏     | 8/19 [00:07<00:08,  1.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07462 0.04657 0.02188 89 640:  42%|████▏     | 8/19 [00:07<00:08,  1.24it/s]40/99 7.3G 0.07462 0.04657 0.02188 89 640:  47%|████▋     | 9/19 [00:07<00:06,  1.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.0743 0.04533 0.02167 54 640:  47%|████▋     | 9/19 [00:08<00:06,  1.44it/s] 40/99 7.3G 0.0743 0.04533 0.02167 54 640:  53%|█████▎    | 10/19 [00:08<00:06,  1.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07325 0.0446 0.0218 55 640:  53%|█████▎    | 10/19 [00:10<00:06,  1.41it/s] 40/99 7.3G 0.07325 0.0446 0.0218 55 640:  58%|█████▊    | 11/19 [00:10<00:08,  1.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07307 0.04401 0.02181 61 640:  58%|█████▊    | 11/19 [00:10<00:08,  1.12s/it]40/99 7.3G 0.07307 0.04401 0.02181 61 640:  63%|██████▎   | 12/19 [00:10<00:05,  1.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07306 0.04451 0.02153 85 640:  63%|██████▎   | 12/19 [00:10<00:05,  1.18it/s]40/99 7.3G 0.07306 0.04451 0.02153 85 640:  68%|██████▊   | 13/19 [00:10<00:03,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07362 0.04594 0.02151 108 640:  68%|██████▊   | 13/19 [00:12<00:03,  1.53it/s]40/99 7.3G 0.07362 0.04594 0.02151 108 640:  74%|███████▎  | 14/19 [00:12<00:05,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07354 0.04442 0.02121 37 640:  74%|███████▎  | 14/19 [00:17<00:05,  1.03s/it] 40/99 7.3G 0.07354 0.04442 0.02121 37 640:  79%|███████▉  | 15/19 [00:17<00:08,  2.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07384 0.04589 0.02089 143 640:  79%|███████▉  | 15/19 [00:23<00:08,  2.08s/it]40/99 7.3G 0.07384 0.04589 0.02089 143 640:  84%|████████▍ | 16/19 [00:23<00:09,  3.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.0746 0.04772 0.0207 169 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.27s/it]  40/99 7.3G 0.0746 0.04772 0.0207 169 640:  89%|████████▉ | 17/19 [00:25<00:05,  2.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07476 0.04775 0.02091 90 640:  89%|████████▉ | 17/19 [00:32<00:05,  2.92s/it]40/99 7.3G 0.07476 0.04775 0.02091 90 640:  95%|█████████▍| 18/19 [00:32<00:04,  4.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.3G 0.07501 0.04779 0.02081 106 640:  95%|█████████▍| 18/19 [00:37<00:04,  4.07s/it]40/99 7.3G 0.07501 0.04779 0.02081 106 640: 100%|██████████| 19/19 [00:37<00:00,  4.30s/it]40/99 7.3G 0.07501 0.04779 0.02081 106 640: 100%|██████████| 19/19 [00:37<00:00,  1.95s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:15<00:15, 15.79s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  7.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  8.58s/it]
                   all         55        256       0.24      0.218      0.154     0.0574
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07255 0.03353 0.01677 58 640:   0%|          | 0/19 [00:00<?, ?it/s]41/99 7.3G 0.07255 0.03353 0.01677 58 640:   5%|▌         | 1/19 [00:00<00:05,  3.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07709 0.04784 0.01623 123 640:   5%|▌         | 1/19 [00:00<00:05,  3.31it/s]41/99 7.3G 0.07709 0.04784 0.01623 123 640:  11%|█         | 2/19 [00:00<00:05,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07894 0.04947 0.01946 103 640:  11%|█         | 2/19 [00:01<00:05,  3.11it/s]41/99 7.3G 0.07894 0.04947 0.01946 103 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07671 0.04543 0.02024 53 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s] 41/99 7.3G 0.07671 0.04543 0.02024 53 640:  21%|██        | 4/19 [00:01<00:07,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07618 0.04624 0.02057 87 640:  21%|██        | 4/19 [00:02<00:07,  1.98it/s]41/99 7.3G 0.07618 0.04624 0.02057 87 640:  26%|██▋       | 5/19 [00:02<00:07,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07657 0.04715 0.02066 100 640:  26%|██▋       | 5/19 [00:02<00:07,  1.81it/s]41/99 7.3G 0.07657 0.04715 0.02066 100 640:  32%|███▏      | 6/19 [00:02<00:05,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.0751 0.04544 0.02027 53 640:  32%|███▏      | 6/19 [00:02<00:05,  2.29it/s]  41/99 7.3G 0.0751 0.04544 0.02027 53 640:  37%|███▋      | 7/19 [00:02<00:04,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07582 0.04623 0.02038 107 640:  37%|███▋      | 7/19 [00:02<00:04,  2.84it/s]41/99 7.3G 0.07582 0.04623 0.02038 107 640:  42%|████▏     | 8/19 [00:02<00:03,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07595 0.04717 0.02027 93 640:  42%|████▏     | 8/19 [00:03<00:03,  3.39it/s] 41/99 7.3G 0.07595 0.04717 0.02027 93 640:  47%|████▋     | 9/19 [00:03<00:02,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07517 0.04593 0.02052 56 640:  47%|████▋     | 9/19 [00:03<00:02,  3.88it/s]41/99 7.3G 0.07517 0.04593 0.02052 56 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.0749 0.04532 0.02096 57 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.89it/s] 41/99 7.3G 0.0749 0.04532 0.02096 57 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07549 0.04696 0.02084 138 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.84it/s]41/99 7.3G 0.07549 0.04696 0.02084 138 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07537 0.04743 0.0204 104 640:  63%|██████▎   | 12/19 [00:08<00:01,  4.27it/s] 41/99 7.3G 0.07537 0.04743 0.0204 104 640:  68%|██████▊   | 13/19 [00:08<00:09,  1.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07555 0.04718 0.02033 82 640:  68%|██████▊   | 13/19 [00:10<00:09,  1.66s/it]41/99 7.3G 0.07555 0.04718 0.02033 82 640:  74%|███████▎  | 14/19 [00:10<00:07,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.0757 0.04744 0.02034 92 640:  74%|███████▎  | 14/19 [00:14<00:07,  1.53s/it] 41/99 7.3G 0.0757 0.04744 0.02034 92 640:  79%|███████▉  | 15/19 [00:14<00:09,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07536 0.04714 0.02039 63 640:  79%|███████▉  | 15/19 [00:17<00:09,  2.39s/it]41/99 7.3G 0.07536 0.04714 0.02039 63 640:  84%|████████▍ | 16/19 [00:17<00:07,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07495 0.04711 0.02046 71 640:  84%|████████▍ | 16/19 [00:20<00:07,  2.45s/it]41/99 7.3G 0.07495 0.04711 0.02046 71 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07496 0.04747 0.02045 84 640:  89%|████████▉ | 17/19 [00:21<00:05,  2.82s/it]41/99 7.3G 0.07496 0.04747 0.02045 84 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07492 0.0474 0.02024 79 640:  95%|█████████▍| 18/19 [00:23<00:02,  2.19s/it] 41/99 7.3G 0.07492 0.0474 0.02024 79 640: 100%|██████████| 19/19 [00:23<00:00,  2.10s/it]41/99 7.3G 0.07492 0.0474 0.02024 79 640: 100%|██████████| 19/19 [00:23<00:00,  1.23s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.71s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]
                   all         55        256      0.218      0.227      0.155     0.0594
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.06862 0.03521 0.02031 55 640:   0%|          | 0/19 [00:00<?, ?it/s]42/99 7.3G 0.06862 0.03521 0.02031 55 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07039 0.03969 0.02065 76 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]42/99 7.3G 0.07039 0.03969 0.02065 76 640:  11%|█         | 2/19 [00:00<00:04,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07122 0.04202 0.01985 85 640:  11%|█         | 2/19 [00:00<00:04,  4.14it/s]42/99 7.3G 0.07122 0.04202 0.01985 85 640:  16%|█▌        | 3/19 [00:00<00:03,  4.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07118 0.04133 0.01941 58 640:  16%|█▌        | 3/19 [00:00<00:03,  4.19it/s]42/99 7.3G 0.07118 0.04133 0.01941 58 640:  21%|██        | 4/19 [00:00<00:03,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07157 0.03922 0.01999 53 640:  21%|██        | 4/19 [00:01<00:03,  4.69it/s]42/99 7.3G 0.07157 0.03922 0.01999 53 640:  26%|██▋       | 5/19 [00:01<00:02,  4.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07133 0.03776 0.01968 47 640:  26%|██▋       | 5/19 [00:01<00:02,  4.93it/s]42/99 7.3G 0.07133 0.03776 0.01968 47 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07289 0.04234 0.01964 126 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]42/99 7.3G 0.07289 0.04234 0.01964 126 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07301 0.04337 0.0199 89 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]  42/99 7.3G 0.07301 0.04337 0.0199 89 640:  42%|████▏     | 8/19 [00:01<00:03,  3.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07283 0.04366 0.01938 89 640:  42%|████▏     | 8/19 [00:03<00:03,  3.51it/s]42/99 7.3G 0.07283 0.04366 0.01938 89 640:  47%|████▋     | 9/19 [00:03<00:06,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07203 0.04166 0.01949 36 640:  47%|████▋     | 9/19 [00:07<00:06,  1.65it/s]42/99 7.3G 0.07203 0.04166 0.01949 36 640:  53%|█████▎    | 10/19 [00:07<00:14,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07357 0.04408 0.01962 178 640:  53%|█████▎    | 10/19 [00:08<00:14,  1.63s/it]42/99 7.3G 0.07357 0.04408 0.01962 178 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07333 0.04364 0.01993 62 640:  58%|█████▊    | 11/19 [00:12<00:12,  1.61s/it] 42/99 7.3G 0.07333 0.04364 0.01993 62 640:  63%|██████▎   | 12/19 [00:12<00:15,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07328 0.04282 0.01973 55 640:  63%|██████▎   | 12/19 [00:14<00:15,  2.27s/it]42/99 7.3G 0.07328 0.04282 0.01973 55 640:  68%|██████▊   | 13/19 [00:14<00:13,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07402 0.04248 0.01997 87 640:  68%|██████▊   | 13/19 [00:17<00:13,  2.24s/it]42/99 7.3G 0.07402 0.04248 0.01997 87 640:  74%|███████▎  | 14/19 [00:17<00:11,  2.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.0741 0.04182 0.01978 58 640:  74%|███████▎  | 14/19 [00:17<00:11,  2.38s/it] 42/99 7.3G 0.0741 0.04182 0.01978 58 640:  79%|███████▉  | 15/19 [00:17<00:06,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07457 0.04218 0.01983 94 640:  79%|███████▉  | 15/19 [00:19<00:06,  1.72s/it]42/99 7.3G 0.07457 0.04218 0.01983 94 640:  84%|████████▍ | 16/19 [00:19<00:05,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07437 0.04144 0.01987 45 640:  84%|████████▍ | 16/19 [00:22<00:05,  1.89s/it]42/99 7.3G 0.07437 0.04144 0.01987 45 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07412 0.04217 0.01965 97 640:  89%|████████▉ | 17/19 [00:28<00:04,  2.13s/it]42/99 7.3G 0.07412 0.04217 0.01965 97 640:  95%|█████████▍| 18/19 [00:28<00:03,  3.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07427 0.04302 0.01969 93 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.26s/it]42/99 7.3G 0.07427 0.04302 0.01969 93 640: 100%|██████████| 19/19 [00:29<00:00,  2.47s/it]42/99 7.3G 0.07427 0.04302 0.01969 93 640: 100%|██████████| 19/19 [00:29<00:00,  1.53s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:18<00:18, 18.28s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:20<00:00,  8.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:20<00:00, 10.18s/it]
                   all         55        256      0.185       0.23      0.167     0.0573
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07776 0.03545 0.01844 66 640:   0%|          | 0/19 [00:00<?, ?it/s]43/99 7.3G 0.07776 0.03545 0.01844 66 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07539 0.04288 0.01922 88 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]43/99 7.3G 0.07539 0.04288 0.01922 88 640:  11%|█         | 2/19 [00:00<00:06,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.0735 0.03833 0.01837 47 640:  11%|█         | 2/19 [00:01<00:06,  2.71it/s] 43/99 7.3G 0.0735 0.03833 0.01837 47 640:  16%|█▌        | 3/19 [00:01<00:05,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07498 0.04894 0.01976 135 640:  16%|█▌        | 3/19 [00:01<00:05,  2.72it/s]43/99 7.3G 0.07498 0.04894 0.01976 135 640:  21%|██        | 4/19 [00:01<00:07,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07715 0.06046 0.02026 212 640:  21%|██        | 4/19 [00:02<00:07,  2.14it/s]43/99 7.3G 0.07715 0.06046 0.02026 212 640:  26%|██▋       | 5/19 [00:02<00:08,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07547 0.05579 0.02073 50 640:  26%|██▋       | 5/19 [00:03<00:08,  1.70it/s] 43/99 7.3G 0.07547 0.05579 0.02073 50 640:  32%|███▏      | 6/19 [00:03<00:08,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07554 0.05634 0.02113 117 640:  32%|███▏      | 6/19 [00:04<00:08,  1.46it/s]43/99 7.3G 0.07554 0.05634 0.02113 117 640:  37%|███▋      | 7/19 [00:04<00:08,  1.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07605 0.05371 0.02131 69 640:  37%|███▋      | 7/19 [00:04<00:08,  1.42it/s] 43/99 7.3G 0.07605 0.05371 0.02131 69 640:  42%|████▏     | 8/19 [00:04<00:07,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07546 0.05345 0.02106 86 640:  42%|████▏     | 8/19 [00:05<00:07,  1.53it/s]43/99 7.3G 0.07546 0.05345 0.02106 86 640:  47%|████▋     | 9/19 [00:05<00:06,  1.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07576 0.0527 0.02115 86 640:  47%|████▋     | 9/19 [00:05<00:06,  1.59it/s] 43/99 7.3G 0.07576 0.0527 0.02115 86 640:  53%|█████▎    | 10/19 [00:05<00:05,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07545 0.05131 0.0211 56 640:  53%|█████▎    | 10/19 [00:07<00:05,  1.80it/s]43/99 7.3G 0.07545 0.05131 0.0211 56 640:  58%|█████▊    | 11/19 [00:07<00:08,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.0751 0.04967 0.02109 50 640:  58%|█████▊    | 11/19 [00:08<00:08,  1.05s/it]43/99 7.3G 0.0751 0.04967 0.02109 50 640:  63%|██████▎   | 12/19 [00:08<00:05,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07532 0.0493 0.02079 86 640:  63%|██████▎   | 12/19 [00:09<00:05,  1.19it/s]43/99 7.3G 0.07532 0.0493 0.02079 86 640:  68%|██████▊   | 13/19 [00:09<00:05,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07521 0.04861 0.02073 67 640:  68%|██████▊   | 13/19 [00:12<00:05,  1.05it/s]43/99 7.3G 0.07521 0.04861 0.02073 67 640:  74%|███████▎  | 14/19 [00:12<00:07,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07556 0.04925 0.02074 110 640:  74%|███████▎  | 14/19 [00:17<00:07,  1.50s/it]43/99 7.3G 0.07556 0.04925 0.02074 110 640:  79%|███████▉  | 15/19 [00:17<00:10,  2.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07561 0.0483 0.02055 67 640:  79%|███████▉  | 15/19 [00:17<00:10,  2.54s/it]  43/99 7.3G 0.07561 0.0483 0.02055 67 640:  84%|████████▍ | 16/19 [00:17<00:05,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.0754 0.0478 0.02046 67 640:  84%|████████▍ | 16/19 [00:26<00:05,  1.93s/it] 43/99 7.3G 0.0754 0.0478 0.02046 67 640:  89%|████████▉ | 17/19 [00:26<00:07,  3.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07509 0.04699 0.02035 55 640:  89%|████████▉ | 17/19 [00:28<00:07,  3.90s/it]43/99 7.3G 0.07509 0.04699 0.02035 55 640:  95%|█████████▍| 18/19 [00:28<00:03,  3.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07469 0.04641 0.02021 56 640:  95%|█████████▍| 18/19 [00:31<00:03,  3.53s/it]43/99 7.3G 0.07469 0.04641 0.02021 56 640: 100%|██████████| 19/19 [00:31<00:00,  3.36s/it]43/99 7.3G 0.07469 0.04641 0.02021 56 640: 100%|██████████| 19/19 [00:31<00:00,  1.67s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  6.03s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.30s/it]
                   all         55        256      0.204      0.227      0.166     0.0563
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07519 0.03888 0.02342 66 640:   0%|          | 0/19 [00:00<?, ?it/s]44/99 7.3G 0.07519 0.03888 0.02342 66 640:   5%|▌         | 1/19 [00:00<00:06,  3.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07246 0.03564 0.01986 60 640:   5%|▌         | 1/19 [00:00<00:06,  3.00it/s]44/99 7.3G 0.07246 0.03564 0.01986 60 640:  11%|█         | 2/19 [00:00<00:04,  4.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0735 0.03643 0.02 68 640:  11%|█         | 2/19 [00:00<00:04,  4.16it/s]    44/99 7.3G 0.0735 0.03643 0.02 68 640:  16%|█▌        | 3/19 [00:00<00:03,  4.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07392 0.04293 0.02063 103 640:  16%|█▌        | 3/19 [00:00<00:03,  4.75it/s]44/99 7.3G 0.07392 0.04293 0.02063 103 640:  21%|██        | 4/19 [00:00<00:02,  5.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07378 0.04383 0.02038 75 640:  21%|██        | 4/19 [00:01<00:02,  5.10it/s] 44/99 7.3G 0.07378 0.04383 0.02038 75 640:  26%|██▋       | 5/19 [00:01<00:02,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07504 0.04552 0.01999 118 640:  26%|██▋       | 5/19 [00:01<00:02,  5.31it/s]44/99 7.3G 0.07504 0.04552 0.01999 118 640:  32%|███▏      | 6/19 [00:01<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07445 0.04448 0.0194 68 640:  32%|███▏      | 6/19 [00:01<00:02,  5.45it/s]  44/99 7.3G 0.07445 0.04448 0.0194 68 640:  37%|███▋      | 7/19 [00:01<00:02,  5.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07493 0.04477 0.01915 90 640:  37%|███▋      | 7/19 [00:01<00:02,  5.55it/s]44/99 7.3G 0.07493 0.04477 0.01915 90 640:  42%|████▏     | 8/19 [00:01<00:01,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07452 0.04326 0.01887 52 640:  42%|████▏     | 8/19 [00:01<00:01,  5.61it/s]44/99 7.3G 0.07452 0.04326 0.01887 52 640:  47%|████▋     | 9/19 [00:01<00:01,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07451 0.04224 0.01912 54 640:  47%|████▋     | 9/19 [00:02<00:01,  5.66it/s]44/99 7.3G 0.07451 0.04224 0.01912 54 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07441 0.04309 0.01916 85 640:  53%|█████▎    | 10/19 [00:06<00:03,  2.31it/s]44/99 7.3G 0.07441 0.04309 0.01916 85 640:  58%|█████▊    | 11/19 [00:06<00:11,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07382 0.04346 0.01883 80 640:  58%|█████▊    | 11/19 [00:10<00:11,  1.48s/it]44/99 7.3G 0.07382 0.04346 0.01883 80 640:  63%|██████▎   | 12/19 [00:10<00:16,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0739 0.04529 0.0194 116 640:  63%|██████▎   | 12/19 [00:11<00:16,  2.29s/it] 44/99 7.3G 0.0739 0.04529 0.0194 116 640:  68%|██████▊   | 13/19 [00:11<00:11,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07459 0.04678 0.01947 127 640:  68%|██████▊   | 13/19 [00:16<00:11,  1.97s/it]44/99 7.3G 0.07459 0.04678 0.01947 127 640:  74%|███████▎  | 14/19 [00:16<00:13,  2.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07454 0.04712 0.01964 90 640:  74%|███████▎  | 14/19 [00:16<00:13,  2.64s/it] 44/99 7.3G 0.07454 0.04712 0.01964 90 640:  79%|███████▉  | 15/19 [00:16<00:07,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07415 0.04589 0.01953 41 640:  79%|███████▉  | 15/19 [00:19<00:07,  1.91s/it]44/99 7.3G 0.07415 0.04589 0.01953 41 640:  84%|████████▍ | 16/19 [00:19<00:06,  2.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07349 0.0454 0.01951 58 640:  84%|████████▍ | 16/19 [00:19<00:06,  2.14s/it] 44/99 7.3G 0.07349 0.0454 0.01951 58 640:  89%|████████▉ | 17/19 [00:19<00:03,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0731 0.0449 0.0197 55 640:  89%|████████▉ | 17/19 [00:23<00:03,  1.63s/it]  44/99 7.3G 0.0731 0.0449 0.0197 55 640:  95%|█████████▍| 18/19 [00:23<00:02,  2.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.07312 0.04451 0.01965 63 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.49s/it]44/99 7.3G 0.07312 0.04451 0.01965 63 640: 100%|██████████| 19/19 [00:28<00:00,  3.03s/it]44/99 7.3G 0.07312 0.04451 0.01965 63 640: 100%|██████████| 19/19 [00:28<00:00,  1.49s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  6.99s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.15s/it]
                   all         55        256      0.213      0.206       0.16     0.0528
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07469 0.06017 0.02117 98 640:   0%|          | 0/19 [00:00<?, ?it/s]45/99 7.3G 0.07469 0.06017 0.02117 98 640:   5%|▌         | 1/19 [00:00<00:03,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07386 0.06608 0.02011 108 640:   5%|▌         | 1/19 [00:00<00:03,  5.59it/s]45/99 7.3G 0.07386 0.06608 0.02011 108 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07343 0.0609 0.01958 84 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]  45/99 7.3G 0.07343 0.0609 0.01958 84 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.0746 0.05642 0.01972 81 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]45/99 7.3G 0.0746 0.05642 0.01972 81 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.0756 0.05474 0.01909 104 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]45/99 7.3G 0.0756 0.05474 0.01909 104 640:  26%|██▋       | 5/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07364 0.05163 0.01881 57 640:  26%|██▋       | 5/19 [00:01<00:02,  5.72it/s]45/99 7.3G 0.07364 0.05163 0.01881 57 640:  32%|███▏      | 6/19 [00:01<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07374 0.05076 0.01941 82 640:  32%|███▏      | 6/19 [00:01<00:02,  5.72it/s]45/99 7.3G 0.07374 0.05076 0.01941 82 640:  37%|███▋      | 7/19 [00:01<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07335 0.04897 0.01965 55 640:  37%|███▋      | 7/19 [00:01<00:02,  5.73it/s]45/99 7.3G 0.07335 0.04897 0.01965 55 640:  42%|████▏     | 8/19 [00:01<00:01,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07309 0.04767 0.01949 61 640:  42%|████▏     | 8/19 [00:05<00:01,  5.74it/s]45/99 7.3G 0.07309 0.04767 0.01949 61 640:  47%|████▋     | 9/19 [00:05<00:12,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07276 0.04638 0.01945 58 640:  47%|████▋     | 9/19 [00:05<00:12,  1.25s/it]45/99 7.3G 0.07276 0.04638 0.01945 58 640:  53%|█████▎    | 10/19 [00:05<00:09,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07317 0.04719 0.01916 105 640:  53%|█████▎    | 10/19 [00:10<00:09,  1.08s/it]45/99 7.3G 0.07317 0.04719 0.01916 105 640:  58%|█████▊    | 11/19 [00:10<00:18,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07345 0.04654 0.0187 87 640:  58%|█████▊    | 11/19 [00:11<00:18,  2.28s/it]  45/99 7.3G 0.07345 0.04654 0.0187 87 640:  63%|██████▎   | 12/19 [00:11<00:11,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07343 0.04666 0.01849 89 640:  63%|██████▎   | 12/19 [00:13<00:11,  1.68s/it]45/99 7.3G 0.07343 0.04666 0.01849 89 640:  68%|██████▊   | 13/19 [00:13<00:11,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.0729 0.04505 0.019 36 640:  68%|██████▊   | 13/19 [00:14<00:11,  1.93s/it]   45/99 7.3G 0.0729 0.04505 0.019 36 640:  74%|███████▎  | 14/19 [00:14<00:07,  1.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07291 0.04467 0.01896 73 640:  74%|███████▎  | 14/19 [00:18<00:07,  1.51s/it]45/99 7.3G 0.07291 0.04467 0.01896 73 640:  79%|███████▉  | 15/19 [00:18<00:09,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07334 0.04442 0.01921 65 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.26s/it]45/99 7.3G 0.07334 0.04442 0.01921 65 640:  84%|████████▍ | 16/19 [00:22<00:08,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07318 0.0439 0.01924 62 640:  84%|████████▍ | 16/19 [00:28<00:08,  2.83s/it] 45/99 7.3G 0.07318 0.0439 0.01924 62 640:  89%|████████▉ | 17/19 [00:28<00:07,  3.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07346 0.04569 0.01922 125 640:  89%|████████▉ | 17/19 [00:28<00:07,  3.92s/it]45/99 7.3G 0.07346 0.04569 0.01922 125 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07337 0.04562 0.01914 76 640:  95%|█████████▍| 18/19 [00:35<00:02,  2.79s/it] 45/99 7.3G 0.07337 0.04562 0.01914 76 640: 100%|██████████| 19/19 [00:35<00:00,  3.92s/it]45/99 7.3G 0.07337 0.04562 0.01914 76 640: 100%|██████████| 19/19 [00:35<00:00,  1.86s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.22s/it]
                   all         55        256      0.245      0.165      0.156     0.0567
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07427 0.0386 0.02025 73 640:   0%|          | 0/19 [00:01<?, ?it/s]46/99 7.3G 0.07427 0.0386 0.02025 73 640:   5%|▌         | 1/19 [00:01<00:23,  1.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07296 0.03863 0.01999 62 640:   5%|▌         | 1/19 [00:02<00:23,  1.28s/it]46/99 7.3G 0.07296 0.03863 0.01999 62 640:  11%|█         | 2/19 [00:02<00:18,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07294 0.03717 0.02052 51 640:  11%|█         | 2/19 [00:02<00:18,  1.07s/it]46/99 7.3G 0.07294 0.03717 0.02052 51 640:  16%|█▌        | 3/19 [00:02<00:12,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07188 0.03767 0.02011 63 640:  16%|█▌        | 3/19 [00:03<00:12,  1.26it/s]46/99 7.3G 0.07188 0.03767 0.02011 63 640:  21%|██        | 4/19 [00:03<00:09,  1.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07142 0.03756 0.02012 60 640:  21%|██        | 4/19 [00:03<00:09,  1.60it/s]46/99 7.3G 0.07142 0.03756 0.02012 60 640:  26%|██▋       | 5/19 [00:03<00:07,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07134 0.03761 0.01966 69 640:  26%|██▋       | 5/19 [00:03<00:07,  1.81it/s]46/99 7.3G 0.07134 0.03761 0.01966 69 640:  32%|███▏      | 6/19 [00:03<00:06,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07174 0.03907 0.01948 80 640:  32%|███▏      | 6/19 [00:04<00:06,  1.89it/s]46/99 7.3G 0.07174 0.03907 0.01948 80 640:  37%|███▋      | 7/19 [00:04<00:06,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07119 0.03829 0.01947 50 640:  37%|███▋      | 7/19 [00:04<00:06,  1.89it/s]46/99 7.3G 0.07119 0.03829 0.01947 50 640:  42%|████▏     | 8/19 [00:04<00:05,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07127 0.03946 0.01966 77 640:  42%|████▏     | 8/19 [00:05<00:05,  2.08it/s]46/99 7.3G 0.07127 0.03946 0.01966 77 640:  47%|████▋     | 9/19 [00:05<00:04,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.0719 0.0411 0.02015 99 640:  47%|████▋     | 9/19 [00:05<00:04,  2.14it/s]  46/99 7.3G 0.0719 0.0411 0.02015 99 640:  53%|█████▎    | 10/19 [00:05<00:04,  1.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07236 0.04147 0.01997 84 640:  53%|█████▎    | 10/19 [00:06<00:04,  1.88it/s]46/99 7.3G 0.07236 0.04147 0.01997 84 640:  58%|█████▊    | 11/19 [00:06<00:04,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07238 0.04097 0.02003 61 640:  58%|█████▊    | 11/19 [00:06<00:04,  1.86it/s]46/99 7.3G 0.07238 0.04097 0.02003 61 640:  63%|██████▎   | 12/19 [00:06<00:03,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.0721 0.04011 0.01966 52 640:  63%|██████▎   | 12/19 [00:08<00:03,  1.93it/s] 46/99 7.3G 0.0721 0.04011 0.01966 52 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07232 0.04062 0.01932 85 640:  68%|██████▊   | 13/19 [00:16<00:05,  1.07it/s]46/99 7.3G 0.07232 0.04062 0.01932 85 640:  74%|███████▎  | 14/19 [00:16<00:14,  2.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07243 0.04077 0.01944 70 640:  74%|███████▎  | 14/19 [00:16<00:14,  2.92s/it]46/99 7.3G 0.07243 0.04077 0.01944 70 640:  79%|███████▉  | 15/19 [00:16<00:08,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07269 0.04076 0.01943 76 640:  79%|███████▉  | 15/19 [00:22<00:08,  2.15s/it]46/99 7.3G 0.07269 0.04076 0.01943 76 640:  84%|████████▍ | 16/19 [00:22<00:09,  3.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07271 0.04083 0.01942 66 640:  84%|████████▍ | 16/19 [00:22<00:09,  3.23s/it]46/99 7.3G 0.07271 0.04083 0.01942 66 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07268 0.04047 0.01914 62 640:  89%|████████▉ | 17/19 [00:26<00:04,  2.31s/it]46/99 7.3G 0.07268 0.04047 0.01914 62 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07282 0.03994 0.01909 61 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.81s/it]46/99 7.3G 0.07282 0.03994 0.01909 61 640: 100%|██████████| 19/19 [00:26<00:00,  2.02s/it]46/99 7.3G 0.07282 0.03994 0.01909 61 640: 100%|██████████| 19/19 [00:26<00:00,  1.41s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.21s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  6.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]
                   all         55        256      0.224      0.164      0.137     0.0537
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07094 0.04102 0.02157 62 640:   0%|          | 0/19 [00:00<?, ?it/s]47/99 7.3G 0.07094 0.04102 0.02157 62 640:   5%|▌         | 1/19 [00:00<00:07,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07362 0.03938 0.01993 67 640:   5%|▌         | 1/19 [00:00<00:07,  2.42it/s]47/99 7.3G 0.07362 0.03938 0.01993 67 640:  11%|█         | 2/19 [00:00<00:04,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07262 0.03849 0.01939 59 640:  11%|█         | 2/19 [00:00<00:04,  3.41it/s]47/99 7.3G 0.07262 0.03849 0.01939 59 640:  16%|█▌        | 3/19 [00:00<00:04,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07362 0.04415 0.02028 112 640:  16%|█▌        | 3/19 [00:01<00:04,  3.92it/s]47/99 7.3G 0.07362 0.04415 0.02028 112 640:  21%|██        | 4/19 [00:01<00:03,  4.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07511 0.04779 0.0196 128 640:  21%|██        | 4/19 [00:01<00:03,  4.08it/s] 47/99 7.3G 0.07511 0.04779 0.0196 128 640:  26%|██▋       | 5/19 [00:01<00:03,  4.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07576 0.04715 0.01989 69 640:  26%|██▋       | 5/19 [00:01<00:03,  4.26it/s]47/99 7.3G 0.07576 0.04715 0.01989 69 640:  32%|███▏      | 6/19 [00:01<00:03,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.076 0.04634 0.01997 65 640:  32%|███▏      | 6/19 [00:01<00:03,  3.55it/s]  47/99 7.3G 0.076 0.04634 0.01997 65 640:  37%|███▋      | 7/19 [00:01<00:03,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07568 0.0449 0.01982 61 640:  37%|███▋      | 7/19 [00:02<00:03,  3.87it/s]47/99 7.3G 0.07568 0.0449 0.01982 61 640:  42%|████▏     | 8/19 [00:02<00:02,  4.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07489 0.04341 0.01954 50 640:  42%|████▏     | 8/19 [00:02<00:02,  4.23it/s]47/99 7.3G 0.07489 0.04341 0.01954 50 640:  47%|████▋     | 9/19 [00:02<00:02,  4.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07505 0.0451 0.01955 101 640:  47%|████▋     | 9/19 [00:03<00:02,  4.57it/s]47/99 7.3G 0.07505 0.0451 0.01955 101 640:  53%|█████▎    | 10/19 [00:03<00:05,  1.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07526 0.04616 0.01958 96 640:  53%|█████▎    | 10/19 [00:10<00:05,  1.74it/s]47/99 7.3G 0.07526 0.04616 0.01958 96 640:  58%|█████▊    | 11/19 [00:10<00:20,  2.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07459 0.04472 0.01942 46 640:  58%|█████▊    | 11/19 [00:11<00:20,  2.57s/it]47/99 7.3G 0.07459 0.04472 0.01942 46 640:  63%|██████▎   | 12/19 [00:11<00:14,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07473 0.04436 0.01954 75 640:  63%|██████▎   | 12/19 [00:14<00:14,  2.01s/it]47/99 7.3G 0.07473 0.04436 0.01954 75 640:  68%|██████▊   | 13/19 [00:14<00:14,  2.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07458 0.04389 0.01963 62 640:  68%|██████▊   | 13/19 [00:15<00:14,  2.46s/it]47/99 7.3G 0.07458 0.04389 0.01963 62 640:  74%|███████▎  | 14/19 [00:15<00:09,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07433 0.04383 0.01949 76 640:  74%|███████▎  | 14/19 [00:17<00:09,  1.96s/it]47/99 7.3G 0.07433 0.04383 0.01949 76 640:  79%|███████▉  | 15/19 [00:17<00:07,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07352 0.04269 0.01925 41 640:  79%|███████▉  | 15/19 [00:17<00:07,  1.89s/it]47/99 7.3G 0.07352 0.04269 0.01925 41 640:  84%|████████▍ | 16/19 [00:17<00:04,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07389 0.04345 0.01927 106 640:  84%|████████▍ | 16/19 [00:22<00:04,  1.38s/it]47/99 7.3G 0.07389 0.04345 0.01927 106 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07374 0.04392 0.01918 94 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.31s/it] 47/99 7.3G 0.07374 0.04392 0.01918 94 640:  95%|█████████▍| 18/19 [00:24<00:02,  2.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07359 0.04383 0.01908 73 640:  95%|█████████▍| 18/19 [00:32<00:02,  2.44s/it]47/99 7.3G 0.07359 0.04383 0.01908 73 640: 100%|██████████| 19/19 [00:32<00:00,  4.14s/it]47/99 7.3G 0.07359 0.04383 0.01908 73 640: 100%|██████████| 19/19 [00:32<00:00,  1.73s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:16<00:16, 16.57s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  7.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:17<00:00,  8.57s/it]
                   all         55        256      0.249      0.175      0.129     0.0478
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08149 0.05297 0.01888 101 640:   0%|          | 0/19 [00:00<?, ?it/s]48/99 7.3G 0.08149 0.05297 0.01888 101 640:   5%|▌         | 1/19 [00:00<00:12,  1.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07512 0.04279 0.01776 53 640:   5%|▌         | 1/19 [00:01<00:12,  1.47it/s] 48/99 7.3G 0.07512 0.04279 0.01776 53 640:  11%|█         | 2/19 [00:01<00:12,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07642 0.05008 0.01798 118 640:  11%|█         | 2/19 [00:02<00:12,  1.37it/s]48/99 7.3G 0.07642 0.05008 0.01798 118 640:  16%|█▌        | 3/19 [00:02<00:11,  1.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07456 0.04452 0.01786 46 640:  16%|█▌        | 3/19 [00:02<00:11,  1.35it/s] 48/99 7.3G 0.07456 0.04452 0.01786 46 640:  21%|██        | 4/19 [00:02<00:11,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07488 0.04585 0.01856 86 640:  21%|██        | 4/19 [00:03<00:11,  1.33it/s]48/99 7.3G 0.07488 0.04585 0.01856 86 640:  26%|██▋       | 5/19 [00:03<00:10,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07455 0.04856 0.01828 104 640:  26%|██▋       | 5/19 [00:04<00:10,  1.39it/s]48/99 7.3G 0.07455 0.04856 0.01828 104 640:  32%|███▏      | 6/19 [00:04<00:08,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07418 0.04769 0.01765 81 640:  32%|███▏      | 6/19 [00:04<00:08,  1.53it/s] 48/99 7.3G 0.07418 0.04769 0.01765 81 640:  37%|███▋      | 7/19 [00:04<00:07,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07349 0.04583 0.01719 59 640:  37%|███▋      | 7/19 [00:05<00:07,  1.53it/s]48/99 7.3G 0.07349 0.04583 0.01719 59 640:  42%|████▏     | 8/19 [00:05<00:08,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07324 0.0443 0.01754 55 640:  42%|████▏     | 8/19 [00:06<00:08,  1.33it/s] 48/99 7.3G 0.07324 0.0443 0.01754 55 640:  47%|████▋     | 9/19 [00:06<00:08,  1.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07421 0.04791 0.01826 166 640:  47%|████▋     | 9/19 [00:07<00:08,  1.18it/s]48/99 7.3G 0.07421 0.04791 0.01826 166 640:  53%|█████▎    | 10/19 [00:07<00:07,  1.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07395 0.04745 0.01852 76 640:  53%|█████▎    | 10/19 [00:08<00:07,  1.15it/s] 48/99 7.3G 0.07395 0.04745 0.01852 76 640:  58%|█████▊    | 11/19 [00:08<00:07,  1.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07396 0.04733 0.01873 77 640:  58%|█████▊    | 11/19 [00:09<00:07,  1.14it/s]48/99 7.3G 0.07396 0.04733 0.01873 77 640:  63%|██████▎   | 12/19 [00:09<00:06,  1.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07421 0.04768 0.01858 109 640:  63%|██████▎   | 12/19 [00:09<00:06,  1.17it/s]48/99 7.3G 0.07421 0.04768 0.01858 109 640:  68%|██████▊   | 13/19 [00:09<00:04,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07367 0.04668 0.01865 47 640:  68%|██████▊   | 13/19 [00:12<00:04,  1.38it/s] 48/99 7.3G 0.07367 0.04668 0.01865 47 640:  74%|███████▎  | 14/19 [00:12<00:05,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07371 0.04535 0.01858 45 640:  74%|███████▎  | 14/19 [00:13<00:05,  1.19s/it]48/99 7.3G 0.07371 0.04535 0.01858 45 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07301 0.04443 0.01847 46 640:  79%|███████▉  | 15/19 [00:23<00:05,  1.31s/it]48/99 7.3G 0.07301 0.04443 0.01847 46 640:  84%|████████▍ | 16/19 [00:23<00:11,  3.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07266 0.04391 0.0184 64 640:  84%|████████▍ | 16/19 [00:23<00:11,  3.86s/it] 48/99 7.3G 0.07266 0.04391 0.0184 64 640:  89%|████████▉ | 17/19 [00:23<00:05,  2.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07333 0.04529 0.01858 146 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.82s/it]48/99 7.3G 0.07333 0.04529 0.01858 146 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07297 0.04523 0.01853 72 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.82s/it] 48/99 7.3G 0.07297 0.04523 0.01853 72 640: 100%|██████████| 19/19 [00:30<00:00,  2.73s/it]48/99 7.3G 0.07297 0.04523 0.01853 72 640: 100%|██████████| 19/19 [00:30<00:00,  1.59s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.70s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.52s/it]
                   all         55        256      0.199      0.206       0.14      0.049
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07725 0.0419 0.01722 78 640:   0%|          | 0/19 [00:00<?, ?it/s]49/99 7.3G 0.07725 0.0419 0.01722 78 640:   5%|▌         | 1/19 [00:00<00:09,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07384 0.04263 0.01687 77 640:   5%|▌         | 1/19 [00:01<00:09,  1.83it/s]49/99 7.3G 0.07384 0.04263 0.01687 77 640:  11%|█         | 2/19 [00:01<00:09,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07361 0.0401 0.01787 59 640:  11%|█         | 2/19 [00:01<00:09,  1.73it/s] 49/99 7.3G 0.07361 0.0401 0.01787 59 640:  16%|█▌        | 3/19 [00:01<00:09,  1.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07355 0.03821 0.01894 54 640:  16%|█▌        | 3/19 [00:02<00:09,  1.67it/s]49/99 7.3G 0.07355 0.03821 0.01894 54 640:  21%|██        | 4/19 [00:02<00:10,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07463 0.04276 0.01885 106 640:  21%|██        | 4/19 [00:03<00:10,  1.46it/s]49/99 7.3G 0.07463 0.04276 0.01885 106 640:  26%|██▋       | 5/19 [00:03<00:10,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07317 0.04199 0.01915 65 640:  26%|██▋       | 5/19 [00:04<00:10,  1.33it/s] 49/99 7.3G 0.07317 0.04199 0.01915 65 640:  32%|███▏      | 6/19 [00:04<00:09,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07347 0.04796 0.01869 160 640:  32%|███▏      | 6/19 [00:04<00:09,  1.39it/s]49/99 7.3G 0.07347 0.04796 0.01869 160 640:  37%|███▋      | 7/19 [00:04<00:06,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07513 0.04759 0.01901 101 640:  37%|███▋      | 7/19 [00:04<00:06,  1.75it/s]49/99 7.3G 0.07513 0.04759 0.01901 101 640:  42%|████▏     | 8/19 [00:04<00:05,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07537 0.04732 0.01937 87 640:  42%|████▏     | 8/19 [00:04<00:05,  2.20it/s] 49/99 7.3G 0.07537 0.04732 0.01937 87 640:  47%|████▋     | 9/19 [00:04<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07498 0.04552 0.01893 48 640:  47%|████▋     | 9/19 [00:05<00:03,  2.57it/s]49/99 7.3G 0.07498 0.04552 0.01893 48 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07402 0.04439 0.01856 51 640:  53%|█████▎    | 10/19 [00:06<00:03,  2.63it/s]49/99 7.3G 0.07402 0.04439 0.01856 51 640:  58%|█████▊    | 11/19 [00:06<00:05,  1.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07406 0.04514 0.01847 100 640:  58%|█████▊    | 11/19 [00:09<00:05,  1.48it/s]49/99 7.3G 0.07406 0.04514 0.01847 100 640:  63%|██████▎   | 12/19 [00:09<00:09,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07362 0.04406 0.01826 52 640:  63%|██████▎   | 12/19 [00:17<00:09,  1.29s/it] 49/99 7.3G 0.07362 0.04406 0.01826 52 640:  68%|██████▊   | 13/19 [00:17<00:20,  3.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07314 0.04285 0.0183 41 640:  68%|██████▊   | 13/19 [00:18<00:20,  3.37s/it] 49/99 7.3G 0.07314 0.04285 0.0183 41 640:  74%|███████▎  | 14/19 [00:18<00:12,  2.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07302 0.04352 0.01852 81 640:  74%|███████▎  | 14/19 [00:23<00:12,  2.60s/it]49/99 7.3G 0.07302 0.04352 0.01852 81 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.0731 0.04393 0.01862 86 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.45s/it] 49/99 7.3G 0.0731 0.04393 0.01862 86 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07308 0.04472 0.01856 97 640:  84%|████████▍ | 16/19 [00:28<00:07,  2.48s/it]49/99 7.3G 0.07308 0.04472 0.01856 97 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07295 0.0446 0.01866 72 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.01s/it] 49/99 7.3G 0.07295 0.0446 0.01866 72 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07339 0.04441 0.01868 83 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.16s/it]49/99 7.3G 0.07339 0.04441 0.01868 83 640: 100%|██████████| 19/19 [00:30<00:00,  2.15s/it]49/99 7.3G 0.07339 0.04441 0.01868 83 640: 100%|██████████| 19/19 [00:30<00:00,  1.60s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.06s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  5.97s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.19s/it]
                   all         55        256       0.21      0.191      0.137     0.0477
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07208 0.04586 0.01655 75 640:   0%|          | 0/19 [00:00<?, ?it/s]50/99 7.3G 0.07208 0.04586 0.01655 75 640:   5%|▌         | 1/19 [00:00<00:03,  4.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07164 0.04769 0.01787 77 640:   5%|▌         | 1/19 [00:00<00:03,  4.79it/s]50/99 7.3G 0.07164 0.04769 0.01787 77 640:  11%|█         | 2/19 [00:00<00:03,  4.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07354 0.05615 0.0187 119 640:  11%|█         | 2/19 [00:00<00:03,  4.81it/s]50/99 7.3G 0.07354 0.05615 0.0187 119 640:  16%|█▌        | 3/19 [00:00<00:03,  4.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07392 0.05259 0.01887 77 640:  16%|█▌        | 3/19 [00:00<00:03,  4.83it/s]50/99 7.3G 0.07392 0.05259 0.01887 77 640:  21%|██        | 4/19 [00:00<00:03,  4.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07324 0.05107 0.01858 69 640:  21%|██        | 4/19 [00:00<00:03,  4.97it/s]50/99 7.3G 0.07324 0.05107 0.01858 69 640:  26%|██▋       | 5/19 [00:00<00:02,  5.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07395 0.04853 0.01821 64 640:  26%|██▋       | 5/19 [00:01<00:02,  5.22it/s]50/99 7.3G 0.07395 0.04853 0.01821 64 640:  32%|███▏      | 6/19 [00:01<00:02,  5.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07475 0.04748 0.01791 86 640:  32%|███▏      | 6/19 [00:01<00:02,  5.38it/s]50/99 7.3G 0.07475 0.04748 0.01791 86 640:  37%|███▋      | 7/19 [00:01<00:02,  4.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07529 0.04678 0.01783 84 640:  37%|███▋      | 7/19 [00:01<00:02,  4.87it/s]50/99 7.3G 0.07529 0.04678 0.01783 84 640:  42%|████▏     | 8/19 [00:01<00:03,  3.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.0742 0.04486 0.01821 50 640:  42%|████▏     | 8/19 [00:03<00:03,  3.54it/s] 50/99 7.3G 0.0742 0.04486 0.01821 50 640:  47%|████▋     | 9/19 [00:03<00:06,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07407 0.0458 0.01852 87 640:  47%|████▋     | 9/19 [00:11<00:06,  1.65it/s]50/99 7.3G 0.07407 0.0458 0.01852 87 640:  53%|█████▎    | 10/19 [00:11<00:26,  3.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07364 0.04555 0.0184 78 640:  53%|█████▎    | 10/19 [00:12<00:26,  3.00s/it]50/99 7.3G 0.07364 0.04555 0.0184 78 640:  58%|█████▊    | 11/19 [00:12<00:18,  2.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07325 0.04508 0.01851 73 640:  58%|█████▊    | 11/19 [00:15<00:18,  2.32s/it]50/99 7.3G 0.07325 0.04508 0.01851 73 640:  63%|██████▎   | 12/19 [00:15<00:18,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07279 0.04408 0.01857 51 640:  63%|██████▎   | 12/19 [00:16<00:18,  2.67s/it]50/99 7.3G 0.07279 0.04408 0.01857 51 640:  68%|██████▊   | 13/19 [00:16<00:12,  2.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07307 0.04316 0.01846 58 640:  68%|██████▊   | 13/19 [00:20<00:12,  2.11s/it]50/99 7.3G 0.07307 0.04316 0.01846 58 640:  74%|███████▎  | 14/19 [00:20<00:12,  2.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.0729 0.04256 0.01834 56 640:  74%|███████▎  | 14/19 [00:20<00:12,  2.55s/it] 50/99 7.3G 0.0729 0.04256 0.01834 56 640:  79%|███████▉  | 15/19 [00:20<00:07,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07301 0.04279 0.01845 81 640:  79%|███████▉  | 15/19 [00:21<00:07,  1.96s/it]50/99 7.3G 0.07301 0.04279 0.01845 81 640:  84%|████████▍ | 16/19 [00:21<00:04,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07282 0.04267 0.01868 65 640:  84%|████████▍ | 16/19 [00:24<00:04,  1.52s/it]50/99 7.3G 0.07282 0.04267 0.01868 65 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07312 0.04326 0.01863 106 640:  89%|████████▉ | 17/19 [00:33<00:04,  2.09s/it]50/99 7.3G 0.07312 0.04326 0.01863 106 640:  95%|█████████▍| 18/19 [00:33<00:04,  4.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07285 0.04293 0.01834 66 640:  95%|█████████▍| 18/19 [00:34<00:04,  4.10s/it] 50/99 7.3G 0.07285 0.04293 0.01834 66 640: 100%|██████████| 19/19 [00:34<00:00,  3.11s/it]50/99 7.3G 0.07285 0.04293 0.01834 66 640: 100%|██████████| 19/19 [00:34<00:00,  1.80s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.76s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.76s/it]
                   all         55        256      0.199      0.202      0.142     0.0568
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07109 0.03253 0.02118 49 640:   0%|          | 0/19 [00:00<?, ?it/s]51/99 7.3G 0.07109 0.03253 0.02118 49 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.06948 0.039 0.01851 72 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]  51/99 7.3G 0.06948 0.039 0.01851 72 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07029 0.04461 0.01784 94 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]51/99 7.3G 0.07029 0.04461 0.01784 94 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07201 0.04523 0.01847 96 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]51/99 7.3G 0.07201 0.04523 0.01847 96 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07148 0.04582 0.01937 74 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]51/99 7.3G 0.07148 0.04582 0.01937 74 640:  26%|██▋       | 5/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07228 0.04529 0.01892 89 640:  26%|██▋       | 5/19 [00:01<00:02,  5.45it/s]51/99 7.3G 0.07228 0.04529 0.01892 89 640:  32%|███▏      | 6/19 [00:01<00:03,  4.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07307 0.04492 0.01868 78 640:  32%|███▏      | 6/19 [00:01<00:03,  4.30it/s]51/99 7.3G 0.07307 0.04492 0.01868 78 640:  37%|███▋      | 7/19 [00:01<00:02,  4.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07214 0.04354 0.01834 52 640:  37%|███▋      | 7/19 [00:02<00:02,  4.44it/s]51/99 7.3G 0.07214 0.04354 0.01834 52 640:  42%|████▏     | 8/19 [00:02<00:04,  2.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07237 0.04366 0.01871 70 640:  42%|████▏     | 8/19 [00:05<00:04,  2.40it/s]51/99 7.3G 0.07237 0.04366 0.01871 70 640:  47%|████▋     | 9/19 [00:05<00:11,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07307 0.04251 0.01819 68 640:  47%|████▋     | 9/19 [00:06<00:11,  1.15s/it]51/99 7.3G 0.07307 0.04251 0.01819 68 640:  53%|█████▎    | 10/19 [00:06<00:09,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07304 0.04279 0.01833 80 640:  53%|█████▎    | 10/19 [00:08<00:09,  1.10s/it]51/99 7.3G 0.07304 0.04279 0.01833 80 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07286 0.04195 0.01818 56 640:  58%|█████▊    | 11/19 [00:09<00:12,  1.57s/it]51/99 7.3G 0.07286 0.04195 0.01818 56 640:  63%|██████▎   | 12/19 [00:09<00:09,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.0735 0.04376 0.01819 136 640:  63%|██████▎   | 12/19 [00:10<00:09,  1.35s/it]51/99 7.3G 0.0735 0.04376 0.01819 136 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07348 0.04435 0.01832 83 640:  68%|██████▊   | 13/19 [00:13<00:06,  1.13s/it]51/99 7.3G 0.07348 0.04435 0.01832 83 640:  74%|███████▎  | 14/19 [00:13<00:08,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07309 0.04348 0.01817 57 640:  74%|███████▎  | 14/19 [00:22<00:08,  1.67s/it]51/99 7.3G 0.07309 0.04348 0.01817 57 640:  79%|███████▉  | 15/19 [00:22<00:15,  3.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07312 0.04485 0.01834 118 640:  79%|███████▉  | 15/19 [00:22<00:15,  3.93s/it]51/99 7.3G 0.07312 0.04485 0.01834 118 640:  84%|████████▍ | 16/19 [00:22<00:08,  2.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07343 0.04517 0.01823 96 640:  84%|████████▍ | 16/19 [00:28<00:08,  2.97s/it] 51/99 7.3G 0.07343 0.04517 0.01823 96 640:  89%|████████▉ | 17/19 [00:28<00:07,  3.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07367 0.0451 0.01809 85 640:  89%|████████▉ | 17/19 [00:29<00:07,  3.88s/it] 51/99 7.3G 0.07367 0.0451 0.01809 85 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07374 0.04403 0.01849 40 640:  95%|█████████▍| 18/19 [00:33<00:02,  2.81s/it]51/99 7.3G 0.07374 0.04403 0.01849 40 640: 100%|██████████| 19/19 [00:33<00:00,  3.25s/it]51/99 7.3G 0.07374 0.04403 0.01849 40 640: 100%|██████████| 19/19 [00:33<00:00,  1.77s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.77s/it]
                   all         55        256      0.247      0.198      0.153      0.061
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07815 0.05234 0.02269 91 640:   0%|          | 0/19 [00:00<?, ?it/s]52/99 7.3G 0.07815 0.05234 0.02269 91 640:   5%|▌         | 1/19 [00:00<00:09,  1.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07359 0.04302 0.02122 54 640:   5%|▌         | 1/19 [00:00<00:09,  1.96it/s]52/99 7.3G 0.07359 0.04302 0.02122 54 640:  11%|█         | 2/19 [00:00<00:07,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07092 0.03811 0.02098 45 640:  11%|█         | 2/19 [00:01<00:07,  2.39it/s]52/99 7.3G 0.07092 0.03811 0.02098 45 640:  16%|█▌        | 3/19 [00:01<00:05,  3.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.06997 0.03808 0.0191 62 640:  16%|█▌        | 3/19 [00:01<00:05,  3.10it/s] 52/99 7.3G 0.06997 0.03808 0.0191 62 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07102 0.04033 0.01838 103 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]52/99 7.3G 0.07102 0.04033 0.01838 103 640:  26%|██▋       | 5/19 [00:01<00:04,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07124 0.04184 0.01794 84 640:  26%|██▋       | 5/19 [00:01<00:04,  3.23it/s] 52/99 7.3G 0.07124 0.04184 0.01794 84 640:  32%|███▏      | 6/19 [00:01<00:03,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07079 0.04103 0.01814 56 640:  32%|███▏      | 6/19 [00:02<00:03,  3.65it/s]52/99 7.3G 0.07079 0.04103 0.01814 56 640:  37%|███▋      | 7/19 [00:02<00:02,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07127 0.03936 0.01818 51 640:  37%|███▋      | 7/19 [00:02<00:02,  4.11it/s]52/99 7.3G 0.07127 0.03936 0.01818 51 640:  42%|████▏     | 8/19 [00:02<00:02,  4.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07088 0.03898 0.01854 51 640:  42%|████▏     | 8/19 [00:02<00:02,  4.51it/s]52/99 7.3G 0.07088 0.03898 0.01854 51 640:  47%|████▋     | 9/19 [00:02<00:02,  4.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07059 0.03956 0.01837 64 640:  47%|████▋     | 9/19 [00:02<00:02,  4.83it/s]52/99 7.3G 0.07059 0.03956 0.01837 64 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07064 0.03964 0.01808 76 640:  53%|█████▎    | 10/19 [00:04<00:01,  4.52it/s]52/99 7.3G 0.07064 0.03964 0.01808 76 640:  58%|█████▊    | 11/19 [00:04<00:06,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07128 0.04223 0.01817 122 640:  58%|█████▊    | 11/19 [00:13<00:06,  1.31it/s]52/99 7.3G 0.07128 0.04223 0.01817 122 640:  63%|██████▎   | 12/19 [00:13<00:23,  3.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07071 0.04146 0.01783 53 640:  63%|██████▎   | 12/19 [00:13<00:23,  3.29s/it] 52/99 7.3G 0.07071 0.04146 0.01783 53 640:  68%|██████▊   | 13/19 [00:13<00:14,  2.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07074 0.041 0.01777 59 640:  68%|██████▊   | 13/19 [00:18<00:14,  2.34s/it]  52/99 7.3G 0.07074 0.041 0.01777 59 640:  74%|███████▎  | 14/19 [00:18<00:14,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07068 0.04098 0.01781 68 640:  74%|███████▎  | 14/19 [00:18<00:14,  2.96s/it]52/99 7.3G 0.07068 0.04098 0.01781 68 640:  79%|███████▉  | 15/19 [00:18<00:08,  2.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07107 0.04176 0.01802 82 640:  79%|███████▉  | 15/19 [00:26<00:08,  2.14s/it]52/99 7.3G 0.07107 0.04176 0.01802 82 640:  84%|████████▍ | 16/19 [00:26<00:11,  3.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07158 0.04191 0.01824 83 640:  84%|████████▍ | 16/19 [00:26<00:11,  3.77s/it]52/99 7.3G 0.07158 0.04191 0.01824 83 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07195 0.04207 0.01813 99 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.69s/it]52/99 7.3G 0.07195 0.04207 0.01813 99 640:  95%|█████████▍| 18/19 [00:26<00:01,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07204 0.04271 0.0179 102 640:  95%|█████████▍| 18/19 [00:29<00:01,  1.93s/it]52/99 7.3G 0.07204 0.04271 0.0179 102 640: 100%|██████████| 19/19 [00:29<00:00,  2.31s/it]52/99 7.3G 0.07204 0.04271 0.0179 102 640: 100%|██████████| 19/19 [00:29<00:00,  1.56s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.26s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.83s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]
                   all         55        256      0.241      0.196      0.162     0.0622
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07065 0.0277 0.0184 51 640:   0%|          | 0/19 [00:00<?, ?it/s]53/99 7.3G 0.07065 0.0277 0.0184 51 640:   5%|▌         | 1/19 [00:00<00:06,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07162 0.03083 0.02088 52 640:   5%|▌         | 1/19 [00:00<00:06,  2.88it/s]53/99 7.3G 0.07162 0.03083 0.02088 52 640:  11%|█         | 2/19 [00:00<00:05,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07138 0.02927 0.01876 46 640:  11%|█         | 2/19 [00:01<00:05,  2.87it/s]53/99 7.3G 0.07138 0.02927 0.01876 46 640:  16%|█▌        | 3/19 [00:01<00:07,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07464 0.03138 0.01829 77 640:  16%|█▌        | 3/19 [00:01<00:07,  2.20it/s]53/99 7.3G 0.07464 0.03138 0.01829 77 640:  21%|██        | 4/19 [00:01<00:06,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07542 0.03244 0.01993 56 640:  21%|██        | 4/19 [00:02<00:06,  2.28it/s]53/99 7.3G 0.07542 0.03244 0.01993 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07691 0.04037 0.01954 200 640:  26%|██▋       | 5/19 [00:02<00:05,  2.39it/s]53/99 7.3G 0.07691 0.04037 0.01954 200 640:  32%|███▏      | 6/19 [00:02<00:04,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07554 0.03831 0.01918 39 640:  32%|███▏      | 6/19 [00:02<00:04,  2.68it/s] 53/99 7.3G 0.07554 0.03831 0.01918 39 640:  37%|███▋      | 7/19 [00:02<00:03,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.0755 0.03997 0.01927 91 640:  37%|███▋      | 7/19 [00:02<00:03,  3.04it/s] 53/99 7.3G 0.0755 0.03997 0.01927 91 640:  42%|████▏     | 8/19 [00:02<00:03,  3.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07535 0.03987 0.01951 61 640:  42%|████▏     | 8/19 [00:08<00:03,  3.57it/s]53/99 7.3G 0.07535 0.03987 0.01951 61 640:  47%|████▋     | 9/19 [00:08<00:19,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07531 0.04068 0.01962 77 640:  47%|████▋     | 9/19 [00:08<00:19,  1.93s/it]53/99 7.3G 0.07531 0.04068 0.01962 77 640:  53%|█████▎    | 10/19 [00:08<00:13,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.075 0.04054 0.0194 67 640:  53%|█████▎    | 10/19 [00:12<00:13,  1.52s/it]   53/99 7.3G 0.075 0.04054 0.0194 67 640:  58%|█████▊    | 11/19 [00:12<00:17,  2.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07497 0.03944 0.01893 47 640:  58%|█████▊    | 11/19 [00:13<00:17,  2.23s/it]53/99 7.3G 0.07497 0.03944 0.01893 47 640:  63%|██████▎   | 12/19 [00:13<00:12,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07528 0.03921 0.01879 62 640:  63%|██████▎   | 12/19 [00:19<00:12,  1.75s/it]53/99 7.3G 0.07528 0.03921 0.01879 62 640:  68%|██████▊   | 13/19 [00:19<00:17,  2.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07446 0.03857 0.01857 52 640:  68%|██████▊   | 13/19 [00:19<00:17,  2.93s/it]53/99 7.3G 0.07446 0.03857 0.01857 52 640:  74%|███████▎  | 14/19 [00:19<00:10,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07414 0.0387 0.01852 63 640:  74%|███████▎  | 14/19 [00:20<00:10,  2.10s/it] 53/99 7.3G 0.07414 0.0387 0.01852 63 640:  79%|███████▉  | 15/19 [00:20<00:07,  1.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07405 0.03864 0.01847 69 640:  79%|███████▉  | 15/19 [00:24<00:07,  1.80s/it]53/99 7.3G 0.07405 0.03864 0.01847 69 640:  84%|████████▍ | 16/19 [00:24<00:07,  2.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07368 0.038 0.01834 43 640:  84%|████████▍ | 16/19 [00:32<00:07,  2.56s/it]  53/99 7.3G 0.07368 0.038 0.01834 43 640:  89%|████████▉ | 17/19 [00:32<00:08,  4.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.0735 0.03801 0.01825 65 640:  89%|████████▉ | 17/19 [00:32<00:08,  4.17s/it]53/99 7.3G 0.0735 0.03801 0.01825 65 640:  95%|█████████▍| 18/19 [00:32<00:02,  2.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07342 0.03749 0.01809 55 640:  95%|█████████▍| 18/19 [00:36<00:02,  2.97s/it]53/99 7.3G 0.07342 0.03749 0.01809 55 640: 100%|██████████| 19/19 [00:36<00:00,  3.23s/it]53/99 7.3G 0.07342 0.03749 0.01809 55 640: 100%|██████████| 19/19 [00:36<00:00,  1.93s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.99s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]
                   all         55        256      0.213        0.2      0.132     0.0475
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07402 0.06488 0.02004 112 640:   0%|          | 0/19 [00:00<?, ?it/s]54/99 7.3G 0.07402 0.06488 0.02004 112 640:   5%|▌         | 1/19 [00:00<00:03,  5.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07687 0.06483 0.01911 120 640:   5%|▌         | 1/19 [00:00<00:03,  5.55it/s]54/99 7.3G 0.07687 0.06483 0.01911 120 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07776 0.05598 0.01883 80 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s] 54/99 7.3G 0.07776 0.05598 0.01883 80 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07637 0.05495 0.01865 91 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]54/99 7.3G 0.07637 0.05495 0.01865 91 640:  21%|██        | 4/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07469 0.04955 0.01897 40 640:  21%|██        | 4/19 [00:00<00:02,  5.70it/s]54/99 7.3G 0.07469 0.04955 0.01897 40 640:  26%|██▋       | 5/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07354 0.04736 0.01866 53 640:  26%|██▋       | 5/19 [00:01<00:02,  5.72it/s]54/99 7.3G 0.07354 0.04736 0.01866 53 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07281 0.0458 0.01802 64 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s] 54/99 7.3G 0.07281 0.0458 0.01802 64 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07237 0.04542 0.01784 72 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]54/99 7.3G 0.07237 0.04542 0.01784 72 640:  42%|████▏     | 8/19 [00:01<00:01,  5.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07258 0.04586 0.01792 81 640:  42%|████▏     | 8/19 [00:01<00:01,  5.55it/s]54/99 7.3G 0.07258 0.04586 0.01792 81 640:  47%|████▋     | 9/19 [00:01<00:03,  3.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07238 0.04672 0.01817 97 640:  47%|████▋     | 9/19 [00:07<00:03,  3.32it/s]54/99 7.3G 0.07238 0.04672 0.01817 97 640:  53%|█████▎    | 10/19 [00:07<00:16,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07261 0.04759 0.01784 104 640:  53%|█████▎    | 10/19 [00:07<00:16,  1.79s/it]54/99 7.3G 0.07261 0.04759 0.01784 104 640:  58%|█████▊    | 11/19 [00:07<00:11,  1.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07347 0.04669 0.01788 81 640:  58%|█████▊    | 11/19 [00:08<00:11,  1.46s/it] 54/99 7.3G 0.07347 0.04669 0.01788 81 640:  63%|██████▎   | 12/19 [00:08<00:08,  1.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07388 0.04729 0.01787 107 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.28s/it]54/99 7.3G 0.07388 0.04729 0.01787 107 640:  68%|██████▊   | 13/19 [00:10<00:09,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07332 0.04686 0.01768 73 640:  68%|██████▊   | 13/19 [00:19<00:09,  1.56s/it] 54/99 7.3G 0.07332 0.04686 0.01768 73 640:  74%|███████▎  | 14/19 [00:19<00:19,  3.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07364 0.04663 0.01749 88 640:  74%|███████▎  | 14/19 [00:20<00:19,  3.82s/it]54/99 7.3G 0.07364 0.04663 0.01749 88 640:  79%|███████▉  | 15/19 [00:20<00:10,  2.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07351 0.04593 0.01771 60 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.73s/it]54/99 7.3G 0.07351 0.04593 0.01771 60 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07302 0.04544 0.01796 54 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.26s/it]54/99 7.3G 0.07302 0.04544 0.01796 54 640:  89%|████████▉ | 17/19 [00:25<00:05,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07283 0.04563 0.0185 78 640:  89%|████████▉ | 17/19 [00:29<00:05,  2.67s/it] 54/99 7.3G 0.07283 0.04563 0.0185 78 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07277 0.04484 0.01858 47 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.84s/it]54/99 7.3G 0.07277 0.04484 0.01858 47 640: 100%|██████████| 19/19 [00:29<00:00,  2.10s/it]54/99 7.3G 0.07277 0.04484 0.01858 47 640: 100%|██████████| 19/19 [00:29<00:00,  1.56s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.35s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]
                   all         55        256      0.208       0.22      0.142      0.054
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0739 0.04808 0.0132 83 640:   0%|          | 0/19 [00:00<?, ?it/s]55/99 7.3G 0.0739 0.04808 0.0132 83 640:   5%|▌         | 1/19 [00:00<00:09,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07439 0.04719 0.01551 77 640:   5%|▌         | 1/19 [00:00<00:09,  1.84it/s]55/99 7.3G 0.07439 0.04719 0.01551 77 640:  11%|█         | 2/19 [00:00<00:07,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07285 0.04274 0.01689 52 640:  11%|█         | 2/19 [00:01<00:07,  2.16it/s]55/99 7.3G 0.07285 0.04274 0.01689 52 640:  16%|█▌        | 3/19 [00:01<00:07,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07132 0.04157 0.01661 62 640:  16%|█▌        | 3/19 [00:02<00:07,  2.03it/s]55/99 7.3G 0.07132 0.04157 0.01661 62 640:  21%|██        | 4/19 [00:02<00:07,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07131 0.04017 0.01717 58 640:  21%|██        | 4/19 [00:02<00:07,  1.94it/s]55/99 7.3G 0.07131 0.04017 0.01717 58 640:  26%|██▋       | 5/19 [00:02<00:06,  2.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07199 0.04077 0.01834 64 640:  26%|██▋       | 5/19 [00:03<00:06,  2.07it/s]55/99 7.3G 0.07199 0.04077 0.01834 64 640:  32%|███▏      | 6/19 [00:03<00:07,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07163 0.04074 0.01896 62 640:  32%|███▏      | 6/19 [00:03<00:07,  1.83it/s]55/99 7.3G 0.07163 0.04074 0.01896 62 640:  37%|███▋      | 7/19 [00:03<00:07,  1.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0708 0.03975 0.01909 48 640:  37%|███▋      | 7/19 [00:04<00:07,  1.71it/s] 55/99 7.3G 0.0708 0.03975 0.01909 48 640:  42%|████▏     | 8/19 [00:04<00:05,  1.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07105 0.04026 0.019 79 640:  42%|████▏     | 8/19 [00:04<00:05,  1.90it/s] 55/99 7.3G 0.07105 0.04026 0.019 79 640:  47%|████▋     | 9/19 [00:04<00:04,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07124 0.04302 0.0189 116 640:  47%|████▋     | 9/19 [00:04<00:04,  2.14it/s]55/99 7.3G 0.07124 0.04302 0.0189 116 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07211 0.0475 0.01892 172 640:  53%|█████▎    | 10/19 [00:14<00:03,  2.39it/s]55/99 7.3G 0.07211 0.0475 0.01892 172 640:  58%|█████▊    | 11/19 [00:14<00:25,  3.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07181 0.04654 0.01882 61 640:  58%|█████▊    | 11/19 [00:14<00:25,  3.19s/it]55/99 7.3G 0.07181 0.04654 0.01882 61 640:  63%|██████▎   | 12/19 [00:14<00:15,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07186 0.04521 0.01893 45 640:  63%|██████▎   | 12/19 [00:19<00:15,  2.27s/it]55/99 7.3G 0.07186 0.04521 0.01893 45 640:  68%|██████▊   | 13/19 [00:19<00:17,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07184 0.0446 0.01869 70 640:  68%|██████▊   | 13/19 [00:19<00:17,  2.96s/it] 55/99 7.3G 0.07184 0.0446 0.01869 70 640:  74%|███████▎  | 14/19 [00:19<00:11,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07204 0.04364 0.01901 50 640:  74%|███████▎  | 14/19 [00:24<00:11,  2.29s/it]55/99 7.3G 0.07204 0.04364 0.01901 50 640:  79%|███████▉  | 15/19 [00:24<00:12,  3.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07219 0.04427 0.01897 97 640:  79%|███████▉  | 15/19 [00:25<00:12,  3.13s/it]55/99 7.3G 0.07219 0.04427 0.01897 97 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07234 0.04381 0.01899 60 640:  84%|████████▍ | 16/19 [00:26<00:07,  2.45s/it]55/99 7.3G 0.07234 0.04381 0.01899 60 640:  89%|████████▉ | 17/19 [00:26<00:03,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07238 0.04331 0.01908 55 640:  89%|████████▉ | 17/19 [00:27<00:03,  1.96s/it]55/99 7.3G 0.07238 0.04331 0.01908 55 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07212 0.04309 0.01884 66 640:  95%|█████████▍| 18/19 [00:35<00:01,  1.67s/it]55/99 7.3G 0.07212 0.04309 0.01884 66 640: 100%|██████████| 19/19 [00:35<00:00,  3.47s/it]55/99 7.3G 0.07212 0.04309 0.01884 66 640: 100%|██████████| 19/19 [00:35<00:00,  1.85s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.33s/it]
                   all         55        256      0.247      0.191      0.155     0.0581
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07634 0.05151 0.02322 89 640:   0%|          | 0/19 [00:00<?, ?it/s]56/99 7.3G 0.07634 0.05151 0.02322 89 640:   5%|▌         | 1/19 [00:00<00:03,  4.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07478 0.04218 0.02074 65 640:   5%|▌         | 1/19 [00:00<00:03,  4.96it/s]56/99 7.3G 0.07478 0.04218 0.02074 65 640:  11%|█         | 2/19 [00:00<00:03,  5.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07299 0.0366 0.02064 38 640:  11%|█         | 2/19 [00:00<00:03,  5.37it/s] 56/99 7.3G 0.07299 0.0366 0.02064 38 640:  16%|█▌        | 3/19 [00:00<00:02,  5.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07392 0.04278 0.0202 111 640:  16%|█▌        | 3/19 [00:00<00:02,  5.54it/s]56/99 7.3G 0.07392 0.04278 0.0202 111 640:  21%|██        | 4/19 [00:00<00:03,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07378 0.04049 0.01958 54 640:  21%|██        | 4/19 [00:01<00:03,  4.88it/s]56/99 7.3G 0.07378 0.04049 0.01958 54 640:  26%|██▋       | 5/19 [00:01<00:03,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07277 0.03982 0.01919 65 640:  26%|██▋       | 5/19 [00:01<00:03,  3.92it/s]56/99 7.3G 0.07277 0.03982 0.01919 65 640:  32%|███▏      | 6/19 [00:01<00:03,  3.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07346 0.04215 0.01907 97 640:  32%|███▏      | 6/19 [00:01<00:03,  3.50it/s]56/99 7.3G 0.07346 0.04215 0.01907 97 640:  37%|███▋      | 7/19 [00:01<00:03,  3.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07349 0.04489 0.01934 108 640:  37%|███▋      | 7/19 [00:02<00:03,  3.44it/s]56/99 7.3G 0.07349 0.04489 0.01934 108 640:  42%|████▏     | 8/19 [00:02<00:03,  3.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07322 0.04504 0.01894 74 640:  42%|████▏     | 8/19 [00:02<00:03,  3.26it/s] 56/99 7.3G 0.07322 0.04504 0.01894 74 640:  47%|████▋     | 9/19 [00:02<00:03,  3.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07301 0.0447 0.01905 62 640:  47%|████▋     | 9/19 [00:03<00:03,  3.10it/s] 56/99 7.3G 0.07301 0.0447 0.01905 62 640:  53%|█████▎    | 10/19 [00:03<00:05,  1.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07294 0.04498 0.01893 77 640:  53%|█████▎    | 10/19 [00:04<00:05,  1.55it/s]56/99 7.3G 0.07294 0.04498 0.01893 77 640:  58%|█████▊    | 11/19 [00:04<00:05,  1.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07252 0.04447 0.01882 69 640:  58%|█████▊    | 11/19 [00:10<00:05,  1.57it/s]56/99 7.3G 0.07252 0.04447 0.01882 69 640:  63%|██████▎   | 12/19 [00:10<00:16,  2.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07202 0.04332 0.01885 45 640:  63%|██████▎   | 12/19 [00:10<00:16,  2.34s/it]56/99 7.3G 0.07202 0.04332 0.01885 45 640:  68%|██████▊   | 13/19 [00:10<00:10,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07216 0.04252 0.01895 59 640:  68%|██████▊   | 13/19 [00:12<00:10,  1.68s/it]56/99 7.3G 0.07216 0.04252 0.01895 59 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07185 0.04287 0.0189 84 640:  74%|███████▎  | 14/19 [00:17<00:08,  1.61s/it] 56/99 7.3G 0.07185 0.04287 0.0189 84 640:  79%|███████▉  | 15/19 [00:17<00:10,  2.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07171 0.04239 0.01874 57 640:  79%|███████▉  | 15/19 [00:25<00:10,  2.60s/it]56/99 7.3G 0.07171 0.04239 0.01874 57 640:  84%|████████▍ | 16/19 [00:25<00:13,  4.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07231 0.04337 0.01868 127 640:  84%|████████▍ | 16/19 [00:26<00:13,  4.42s/it]56/99 7.3G 0.07231 0.04337 0.01868 127 640:  89%|████████▉ | 17/19 [00:26<00:06,  3.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07222 0.04466 0.01886 115 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.21s/it]56/99 7.3G 0.07222 0.04466 0.01886 115 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07227 0.04414 0.01884 59 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.31s/it] 56/99 7.3G 0.07227 0.04414 0.01884 59 640: 100%|██████████| 19/19 [00:30<00:00,  2.52s/it]56/99 7.3G 0.07227 0.04414 0.01884 59 640: 100%|██████████| 19/19 [00:30<00:00,  1.60s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]
                   all         55        256      0.251      0.175      0.153      0.056
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.06764 0.04439 0.01674 75 640:   0%|          | 0/19 [00:00<?, ?it/s]57/99 7.3G 0.06764 0.04439 0.01674 75 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07651 0.06289 0.01706 195 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]57/99 7.3G 0.07651 0.06289 0.01706 195 640:  11%|█         | 2/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07664 0.05546 0.01602 93 640:  11%|█         | 2/19 [00:00<00:02,  5.70it/s] 57/99 7.3G 0.07664 0.05546 0.01602 93 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07448 0.04981 0.01692 50 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]57/99 7.3G 0.07448 0.04981 0.01692 50 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07327 0.04646 0.01674 51 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]57/99 7.3G 0.07327 0.04646 0.01674 51 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07417 0.04805 0.0173 94 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s] 57/99 7.3G 0.07417 0.04805 0.0173 94 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07368 0.04824 0.01698 83 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]57/99 7.3G 0.07368 0.04824 0.01698 83 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07391 0.0467 0.01681 63 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s] 57/99 7.3G 0.07391 0.0467 0.01681 63 640:  42%|████▏     | 8/19 [00:01<00:01,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07378 0.0474 0.01697 88 640:  42%|████▏     | 8/19 [00:02<00:01,  5.76it/s]57/99 7.3G 0.07378 0.0474 0.01697 88 640:  47%|████▋     | 9/19 [00:02<00:05,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07388 0.04672 0.01791 67 640:  47%|████▋     | 9/19 [00:03<00:05,  1.82it/s]57/99 7.3G 0.07388 0.04672 0.01791 67 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07322 0.04595 0.01836 70 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.18it/s]57/99 7.3G 0.07322 0.04595 0.01836 70 640:  58%|█████▊    | 11/19 [00:05<00:09,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07302 0.04563 0.01832 69 640:  58%|█████▊    | 11/19 [00:09<00:09,  1.18s/it]57/99 7.3G 0.07302 0.04563 0.01832 69 640:  63%|██████▎   | 12/19 [00:09<00:12,  1.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07273 0.04523 0.0185 75 640:  63%|██████▎   | 12/19 [00:18<00:12,  1.83s/it] 57/99 7.3G 0.07273 0.04523 0.0185 75 640:  68%|██████▊   | 13/19 [00:18<00:25,  4.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07249 0.0445 0.01831 61 640:  68%|██████▊   | 13/19 [00:19<00:25,  4.24s/it]57/99 7.3G 0.07249 0.0445 0.01831 61 640:  74%|███████▎  | 14/19 [00:19<00:16,  3.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07191 0.04392 0.01831 54 640:  74%|███████▎  | 14/19 [00:23<00:16,  3.25s/it]57/99 7.3G 0.07191 0.04392 0.01831 54 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.0721 0.04498 0.01815 104 640:  79%|███████▉  | 15/19 [00:23<00:13,  3.27s/it]57/99 7.3G 0.0721 0.04498 0.01815 104 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07221 0.04533 0.01819 82 640:  84%|████████▍ | 16/19 [00:26<00:07,  2.50s/it]57/99 7.3G 0.07221 0.04533 0.01819 82 640:  89%|████████▉ | 17/19 [00:26<00:04,  2.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07262 0.04544 0.01807 126 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.49s/it]57/99 7.3G 0.07262 0.04544 0.01807 126 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07286 0.04561 0.01806 83 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.95s/it] 57/99 7.3G 0.07286 0.04561 0.01806 83 640: 100%|██████████| 19/19 [00:27<00:00,  1.49s/it]57/99 7.3G 0.07286 0.04561 0.01806 83 640: 100%|██████████| 19/19 [00:27<00:00,  1.45s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  6.66s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.45s/it]
                   all         55        256      0.281      0.199      0.161     0.0606
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.066 0.0286 0.01554 50 640:   0%|          | 0/19 [00:00<?, ?it/s]58/99 7.3G 0.066 0.0286 0.01554 50 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.06874 0.03932 0.0163 89 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]58/99 7.3G 0.06874 0.03932 0.0163 89 640:  11%|█         | 2/19 [00:00<00:06,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.06995 0.04498 0.01715 100 640:  11%|█         | 2/19 [00:01<00:06,  2.79it/s]58/99 7.3G 0.06995 0.04498 0.01715 100 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07011 0.04419 0.01689 73 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s] 58/99 7.3G 0.07011 0.04419 0.01689 73 640:  21%|██        | 4/19 [00:01<00:06,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07188 0.04083 0.01707 55 640:  21%|██        | 4/19 [00:01<00:06,  2.43it/s]58/99 7.3G 0.07188 0.04083 0.01707 55 640:  26%|██▋       | 5/19 [00:01<00:05,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07243 0.0395 0.0174 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.55it/s]  58/99 7.3G 0.07243 0.0395 0.0174 56 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07167 0.0386 0.01707 57 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s]58/99 7.3G 0.07167 0.0386 0.01707 57 640:  37%|███▋      | 7/19 [00:02<00:05,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07229 0.04075 0.01705 115 640:  37%|███▋      | 7/19 [00:03<00:05,  2.30it/s]58/99 7.3G 0.07229 0.04075 0.01705 115 640:  42%|████▏     | 8/19 [00:03<00:05,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07278 0.04454 0.01727 148 640:  42%|████▏     | 8/19 [00:04<00:05,  1.89it/s]58/99 7.3G 0.07278 0.04454 0.01727 148 640:  47%|████▋     | 9/19 [00:04<00:05,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07243 0.04363 0.01798 54 640:  47%|████▋     | 9/19 [00:13<00:05,  1.80it/s] 58/99 7.3G 0.07243 0.04363 0.01798 54 640:  53%|█████▎    | 10/19 [00:13<00:30,  3.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07303 0.0432 0.01868 68 640:  53%|█████▎    | 10/19 [00:14<00:30,  3.34s/it] 58/99 7.3G 0.07303 0.0432 0.01868 68 640:  58%|█████▊    | 11/19 [00:14<00:20,  2.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07266 0.04219 0.01866 49 640:  58%|█████▊    | 11/19 [00:17<00:20,  2.57s/it]58/99 7.3G 0.07266 0.04219 0.01866 49 640:  63%|██████▎   | 12/19 [00:17<00:17,  2.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07333 0.04321 0.01895 106 640:  63%|██████▎   | 12/19 [00:17<00:17,  2.55s/it]58/99 7.3G 0.07333 0.04321 0.01895 106 640:  68%|██████▊   | 13/19 [00:17<00:11,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07343 0.04289 0.01886 63 640:  68%|██████▊   | 13/19 [00:23<00:11,  1.97s/it] 58/99 7.3G 0.07343 0.04289 0.01886 63 640:  74%|███████▎  | 14/19 [00:23<00:16,  3.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07331 0.04277 0.01858 79 640:  74%|███████▎  | 14/19 [00:24<00:16,  3.24s/it]58/99 7.3G 0.07331 0.04277 0.01858 79 640:  79%|███████▉  | 15/19 [00:24<00:09,  2.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07383 0.04345 0.01878 123 640:  79%|███████▉  | 15/19 [00:25<00:09,  2.48s/it]58/99 7.3G 0.07383 0.04345 0.01878 123 640:  84%|████████▍ | 16/19 [00:25<00:05,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07396 0.04492 0.01864 138 640:  84%|████████▍ | 16/19 [00:26<00:05,  1.99s/it]58/99 7.3G 0.07396 0.04492 0.01864 138 640:  89%|████████▉ | 17/19 [00:26<00:03,  1.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07355 0.04439 0.01881 58 640:  89%|████████▉ | 17/19 [00:36<00:03,  1.73s/it] 58/99 7.3G 0.07355 0.04439 0.01881 58 640:  95%|█████████▍| 18/19 [00:36<00:04,  4.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07383 0.04607 0.01884 140 640:  95%|█████████▍| 18/19 [00:36<00:04,  4.10s/it]58/99 7.3G 0.07383 0.04607 0.01884 140 640: 100%|██████████| 19/19 [00:36<00:00,  2.97s/it]58/99 7.3G 0.07383 0.04607 0.01884 140 640: 100%|██████████| 19/19 [00:36<00:00,  1.92s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.60s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.54s/it]
                   all         55        256      0.258      0.198      0.159      0.059
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07918 0.04983 0.01727 90 640:   0%|          | 0/19 [00:00<?, ?it/s]59/99 7.3G 0.07918 0.04983 0.01727 90 640:   5%|▌         | 1/19 [00:00<00:11,  1.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07603 0.05317 0.01904 115 640:   5%|▌         | 1/19 [00:01<00:11,  1.56it/s]59/99 7.3G 0.07603 0.05317 0.01904 115 640:  11%|█         | 2/19 [00:01<00:10,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07276 0.04767 0.01885 58 640:  11%|█         | 2/19 [00:01<00:10,  1.66it/s] 59/99 7.3G 0.07276 0.04767 0.01885 58 640:  16%|█▌        | 3/19 [00:01<00:08,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07394 0.04726 0.01843 97 640:  16%|█▌        | 3/19 [00:02<00:08,  1.80it/s]59/99 7.3G 0.07394 0.04726 0.01843 97 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07436 0.05111 0.01922 127 640:  21%|██        | 4/19 [00:02<00:07,  2.06it/s]59/99 7.3G 0.07436 0.05111 0.01922 127 640:  26%|██▋       | 5/19 [00:02<00:05,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07245 0.04818 0.01837 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.56it/s] 59/99 7.3G 0.07245 0.04818 0.01837 56 640:  32%|███▏      | 6/19 [00:02<00:04,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07316 0.05036 0.01823 135 640:  32%|███▏      | 6/19 [00:02<00:04,  2.96it/s]59/99 7.3G 0.07316 0.05036 0.01823 135 640:  37%|███▋      | 7/19 [00:02<00:03,  3.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07248 0.04865 0.01764 66 640:  37%|███▋      | 7/19 [00:03<00:03,  3.32it/s] 59/99 7.3G 0.07248 0.04865 0.01764 66 640:  42%|████▏     | 8/19 [00:03<00:03,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07173 0.04759 0.0183 58 640:  42%|████▏     | 8/19 [00:03<00:03,  3.55it/s] 59/99 7.3G 0.07173 0.04759 0.0183 58 640:  47%|████▋     | 9/19 [00:03<00:02,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07101 0.04752 0.01788 79 640:  47%|████▋     | 9/19 [00:03<00:02,  3.39it/s]59/99 7.3G 0.07101 0.04752 0.01788 79 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07136 0.05019 0.0177 140 640:  53%|█████▎    | 10/19 [00:10<00:02,  3.88it/s]59/99 7.3G 0.07136 0.05019 0.0177 140 640:  58%|█████▊    | 11/19 [00:10<00:18,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07193 0.04833 0.01767 53 640:  58%|█████▊    | 11/19 [00:10<00:18,  2.29s/it]59/99 7.3G 0.07193 0.04833 0.01767 53 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07183 0.04792 0.01744 74 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.67s/it]59/99 7.3G 0.07183 0.04792 0.01744 74 640:  68%|██████▊   | 13/19 [00:10<00:07,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.0717 0.04686 0.01759 57 640:  68%|██████▊   | 13/19 [00:13<00:07,  1.25s/it] 59/99 7.3G 0.0717 0.04686 0.01759 57 640:  74%|███████▎  | 14/19 [00:13<00:08,  1.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07166 0.0477 0.01775 94 640:  74%|███████▎  | 14/19 [00:23<00:08,  1.70s/it]59/99 7.3G 0.07166 0.0477 0.01775 94 640:  79%|███████▉  | 15/19 [00:23<00:16,  4.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.0712 0.04672 0.01775 47 640:  79%|███████▉  | 15/19 [00:24<00:16,  4.24s/it]59/99 7.3G 0.0712 0.04672 0.01775 47 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07115 0.04661 0.01769 75 640:  84%|████████▍ | 16/19 [00:27<00:09,  3.15s/it]59/99 7.3G 0.07115 0.04661 0.01769 75 640:  89%|████████▉ | 17/19 [00:27<00:05,  3.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07193 0.04913 0.01775 217 640:  89%|████████▉ | 17/19 [00:27<00:05,  3.00s/it]59/99 7.3G 0.07193 0.04913 0.01775 217 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07169 0.0489 0.01756 77 640:  95%|█████████▍| 18/19 [00:32<00:02,  2.24s/it]  59/99 7.3G 0.07169 0.0489 0.01756 77 640: 100%|██████████| 19/19 [00:32<00:00,  2.96s/it]59/99 7.3G 0.07169 0.0489 0.01756 77 640: 100%|██████████| 19/19 [00:32<00:00,  1.69s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.81s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.00s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.72s/it]
                   all         55        256      0.256      0.204      0.171     0.0591
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08293 0.06033 0.019 127 640:   0%|          | 0/19 [00:00<?, ?it/s]60/99 7.3G 0.08293 0.06033 0.019 127 640:   5%|▌         | 1/19 [00:00<00:06,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07966 0.04692 0.02082 54 640:   5%|▌         | 1/19 [00:00<00:06,  2.82it/s]60/99 7.3G 0.07966 0.04692 0.02082 54 640:  11%|█         | 2/19 [00:00<00:05,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07897 0.0449 0.02004 86 640:  11%|█         | 2/19 [00:01<00:05,  2.86it/s] 60/99 7.3G 0.07897 0.0449 0.02004 86 640:  16%|█▌        | 3/19 [00:01<00:05,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07892 0.05354 0.01919 172 640:  16%|█▌        | 3/19 [00:01<00:05,  2.84it/s]60/99 7.3G 0.07892 0.05354 0.01919 172 640:  21%|██        | 4/19 [00:01<00:06,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0774 0.04966 0.01934 55 640:  21%|██        | 4/19 [00:02<00:06,  2.39it/s]  60/99 7.3G 0.0774 0.04966 0.01934 55 640:  26%|██▋       | 5/19 [00:02<00:06,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07793 0.05019 0.01925 134 640:  26%|██▋       | 5/19 [00:02<00:06,  2.20it/s]60/99 7.3G 0.07793 0.05019 0.01925 134 640:  32%|███▏      | 6/19 [00:02<00:06,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0784 0.05029 0.01893 98 640:  32%|███▏      | 6/19 [00:03<00:06,  2.06it/s]  60/99 7.3G 0.0784 0.05029 0.01893 98 640:  37%|███▋      | 7/19 [00:03<00:05,  2.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07803 0.05007 0.01859 94 640:  37%|███▋      | 7/19 [00:03<00:05,  2.09it/s]60/99 7.3G 0.07803 0.05007 0.01859 94 640:  42%|████▏     | 8/19 [00:03<00:04,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07706 0.04816 0.01906 54 640:  42%|████▏     | 8/19 [00:03<00:04,  2.29it/s]60/99 7.3G 0.07706 0.04816 0.01906 54 640:  47%|████▋     | 9/19 [00:03<00:03,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0775 0.05228 0.01875 218 640:  47%|████▋     | 9/19 [00:04<00:03,  2.76it/s]60/99 7.3G 0.0775 0.05228 0.01875 218 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07693 0.05105 0.01902 57 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.68it/s]60/99 7.3G 0.07693 0.05105 0.01902 57 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07622 0.04907 0.01887 46 640:  58%|█████▊    | 11/19 [00:15<00:02,  2.69it/s]60/99 7.3G 0.07622 0.04907 0.01887 46 640:  63%|██████▎   | 12/19 [00:15<00:24,  3.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07523 0.04661 0.01848 27 640:  63%|██████▎   | 12/19 [00:16<00:24,  3.55s/it]60/99 7.3G 0.07523 0.04661 0.01848 27 640:  68%|██████▊   | 13/19 [00:16<00:16,  2.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07493 0.04653 0.01844 82 640:  68%|██████▊   | 13/19 [00:20<00:16,  2.71s/it]60/99 7.3G 0.07493 0.04653 0.01844 82 640:  74%|███████▎  | 14/19 [00:20<00:15,  3.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07377 0.04515 0.01829 41 640:  74%|███████▎  | 14/19 [00:20<00:15,  3.14s/it]60/99 7.3G 0.07377 0.04515 0.01829 41 640:  79%|███████▉  | 15/19 [00:20<00:09,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07355 0.04426 0.0182 56 640:  79%|███████▉  | 15/19 [00:25<00:09,  2.28s/it] 60/99 7.3G 0.07355 0.04426 0.0182 56 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07337 0.04453 0.01833 94 640:  84%|████████▍ | 16/19 [00:26<00:09,  3.06s/it]60/99 7.3G 0.07337 0.04453 0.01833 94 640:  89%|████████▉ | 17/19 [00:26<00:05,  2.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07301 0.04369 0.01829 50 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.55s/it]60/99 7.3G 0.07301 0.04369 0.01829 50 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.07296 0.04326 0.01821 62 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.17s/it]60/99 7.3G 0.07296 0.04326 0.01821 62 640: 100%|██████████| 19/19 [00:29<00:00,  1.90s/it]60/99 7.3G 0.07296 0.04326 0.01821 62 640: 100%|██████████| 19/19 [00:29<00:00,  1.54s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]
                   all         55        256        0.3      0.204      0.171     0.0631
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07732 0.05216 0.01966 103 640:   0%|          | 0/19 [00:00<?, ?it/s]61/99 7.3G 0.07732 0.05216 0.01966 103 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.072 0.0426 0.01868 58 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s]    61/99 7.3G 0.072 0.0426 0.01868 58 640:  11%|█         | 2/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.0714 0.04823 0.01792 98 640:  11%|█         | 2/19 [00:01<00:06,  2.81it/s]61/99 7.3G 0.0714 0.04823 0.01792 98 640:  16%|█▌        | 3/19 [00:01<00:05,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07168 0.04768 0.01867 77 640:  16%|█▌        | 3/19 [00:01<00:05,  2.77it/s]61/99 7.3G 0.07168 0.04768 0.01867 77 640:  21%|██        | 4/19 [00:01<00:04,  3.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07166 0.04648 0.01855 71 640:  21%|██        | 4/19 [00:01<00:04,  3.26it/s]61/99 7.3G 0.07166 0.04648 0.01855 71 640:  26%|██▋       | 5/19 [00:01<00:03,  3.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07113 0.0458 0.01916 74 640:  26%|██▋       | 5/19 [00:01<00:03,  3.68it/s] 61/99 7.3G 0.07113 0.0458 0.01916 74 640:  32%|███▏      | 6/19 [00:01<00:03,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07139 0.04626 0.01942 76 640:  32%|███▏      | 6/19 [00:01<00:03,  3.97it/s]61/99 7.3G 0.07139 0.04626 0.01942 76 640:  37%|███▋      | 7/19 [00:01<00:02,  4.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07102 0.04606 0.01879 77 640:  37%|███▋      | 7/19 [00:02<00:02,  4.18it/s]61/99 7.3G 0.07102 0.04606 0.01879 77 640:  42%|████▏     | 8/19 [00:02<00:02,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07158 0.04703 0.01887 93 640:  42%|████▏     | 8/19 [00:04<00:02,  4.22it/s]61/99 7.3G 0.07158 0.04703 0.01887 93 640:  47%|████▋     | 9/19 [00:04<00:07,  1.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07145 0.0463 0.0189 61 640:  47%|████▋     | 9/19 [00:05<00:07,  1.35it/s]  61/99 7.3G 0.07145 0.0463 0.0189 61 640:  53%|█████▎    | 10/19 [00:05<00:09,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07119 0.04935 0.01854 134 640:  53%|█████▎    | 10/19 [00:10<00:09,  1.03s/it]61/99 7.3G 0.07119 0.04935 0.01854 134 640:  58%|█████▊    | 11/19 [00:10<00:17,  2.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07204 0.0485 0.01802 95 640:  58%|█████▊    | 11/19 [00:10<00:17,  2.18s/it]  61/99 7.3G 0.07204 0.0485 0.01802 95 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.0721 0.04762 0.01792 62 640:  63%|██████▎   | 12/19 [00:18<00:11,  1.63s/it]61/99 7.3G 0.0721 0.04762 0.01792 62 640:  68%|██████▊   | 13/19 [00:18<00:20,  3.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.0719 0.04664 0.01789 54 640:  68%|██████▊   | 13/19 [00:18<00:20,  3.34s/it]61/99 7.3G 0.0719 0.04664 0.01789 54 640:  74%|███████▎  | 14/19 [00:18<00:12,  2.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07175 0.04632 0.01791 71 640:  74%|███████▎  | 14/19 [00:19<00:12,  2.51s/it]61/99 7.3G 0.07175 0.04632 0.01791 71 640:  79%|███████▉  | 15/19 [00:19<00:07,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07199 0.04557 0.01782 86 640:  79%|███████▉  | 15/19 [00:22<00:07,  1.97s/it]61/99 7.3G 0.07199 0.04557 0.01782 86 640:  84%|████████▍ | 16/19 [00:22<00:06,  2.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07124 0.04505 0.0178 58 640:  84%|████████▍ | 16/19 [00:28<00:06,  2.23s/it] 61/99 7.3G 0.07124 0.04505 0.0178 58 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.0719 0.04768 0.01797 178 640:  89%|████████▉ | 17/19 [00:30<00:06,  3.35s/it]61/99 7.3G 0.0719 0.04768 0.01797 178 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07173 0.04684 0.01793 47 640:  95%|█████████▍| 18/19 [00:33<00:02,  2.89s/it]61/99 7.3G 0.07173 0.04684 0.01793 47 640: 100%|██████████| 19/19 [00:33<00:00,  3.02s/it]61/99 7.3G 0.07173 0.04684 0.01793 47 640: 100%|██████████| 19/19 [00:33<00:00,  1.76s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.26s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]
                   all         55        256      0.222      0.247       0.18     0.0656
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07509 0.04812 0.01757 84 640:   0%|          | 0/19 [00:00<?, ?it/s]62/99 7.3G 0.07509 0.04812 0.01757 84 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07553 0.04147 0.01744 72 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s]62/99 7.3G 0.07553 0.04147 0.01744 72 640:  11%|█         | 2/19 [00:00<00:05,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.0742 0.04513 0.01688 97 640:  11%|█         | 2/19 [00:00<00:05,  3.29it/s] 62/99 7.3G 0.0742 0.04513 0.01688 97 640:  16%|█▌        | 3/19 [00:00<00:04,  3.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07626 0.04913 0.01827 141 640:  16%|█▌        | 3/19 [00:01<00:04,  3.59it/s]62/99 7.3G 0.07626 0.04913 0.01827 141 640:  21%|██        | 4/19 [00:01<00:04,  3.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07635 0.04701 0.01768 74 640:  21%|██        | 4/19 [00:01<00:04,  3.56it/s] 62/99 7.3G 0.07635 0.04701 0.01768 74 640:  26%|██▋       | 5/19 [00:01<00:03,  4.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07527 0.04639 0.01813 71 640:  26%|██▋       | 5/19 [00:01<00:03,  4.13it/s]62/99 7.3G 0.07527 0.04639 0.01813 71 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07465 0.04491 0.01799 65 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]62/99 7.3G 0.07465 0.04491 0.01799 65 640:  37%|███▋      | 7/19 [00:01<00:02,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07483 0.04623 0.01773 113 640:  37%|███▋      | 7/19 [00:01<00:02,  4.88it/s]62/99 7.3G 0.07483 0.04623 0.01773 113 640:  42%|████▏     | 8/19 [00:01<00:02,  5.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07409 0.04507 0.01735 63 640:  42%|████▏     | 8/19 [00:02<00:02,  5.13it/s] 62/99 7.3G 0.07409 0.04507 0.01735 63 640:  47%|████▋     | 9/19 [00:02<00:01,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.0742 0.0446 0.01774 70 640:  47%|████▋     | 9/19 [00:07<00:01,  5.31it/s]  62/99 7.3G 0.0742 0.0446 0.01774 70 640:  53%|█████▎    | 10/19 [00:07<00:15,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07461 0.04537 0.01762 116 640:  53%|█████▎    | 10/19 [00:07<00:15,  1.67s/it]62/99 7.3G 0.07461 0.04537 0.01762 116 640:  58%|█████▊    | 11/19 [00:07<00:09,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07412 0.04513 0.01772 68 640:  58%|█████▊    | 11/19 [00:09<00:09,  1.21s/it] 62/99 7.3G 0.07412 0.04513 0.01772 68 640:  63%|██████▎   | 12/19 [00:09<00:11,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07467 0.04508 0.01759 94 640:  63%|██████▎   | 12/19 [00:11<00:11,  1.58s/it]62/99 7.3G 0.07467 0.04508 0.01759 94 640:  68%|██████▊   | 13/19 [00:11<00:10,  1.74s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07433 0.0445 0.01758 57 640:  68%|██████▊   | 13/19 [00:18<00:10,  1.74s/it] 62/99 7.3G 0.07433 0.0445 0.01758 57 640:  74%|███████▎  | 14/19 [00:18<00:16,  3.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07421 0.04455 0.0179 65 640:  74%|███████▎  | 14/19 [00:22<00:16,  3.34s/it]62/99 7.3G 0.07421 0.04455 0.0179 65 640:  79%|███████▉  | 15/19 [00:22<00:14,  3.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07389 0.04377 0.01766 52 640:  79%|███████▉  | 15/19 [00:25<00:14,  3.57s/it]62/99 7.3G 0.07389 0.04377 0.01766 52 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07346 0.04281 0.01739 47 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.24s/it]62/99 7.3G 0.07346 0.04281 0.01739 47 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07339 0.04302 0.01729 85 640:  89%|████████▉ | 17/19 [00:31<00:04,  2.37s/it]62/99 7.3G 0.07339 0.04302 0.01729 85 640:  95%|█████████▍| 18/19 [00:31<00:03,  3.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.07325 0.04271 0.01718 65 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.37s/it]62/99 7.3G 0.07325 0.04271 0.01718 65 640: 100%|██████████| 19/19 [00:32<00:00,  2.72s/it]62/99 7.3G 0.07325 0.04271 0.01718 65 640: 100%|██████████| 19/19 [00:32<00:00,  1.72s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.21s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.27s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.16s/it]
                   all         55        256      0.273      0.222      0.169     0.0568
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.06771 0.04154 0.01249 79 640:   0%|          | 0/19 [00:00<?, ?it/s]63/99 7.3G 0.06771 0.04154 0.01249 79 640:   5%|▌         | 1/19 [00:00<00:06,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.06605 0.03313 0.01448 36 640:   5%|▌         | 1/19 [00:00<00:06,  2.78it/s]63/99 7.3G 0.06605 0.03313 0.01448 36 640:  11%|█         | 2/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.06922 0.03659 0.0163 71 640:  11%|█         | 2/19 [00:01<00:06,  2.81it/s] 63/99 7.3G 0.06922 0.03659 0.0163 71 640:  16%|█▌        | 3/19 [00:01<00:05,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.0696 0.03938 0.01688 86 640:  16%|█▌        | 3/19 [00:01<00:05,  2.82it/s]63/99 7.3G 0.0696 0.03938 0.01688 86 640:  21%|██        | 4/19 [00:01<00:05,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07005 0.04153 0.01731 84 640:  21%|██        | 4/19 [00:01<00:05,  2.83it/s]63/99 7.3G 0.07005 0.04153 0.01731 84 640:  26%|██▋       | 5/19 [00:01<00:04,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07006 0.04177 0.01706 77 640:  26%|██▋       | 5/19 [00:02<00:04,  2.84it/s]63/99 7.3G 0.07006 0.04177 0.01706 77 640:  32%|███▏      | 6/19 [00:02<00:04,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07016 0.04093 0.01691 61 640:  32%|███▏      | 6/19 [00:02<00:04,  2.85it/s]63/99 7.3G 0.07016 0.04093 0.01691 61 640:  37%|███▋      | 7/19 [00:02<00:04,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07146 0.04107 0.01734 82 640:  37%|███▋      | 7/19 [00:02<00:04,  2.95it/s]63/99 7.3G 0.07146 0.04107 0.01734 82 640:  42%|████▏     | 8/19 [00:02<00:03,  3.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07094 0.0409 0.01677 75 640:  42%|████▏     | 8/19 [00:02<00:03,  3.49it/s] 63/99 7.3G 0.07094 0.0409 0.01677 75 640:  47%|████▋     | 9/19 [00:02<00:02,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07058 0.0395 0.01691 40 640:  47%|████▋     | 9/19 [00:02<00:02,  3.97it/s]63/99 7.3G 0.07058 0.0395 0.01691 40 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07061 0.03827 0.01692 43 640:  53%|█████▎    | 10/19 [00:08<00:02,  4.40it/s]63/99 7.3G 0.07061 0.03827 0.01692 43 640:  58%|█████▊    | 11/19 [00:08<00:15,  1.94s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07062 0.03804 0.01726 63 640:  58%|█████▊    | 11/19 [00:15<00:15,  1.94s/it]63/99 7.3G 0.07062 0.03804 0.01726 63 640:  63%|██████▎   | 12/19 [00:15<00:22,  3.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.0706 0.03791 0.01722 62 640:  63%|██████▎   | 12/19 [00:16<00:22,  3.25s/it] 63/99 7.3G 0.0706 0.03791 0.01722 62 640:  68%|██████▊   | 13/19 [00:16<00:16,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.0704 0.03855 0.01707 83 640:  68%|██████▊   | 13/19 [00:16<00:16,  2.69s/it]63/99 7.3G 0.0704 0.03855 0.01707 83 640:  74%|███████▎  | 14/19 [00:16<00:09,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07078 0.04109 0.01738 132 640:  74%|███████▎  | 14/19 [00:22<00:09,  1.93s/it]63/99 7.3G 0.07078 0.04109 0.01738 132 640:  79%|███████▉  | 15/19 [00:22<00:12,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07083 0.0419 0.01722 97 640:  79%|███████▉  | 15/19 [00:23<00:12,  3.15s/it]  63/99 7.3G 0.07083 0.0419 0.01722 97 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07075 0.04206 0.01726 74 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.57s/it]63/99 7.3G 0.07075 0.04206 0.01726 74 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07076 0.04175 0.01731 59 640:  89%|████████▉ | 17/19 [00:26<00:04,  2.17s/it]63/99 7.3G 0.07076 0.04175 0.01731 59 640:  95%|█████████▍| 18/19 [00:26<00:01,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.0709 0.04273 0.01743 105 640:  95%|█████████▍| 18/19 [00:29<00:01,  1.85s/it]63/99 7.3G 0.0709 0.04273 0.01743 105 640: 100%|██████████| 19/19 [00:29<00:00,  2.40s/it]63/99 7.3G 0.0709 0.04273 0.01743 105 640: 100%|██████████| 19/19 [00:29<00:00,  1.57s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.74s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.28s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.10s/it]
                   all         55        256      0.283      0.229      0.168     0.0582
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.0651 0.02285 0.01372 36 640:   0%|          | 0/19 [00:00<?, ?it/s]64/99 7.3G 0.0651 0.02285 0.01372 36 640:   5%|▌         | 1/19 [00:00<00:04,  4.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07115 0.03216 0.01552 85 640:   5%|▌         | 1/19 [00:00<00:04,  4.36it/s]64/99 7.3G 0.07115 0.03216 0.01552 85 640:  11%|█         | 2/19 [00:00<00:05,  3.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07308 0.04164 0.01714 116 640:  11%|█         | 2/19 [00:00<00:05,  3.31it/s]64/99 7.3G 0.07308 0.04164 0.01714 116 640:  16%|█▌        | 3/19 [00:00<00:04,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07147 0.04133 0.0177 61 640:  16%|█▌        | 3/19 [00:00<00:04,  3.87it/s]  64/99 7.3G 0.07147 0.04133 0.0177 61 640:  21%|██        | 4/19 [00:00<00:03,  4.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07194 0.0433 0.01844 79 640:  21%|██        | 4/19 [00:01<00:03,  4.30it/s]64/99 7.3G 0.07194 0.0433 0.01844 79 640:  26%|██▋       | 5/19 [00:01<00:02,  4.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07243 0.04769 0.01796 136 640:  26%|██▋       | 5/19 [00:01<00:02,  4.72it/s]64/99 7.3G 0.07243 0.04769 0.01796 136 640:  32%|███▏      | 6/19 [00:01<00:02,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07188 0.04605 0.01802 56 640:  32%|███▏      | 6/19 [00:01<00:02,  5.02it/s] 64/99 7.3G 0.07188 0.04605 0.01802 56 640:  37%|███▋      | 7/19 [00:01<00:02,  5.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07217 0.04773 0.01835 114 640:  37%|███▋      | 7/19 [00:01<00:02,  5.03it/s]64/99 7.3G 0.07217 0.04773 0.01835 114 640:  42%|████▏     | 8/19 [00:01<00:02,  4.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07197 0.04609 0.01799 58 640:  42%|████▏     | 8/19 [00:05<00:02,  4.02it/s] 64/99 7.3G 0.07197 0.04609 0.01799 58 640:  47%|████▋     | 9/19 [00:05<00:12,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07159 0.04494 0.01787 59 640:  47%|████▋     | 9/19 [00:06<00:12,  1.29s/it]64/99 7.3G 0.07159 0.04494 0.01787 59 640:  53%|█████▎    | 10/19 [00:06<00:11,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07195 0.04711 0.01762 134 640:  53%|█████▎    | 10/19 [00:06<00:11,  1.25s/it]64/99 7.3G 0.07195 0.04711 0.01762 134 640:  58%|█████▊    | 11/19 [00:06<00:07,  1.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07195 0.04682 0.01785 67 640:  58%|█████▊    | 11/19 [00:16<00:07,  1.09it/s] 64/99 7.3G 0.07195 0.04682 0.01785 67 640:  63%|██████▎   | 12/19 [00:16<00:24,  3.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07267 0.0474 0.01785 136 640:  63%|██████▎   | 12/19 [00:17<00:24,  3.54s/it]64/99 7.3G 0.07267 0.0474 0.01785 136 640:  68%|██████▊   | 13/19 [00:17<00:16,  2.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07196 0.04596 0.01766 47 640:  68%|██████▊   | 13/19 [00:17<00:16,  2.70s/it]64/99 7.3G 0.07196 0.04596 0.01766 47 640:  74%|███████▎  | 14/19 [00:17<00:10,  2.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07164 0.04533 0.0175 59 640:  74%|███████▎  | 14/19 [00:18<00:10,  2.13s/it] 64/99 7.3G 0.07164 0.04533 0.0175 59 640:  79%|███████▉  | 15/19 [00:18<00:07,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07128 0.0445 0.0177 53 640:  79%|███████▉  | 15/19 [00:23<00:07,  1.76s/it] 64/99 7.3G 0.07128 0.0445 0.0177 53 640:  84%|████████▍ | 16/19 [00:23<00:08,  2.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07116 0.04495 0.01796 89 640:  84%|████████▍ | 16/19 [00:28<00:08,  2.77s/it]64/99 7.3G 0.07116 0.04495 0.01796 89 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07022 0.04367 0.01749 33 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.38s/it]64/99 7.3G 0.07022 0.04367 0.01749 33 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07078 0.04442 0.01749 107 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.43s/it]64/99 7.3G 0.07078 0.04442 0.01749 107 640: 100%|██████████| 19/19 [00:29<00:00,  1.78s/it]64/99 7.3G 0.07078 0.04442 0.01749 107 640: 100%|██████████| 19/19 [00:29<00:00,  1.54s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.73s/it]
                   all         55        256      0.249      0.231       0.17     0.0597
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.0717 0.03073 0.01598 58 640:   0%|          | 0/19 [00:00<?, ?it/s]65/99 7.3G 0.0717 0.03073 0.01598 58 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07326 0.04389 0.01569 121 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s]65/99 7.3G 0.07326 0.04389 0.01569 121 640:  11%|█         | 2/19 [00:00<00:03,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07515 0.04128 0.01728 65 640:  11%|█         | 2/19 [00:00<00:03,  5.61it/s] 65/99 7.3G 0.07515 0.04128 0.01728 65 640:  16%|█▌        | 3/19 [00:00<00:02,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07425 0.04091 0.01719 67 640:  16%|█▌        | 3/19 [00:00<00:02,  5.66it/s]65/99 7.3G 0.07425 0.04091 0.01719 67 640:  21%|██        | 4/19 [00:00<00:02,  5.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07278 0.03946 0.01948 52 640:  21%|██        | 4/19 [00:01<00:02,  5.39it/s]65/99 7.3G 0.07278 0.03946 0.01948 52 640:  26%|██▋       | 5/19 [00:01<00:03,  4.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07256 0.03956 0.01978 65 640:  26%|██▋       | 5/19 [00:01<00:03,  4.06it/s]65/99 7.3G 0.07256 0.03956 0.01978 65 640:  32%|███▏      | 6/19 [00:01<00:03,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07297 0.03919 0.01955 67 640:  32%|███▏      | 6/19 [00:01<00:03,  3.66it/s]65/99 7.3G 0.07297 0.03919 0.01955 67 640:  37%|███▋      | 7/19 [00:01<00:02,  4.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07364 0.04037 0.01992 93 640:  37%|███▋      | 7/19 [00:01<00:02,  4.16it/s]65/99 7.3G 0.07364 0.04037 0.01992 93 640:  42%|████▏     | 8/19 [00:01<00:02,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07334 0.04028 0.01904 75 640:  42%|████▏     | 8/19 [00:06<00:02,  4.56it/s]65/99 7.3G 0.07334 0.04028 0.01904 75 640:  47%|████▋     | 9/19 [00:06<00:15,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07232 0.03972 0.01878 53 640:  47%|████▋     | 9/19 [00:06<00:15,  1.58s/it]65/99 7.3G 0.07232 0.03972 0.01878 53 640:  53%|█████▎    | 10/19 [00:06<00:11,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07215 0.03941 0.01863 64 640:  53%|█████▎    | 10/19 [00:07<00:11,  1.25s/it]65/99 7.3G 0.07215 0.03941 0.01863 64 640:  58%|█████▊    | 11/19 [00:07<00:07,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07223 0.03838 0.01865 42 640:  58%|█████▊    | 11/19 [00:07<00:07,  1.02it/s]65/99 7.3G 0.07223 0.03838 0.01865 42 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07222 0.03867 0.0185 68 640:  63%|██████▎   | 12/19 [00:17<00:05,  1.26it/s] 65/99 7.3G 0.07222 0.03867 0.0185 68 640:  68%|██████▊   | 13/19 [00:17<00:20,  3.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07186 0.03826 0.01812 54 640:  68%|██████▊   | 13/19 [00:23<00:20,  3.43s/it]65/99 7.3G 0.07186 0.03826 0.01812 54 640:  74%|███████▎  | 14/19 [00:23<00:21,  4.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07197 0.03985 0.01831 103 640:  74%|███████▎  | 14/19 [00:23<00:21,  4.21s/it]65/99 7.3G 0.07197 0.03985 0.01831 103 640:  79%|███████▉  | 15/19 [00:23<00:12,  3.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07189 0.03958 0.01823 59 640:  79%|███████▉  | 15/19 [00:23<00:12,  3.05s/it] 65/99 7.3G 0.07189 0.03958 0.01823 59 640:  84%|████████▍ | 16/19 [00:23<00:06,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07208 0.04032 0.01823 95 640:  84%|████████▍ | 16/19 [00:28<00:06,  2.29s/it]65/99 7.3G 0.07208 0.04032 0.01823 95 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.072 0.04075 0.01831 76 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.11s/it]  65/99 7.3G 0.072 0.04075 0.01831 76 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07184 0.0403 0.01841 60 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.40s/it]65/99 7.3G 0.07184 0.0403 0.01841 60 640: 100%|██████████| 19/19 [00:30<00:00,  1.94s/it]65/99 7.3G 0.07184 0.0403 0.01841 60 640: 100%|██████████| 19/19 [00:30<00:00,  1.61s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.80s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]
                   all         55        256      0.246      0.206      0.157     0.0581
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06844 0.03307 0.01863 55 640:   0%|          | 0/19 [00:00<?, ?it/s]66/99 7.3G 0.06844 0.03307 0.01863 55 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06581 0.03407 0.01701 50 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]66/99 7.3G 0.06581 0.03407 0.01701 50 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06553 0.03296 0.01741 50 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]66/99 7.3G 0.06553 0.03296 0.01741 50 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06857 0.03548 0.01782 75 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]66/99 7.3G 0.06857 0.03548 0.01782 75 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06892 0.03873 0.01859 83 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]66/99 7.3G 0.06892 0.03873 0.01859 83 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.069 0.03724 0.01841 48 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s]  66/99 7.3G 0.069 0.03724 0.01841 48 640:  32%|███▏      | 6/19 [00:01<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06974 0.03671 0.01787 62 640:  32%|███▏      | 6/19 [00:01<00:02,  5.67it/s]66/99 7.3G 0.06974 0.03671 0.01787 62 640:  37%|███▋      | 7/19 [00:01<00:02,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06992 0.03794 0.01726 87 640:  37%|███▋      | 7/19 [00:01<00:02,  5.64it/s]66/99 7.3G 0.06992 0.03794 0.01726 87 640:  42%|████▏     | 8/19 [00:01<00:02,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06983 0.03802 0.01678 73 640:  42%|████▏     | 8/19 [00:02<00:02,  4.22it/s]66/99 7.3G 0.06983 0.03802 0.01678 73 640:  47%|████▋     | 9/19 [00:02<00:03,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.06951 0.03741 0.01709 51 640:  47%|████▋     | 9/19 [00:08<00:03,  3.04it/s]66/99 7.3G 0.06951 0.03741 0.01709 51 640:  53%|█████▎    | 10/19 [00:08<00:18,  2.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07032 0.03897 0.01716 104 640:  53%|█████▎    | 10/19 [00:15<00:18,  2.08s/it]66/99 7.3G 0.07032 0.03897 0.01716 104 640:  58%|█████▊    | 11/19 [00:15<00:30,  3.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.0704 0.03916 0.01709 73 640:  58%|█████▊    | 11/19 [00:15<00:30,  3.77s/it]  66/99 7.3G 0.0704 0.03916 0.01709 73 640:  63%|██████▎   | 12/19 [00:15<00:18,  2.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07035 0.03933 0.01686 73 640:  63%|██████▎   | 12/19 [00:16<00:18,  2.68s/it]66/99 7.3G 0.07035 0.03933 0.01686 73 640:  68%|██████▊   | 13/19 [00:16<00:11,  1.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07081 0.04048 0.01693 107 640:  68%|██████▊   | 13/19 [00:20<00:11,  1.92s/it]66/99 7.3G 0.07081 0.04048 0.01693 107 640:  74%|███████▎  | 14/19 [00:20<00:14,  2.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07088 0.04205 0.01709 102 640:  74%|███████▎  | 14/19 [00:22<00:14,  2.82s/it]66/99 7.3G 0.07088 0.04205 0.01709 102 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.0715 0.04179 0.0174 79 640:  79%|███████▉  | 15/19 [00:23<00:09,  2.35s/it]   66/99 7.3G 0.0715 0.04179 0.0174 79 640:  84%|████████▍ | 16/19 [00:23<00:05,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.0714 0.04189 0.01736 70 640:  84%|████████▍ | 16/19 [00:24<00:05,  1.96s/it]66/99 7.3G 0.0714 0.04189 0.01736 70 640:  89%|████████▉ | 17/19 [00:24<00:03,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07152 0.04132 0.01738 55 640:  89%|████████▉ | 17/19 [00:28<00:03,  1.72s/it]66/99 7.3G 0.07152 0.04132 0.01738 55 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07142 0.04154 0.01735 81 640:  95%|█████████▍| 18/19 [00:35<00:02,  2.53s/it]66/99 7.3G 0.07142 0.04154 0.01735 81 640: 100%|██████████| 19/19 [00:35<00:00,  3.67s/it]66/99 7.3G 0.07142 0.04154 0.01735 81 640: 100%|██████████| 19/19 [00:35<00:00,  1.85s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.57s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.35s/it]
                   all         55        256      0.253      0.206      0.165     0.0601
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.06826 0.03156 0.01636 50 640:   0%|          | 0/19 [00:00<?, ?it/s]67/99 7.3G 0.06826 0.03156 0.01636 50 640:   5%|▌         | 1/19 [00:00<00:04,  3.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07241 0.04638 0.01895 121 640:   5%|▌         | 1/19 [00:00<00:04,  3.81it/s]67/99 7.3G 0.07241 0.04638 0.01895 121 640:  11%|█         | 2/19 [00:00<00:05,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07228 0.04165 0.01792 59 640:  11%|█         | 2/19 [00:00<00:05,  3.39it/s] 67/99 7.3G 0.07228 0.04165 0.01792 59 640:  16%|█▌        | 3/19 [00:00<00:04,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.0741 0.04302 0.0183 100 640:  16%|█▌        | 3/19 [00:00<00:04,  3.92it/s] 67/99 7.3G 0.0741 0.04302 0.0183 100 640:  21%|██        | 4/19 [00:00<00:03,  4.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07448 0.04183 0.01865 66 640:  21%|██        | 4/19 [00:01<00:03,  4.23it/s]67/99 7.3G 0.07448 0.04183 0.01865 66 640:  26%|██▋       | 5/19 [00:01<00:03,  4.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07364 0.04039 0.0177 58 640:  26%|██▋       | 5/19 [00:01<00:03,  4.43it/s] 67/99 7.3G 0.07364 0.04039 0.0177 58 640:  32%|███▏      | 6/19 [00:01<00:02,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07229 0.03911 0.01759 51 640:  32%|███▏      | 6/19 [00:01<00:02,  4.56it/s]67/99 7.3G 0.07229 0.03911 0.01759 51 640:  37%|███▋      | 7/19 [00:01<00:02,  4.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07108 0.03839 0.01788 54 640:  37%|███▋      | 7/19 [00:01<00:02,  4.43it/s]67/99 7.3G 0.07108 0.03839 0.01788 54 640:  42%|████▏     | 8/19 [00:01<00:02,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07181 0.03885 0.01759 86 640:  42%|████▏     | 8/19 [00:02<00:02,  4.34it/s]67/99 7.3G 0.07181 0.03885 0.01759 86 640:  47%|████▋     | 9/19 [00:02<00:02,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07119 0.03846 0.0172 59 640:  47%|████▋     | 9/19 [00:02<00:02,  4.37it/s] 67/99 7.3G 0.07119 0.03846 0.0172 59 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07123 0.03918 0.01693 76 640:  53%|█████▎    | 10/19 [00:07<00:02,  3.93it/s]67/99 7.3G 0.07123 0.03918 0.01693 76 640:  58%|█████▊    | 11/19 [00:07<00:13,  1.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07144 0.03921 0.0169 83 640:  58%|█████▊    | 11/19 [00:08<00:13,  1.73s/it] 67/99 7.3G 0.07144 0.03921 0.0169 83 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07129 0.03903 0.01679 63 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.38s/it]67/99 7.3G 0.07129 0.03903 0.01679 63 640:  68%|██████▊   | 13/19 [00:08<00:06,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07179 0.03921 0.01709 73 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.03s/it]67/99 7.3G 0.07179 0.03921 0.01709 73 640:  74%|███████▎  | 14/19 [00:10<00:07,  1.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07153 0.0392 0.017 64 640:  74%|███████▎  | 14/19 [00:17<00:07,  1.41s/it]   67/99 7.3G 0.07153 0.0392 0.017 64 640:  79%|███████▉  | 15/19 [00:17<00:12,  3.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07201 0.0404 0.01728 109 640:  79%|███████▉  | 15/19 [00:23<00:12,  3.09s/it]67/99 7.3G 0.07201 0.0404 0.01728 109 640:  84%|████████▍ | 16/19 [00:23<00:11,  3.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.0717 0.0398 0.01725 50 640:  84%|████████▍ | 16/19 [00:23<00:11,  3.90s/it]  67/99 7.3G 0.0717 0.0398 0.01725 50 640:  89%|████████▉ | 17/19 [00:23<00:05,  2.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.0718 0.03893 0.01739 40 640:  89%|████████▉ | 17/19 [00:23<00:05,  2.78s/it]67/99 7.3G 0.0718 0.03893 0.01739 40 640:  95%|█████████▍| 18/19 [00:23<00:01,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07206 0.03926 0.01734 88 640:  95%|█████████▍| 18/19 [00:29<00:01,  2.00s/it]67/99 7.3G 0.07206 0.03926 0.01734 88 640: 100%|██████████| 19/19 [00:29<00:00,  3.16s/it]67/99 7.3G 0.07206 0.03926 0.01734 88 640: 100%|██████████| 19/19 [00:29<00:00,  1.56s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.01s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.19s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]
                   all         55        256       0.24      0.204       0.16     0.0571
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07482 0.05437 0.02103 97 640:   0%|          | 0/19 [00:00<?, ?it/s]68/99 7.3G 0.07482 0.05437 0.02103 97 640:   5%|▌         | 1/19 [00:00<00:10,  1.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07545 0.04469 0.02184 51 640:   5%|▌         | 1/19 [00:01<00:10,  1.67it/s]68/99 7.3G 0.07545 0.04469 0.02184 51 640:  11%|█         | 2/19 [00:01<00:11,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07402 0.04023 0.02036 52 640:  11%|█         | 2/19 [00:02<00:11,  1.50it/s]68/99 7.3G 0.07402 0.04023 0.02036 52 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07305 0.04114 0.0194 74 640:  16%|█▌        | 3/19 [00:02<00:11,  1.45it/s] 68/99 7.3G 0.07305 0.04114 0.0194 74 640:  21%|██        | 4/19 [00:02<00:11,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07266 0.04127 0.0197 64 640:  21%|██        | 4/19 [00:03<00:11,  1.34it/s]68/99 7.3G 0.07266 0.04127 0.0197 64 640:  26%|██▋       | 5/19 [00:03<00:10,  1.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07266 0.04081 0.01894 77 640:  26%|██▋       | 5/19 [00:04<00:10,  1.35it/s]68/99 7.3G 0.07266 0.04081 0.01894 77 640:  32%|███▏      | 6/19 [00:04<00:08,  1.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07219 0.03929 0.01853 54 640:  32%|███▏      | 6/19 [00:04<00:08,  1.48it/s]68/99 7.3G 0.07219 0.03929 0.01853 54 640:  37%|███▋      | 7/19 [00:04<00:07,  1.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07231 0.04033 0.01805 83 640:  37%|███▋      | 7/19 [00:05<00:07,  1.58it/s]68/99 7.3G 0.07231 0.04033 0.01805 83 640:  42%|████▏     | 8/19 [00:05<00:06,  1.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07203 0.04026 0.01826 63 640:  42%|████▏     | 8/19 [00:05<00:06,  1.72it/s]68/99 7.3G 0.07203 0.04026 0.01826 63 640:  47%|████▋     | 9/19 [00:05<00:04,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07229 0.03974 0.01801 62 640:  47%|████▋     | 9/19 [00:05<00:04,  2.11it/s]68/99 7.3G 0.07229 0.03974 0.01801 62 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07281 0.04177 0.01774 117 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.55it/s]68/99 7.3G 0.07281 0.04177 0.01774 117 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07262 0.04381 0.01781 109 640:  58%|█████▊    | 11/19 [00:10<00:03,  2.63it/s]68/99 7.3G 0.07262 0.04381 0.01781 109 640:  63%|██████▎   | 12/19 [00:10<00:11,  1.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.0729 0.04704 0.01774 151 640:  63%|██████▎   | 12/19 [00:17<00:11,  1.69s/it] 68/99 7.3G 0.0729 0.04704 0.01774 151 640:  68%|██████▊   | 13/19 [00:17<00:19,  3.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07351 0.0464 0.01754 95 640:  68%|██████▊   | 13/19 [00:17<00:19,  3.32s/it] 68/99 7.3G 0.07351 0.0464 0.01754 95 640:  74%|███████▎  | 14/19 [00:17<00:11,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07323 0.04605 0.0176 77 640:  74%|███████▎  | 14/19 [00:18<00:11,  2.37s/it]68/99 7.3G 0.07323 0.04605 0.0176 77 640:  79%|███████▉  | 15/19 [00:18<00:06,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07345 0.04687 0.01758 129 640:  79%|███████▉  | 15/19 [00:23<00:06,  1.71s/it]68/99 7.3G 0.07345 0.04687 0.01758 129 640:  84%|████████▍ | 16/19 [00:23<00:08,  2.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07324 0.04699 0.01752 86 640:  84%|████████▍ | 16/19 [00:24<00:08,  2.75s/it] 68/99 7.3G 0.07324 0.04699 0.01752 86 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07314 0.0478 0.0173 111 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.38s/it] 68/99 7.3G 0.07314 0.0478 0.0173 111 640:  95%|█████████▍| 18/19 [00:25<00:01,  1.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07309 0.04747 0.01724 66 640:  95%|█████████▍| 18/19 [00:26<00:01,  1.92s/it]68/99 7.3G 0.07309 0.04747 0.01724 66 640: 100%|██████████| 19/19 [00:26<00:00,  1.57s/it]68/99 7.3G 0.07309 0.04747 0.01724 66 640: 100%|██████████| 19/19 [00:26<00:00,  1.39s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.46s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.97s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.79s/it]
                   all         55        256      0.235       0.22      0.153     0.0551
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07819 0.04797 0.01577 111 640:   0%|          | 0/19 [00:00<?, ?it/s]69/99 7.3G 0.07819 0.04797 0.01577 111 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07307 0.04125 0.01586 57 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s] 69/99 7.3G 0.07307 0.04125 0.01586 57 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.06937 0.04037 0.0178 61 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s] 69/99 7.3G 0.06937 0.04037 0.0178 61 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07063 0.03925 0.01695 70 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]69/99 7.3G 0.07063 0.03925 0.01695 70 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07138 0.04173 0.01677 93 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]69/99 7.3G 0.07138 0.04173 0.01677 93 640:  26%|██▋       | 5/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.0715 0.04489 0.01732 98 640:  26%|██▋       | 5/19 [00:01<00:02,  5.71it/s] 69/99 7.3G 0.0715 0.04489 0.01732 98 640:  32%|███▏      | 6/19 [00:01<00:03,  4.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07153 0.04488 0.018 72 640:  32%|███▏      | 6/19 [00:01<00:03,  4.26it/s] 69/99 7.3G 0.07153 0.04488 0.018 72 640:  37%|███▋      | 7/19 [00:01<00:03,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07081 0.04511 0.01785 80 640:  37%|███▋      | 7/19 [00:01<00:03,  3.66it/s]69/99 7.3G 0.07081 0.04511 0.01785 80 640:  42%|████▏     | 8/19 [00:01<00:03,  3.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07187 0.04616 0.01756 125 640:  42%|████▏     | 8/19 [00:06<00:03,  3.33it/s]69/99 7.3G 0.07187 0.04616 0.01756 125 640:  47%|████▋     | 9/19 [00:06<00:17,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07068 0.045 0.01765 57 640:  47%|████▋     | 9/19 [00:16<00:17,  1.71s/it]   69/99 7.3G 0.07068 0.045 0.01765 57 640:  53%|█████▎    | 10/19 [00:16<00:37,  4.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07125 0.04457 0.01742 81 640:  53%|█████▎    | 10/19 [00:16<00:37,  4.17s/it]69/99 7.3G 0.07125 0.04457 0.01742 81 640:  58%|█████▊    | 11/19 [00:16<00:23,  2.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07115 0.04479 0.01723 79 640:  58%|█████▊    | 11/19 [00:16<00:23,  2.95s/it]69/99 7.3G 0.07115 0.04479 0.01723 79 640:  63%|██████▎   | 12/19 [00:16<00:14,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07208 0.04867 0.01733 205 640:  63%|██████▎   | 12/19 [00:23<00:14,  2.12s/it]69/99 7.3G 0.07208 0.04867 0.01733 205 640:  68%|██████▊   | 13/19 [00:23<00:21,  3.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07228 0.04848 0.01729 83 640:  68%|██████▊   | 13/19 [00:24<00:21,  3.64s/it] 69/99 7.3G 0.07228 0.04848 0.01729 83 640:  74%|███████▎  | 14/19 [00:24<00:13,  2.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07213 0.04811 0.01717 72 640:  74%|███████▎  | 14/19 [00:25<00:13,  2.77s/it]69/99 7.3G 0.07213 0.04811 0.01717 72 640:  79%|███████▉  | 15/19 [00:25<00:08,  2.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07211 0.04759 0.01739 64 640:  79%|███████▉  | 15/19 [00:26<00:08,  2.16s/it]69/99 7.3G 0.07211 0.04759 0.01739 64 640:  84%|████████▍ | 16/19 [00:26<00:05,  1.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07237 0.04719 0.01763 77 640:  84%|████████▍ | 16/19 [00:32<00:05,  1.77s/it]69/99 7.3G 0.07237 0.04719 0.01763 77 640:  89%|████████▉ | 17/19 [00:32<00:06,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07232 0.04754 0.01743 93 640:  89%|████████▉ | 17/19 [00:36<00:06,  3.07s/it]69/99 7.3G 0.07232 0.04754 0.01743 93 640:  95%|█████████▍| 18/19 [00:36<00:03,  3.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07229 0.04766 0.01731 82 640:  95%|█████████▍| 18/19 [00:36<00:03,  3.42s/it]69/99 7.3G 0.07229 0.04766 0.01731 82 640: 100%|██████████| 19/19 [00:36<00:00,  2.46s/it]69/99 7.3G 0.07229 0.04766 0.01731 82 640: 100%|██████████| 19/19 [00:36<00:00,  1.94s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  5.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
                   all         55        256      0.255      0.247      0.168     0.0563
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07525 0.04474 0.01487 94 640:   0%|          | 0/19 [00:00<?, ?it/s]70/99 7.3G 0.07525 0.04474 0.01487 94 640:   5%|▌         | 1/19 [00:00<00:05,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07115 0.04566 0.01658 75 640:   5%|▌         | 1/19 [00:00<00:05,  3.09it/s]70/99 7.3G 0.07115 0.04566 0.01658 75 640:  11%|█         | 2/19 [00:00<00:06,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.06909 0.04118 0.01525 58 640:  11%|█         | 2/19 [00:01<00:06,  2.64it/s]70/99 7.3G 0.06909 0.04118 0.01525 58 640:  16%|█▌        | 3/19 [00:01<00:08,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07 0.04718 0.01532 119 640:  16%|█▌        | 3/19 [00:01<00:08,  1.97it/s]  70/99 7.3G 0.07 0.04718 0.01532 119 640:  21%|██        | 4/19 [00:01<00:07,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07013 0.04777 0.01603 88 640:  21%|██        | 4/19 [00:02<00:07,  1.92it/s]70/99 7.3G 0.07013 0.04777 0.01603 88 640:  26%|██▋       | 5/19 [00:02<00:06,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07017 0.04906 0.01643 97 640:  26%|██▋       | 5/19 [00:02<00:06,  2.06it/s]70/99 7.3G 0.07017 0.04906 0.01643 97 640:  32%|███▏      | 6/19 [00:02<00:05,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07016 0.04749 0.01679 67 640:  32%|███▏      | 6/19 [00:03<00:05,  2.20it/s]70/99 7.3G 0.07016 0.04749 0.01679 67 640:  37%|███▋      | 7/19 [00:03<00:05,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07126 0.04839 0.01709 128 640:  37%|███▋      | 7/19 [00:03<00:05,  2.28it/s]70/99 7.3G 0.07126 0.04839 0.01709 128 640:  42%|████▏     | 8/19 [00:03<00:04,  2.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07206 0.04703 0.01677 75 640:  42%|████▏     | 8/19 [00:04<00:04,  2.22it/s] 70/99 7.3G 0.07206 0.04703 0.01677 75 640:  47%|████▋     | 9/19 [00:04<00:04,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07251 0.04652 0.01703 68 640:  47%|████▋     | 9/19 [00:07<00:04,  2.04it/s]70/99 7.3G 0.07251 0.04652 0.01703 68 640:  53%|█████▎    | 10/19 [00:07<00:13,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.0725 0.04658 0.01661 83 640:  53%|█████▎    | 10/19 [00:08<00:13,  1.49s/it] 70/99 7.3G 0.0725 0.04658 0.01661 83 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07239 0.04523 0.01658 49 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.19s/it]70/99 7.3G 0.07239 0.04523 0.01658 49 640:  63%|██████▎   | 12/19 [00:08<00:06,  1.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07252 0.04621 0.01678 114 640:  63%|██████▎   | 12/19 [00:11<00:06,  1.11it/s]70/99 7.3G 0.07252 0.04621 0.01678 114 640:  68%|██████▊   | 13/19 [00:11<00:08,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.0722 0.04566 0.01695 66 640:  68%|██████▊   | 13/19 [00:18<00:08,  1.38s/it]  70/99 7.3G 0.0722 0.04566 0.01695 66 640:  74%|███████▎  | 14/19 [00:18<00:16,  3.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07204 0.04548 0.01687 70 640:  74%|███████▎  | 14/19 [00:23<00:16,  3.27s/it]70/99 7.3G 0.07204 0.04548 0.01687 70 640:  79%|███████▉  | 15/19 [00:23<00:14,  3.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07183 0.04467 0.01679 52 640:  79%|███████▉  | 15/19 [00:23<00:14,  3.63s/it]70/99 7.3G 0.07183 0.04467 0.01679 52 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.0721 0.04514 0.01704 94 640:  84%|████████▍ | 16/19 [00:23<00:07,  2.59s/it] 70/99 7.3G 0.0721 0.04514 0.01704 94 640:  89%|████████▉ | 17/19 [00:23<00:03,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07215 0.04524 0.01686 89 640:  89%|████████▉ | 17/19 [00:29<00:03,  1.86s/it]70/99 7.3G 0.07215 0.04524 0.01686 89 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07148 0.04439 0.01679 46 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.07s/it]70/99 7.3G 0.07148 0.04439 0.01679 46 640: 100%|██████████| 19/19 [00:29<00:00,  2.20s/it]70/99 7.3G 0.07148 0.04439 0.01679 46 640: 100%|██████████| 19/19 [00:29<00:00,  1.56s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.66s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]
                   all         55        256      0.185      0.264      0.146     0.0514
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07704 0.04685 0.01791 101 640:   0%|          | 0/19 [00:00<?, ?it/s]71/99 7.3G 0.07704 0.04685 0.01791 101 640:   5%|▌         | 1/19 [00:00<00:07,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07193 0.04039 0.01655 61 640:   5%|▌         | 1/19 [00:00<00:07,  2.44it/s] 71/99 7.3G 0.07193 0.04039 0.01655 61 640:  11%|█         | 2/19 [00:00<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07167 0.04161 0.01573 86 640:  11%|█         | 2/19 [00:01<00:06,  2.50it/s]71/99 7.3G 0.07167 0.04161 0.01573 86 640:  16%|█▌        | 3/19 [00:01<00:05,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.06972 0.0388 0.01581 50 640:  16%|█▌        | 3/19 [00:01<00:05,  3.11it/s] 71/99 7.3G 0.06972 0.0388 0.01581 50 640:  21%|██        | 4/19 [00:01<00:04,  3.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.06984 0.03593 0.01619 42 640:  21%|██        | 4/19 [00:01<00:04,  3.53it/s]71/99 7.3G 0.06984 0.03593 0.01619 42 640:  26%|██▋       | 5/19 [00:01<00:03,  3.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07152 0.04161 0.01629 163 640:  26%|██▋       | 5/19 [00:02<00:03,  3.71it/s]71/99 7.3G 0.07152 0.04161 0.01629 163 640:  32%|███▏      | 6/19 [00:02<00:04,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07168 0.04182 0.0167 67 640:  32%|███▏      | 6/19 [00:02<00:04,  2.87it/s]  71/99 7.3G 0.07168 0.04182 0.0167 67 640:  37%|███▋      | 7/19 [00:02<00:05,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07112 0.04252 0.01665 86 640:  37%|███▋      | 7/19 [00:03<00:05,  2.16it/s]71/99 7.3G 0.07112 0.04252 0.01665 86 640:  42%|████▏     | 8/19 [00:03<00:05,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.06979 0.04069 0.01674 38 640:  42%|████▏     | 8/19 [00:03<00:05,  2.05it/s]71/99 7.3G 0.06979 0.04069 0.01674 38 640:  47%|████▋     | 9/19 [00:03<00:04,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.06981 0.04075 0.01655 76 640:  47%|████▋     | 9/19 [00:05<00:04,  2.12it/s]71/99 7.3G 0.06981 0.04075 0.01655 76 640:  53%|█████▎    | 10/19 [00:05<00:06,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.06981 0.04072 0.0169 71 640:  53%|█████▎    | 10/19 [00:10<00:06,  1.33it/s] 71/99 7.3G 0.06981 0.04072 0.0169 71 640:  58%|█████▊    | 11/19 [00:10<00:17,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07024 0.04337 0.01719 128 640:  58%|█████▊    | 11/19 [00:17<00:17,  2.24s/it]71/99 7.3G 0.07024 0.04337 0.01719 128 640:  63%|██████▎   | 12/19 [00:17<00:26,  3.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07067 0.04353 0.0169 92 640:  63%|██████▎   | 12/19 [00:18<00:26,  3.77s/it]  71/99 7.3G 0.07067 0.04353 0.0169 92 640:  68%|██████▊   | 13/19 [00:18<00:16,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07076 0.04369 0.01702 78 640:  68%|██████▊   | 13/19 [00:18<00:16,  2.69s/it]71/99 7.3G 0.07076 0.04369 0.01702 78 640:  74%|███████▎  | 14/19 [00:18<00:09,  1.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07073 0.04351 0.01691 64 640:  74%|███████▎  | 14/19 [00:24<00:09,  1.95s/it]71/99 7.3G 0.07073 0.04351 0.01691 64 640:  79%|███████▉  | 15/19 [00:24<00:12,  3.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07039 0.04306 0.01694 56 640:  79%|███████▉  | 15/19 [00:24<00:12,  3.24s/it]71/99 7.3G 0.07039 0.04306 0.01694 56 640:  84%|████████▍ | 16/19 [00:24<00:06,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07056 0.04336 0.01709 82 640:  84%|████████▍ | 16/19 [00:25<00:06,  2.33s/it]71/99 7.3G 0.07056 0.04336 0.01709 82 640:  89%|████████▉ | 17/19 [00:25<00:03,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07055 0.04351 0.01706 82 640:  89%|████████▉ | 17/19 [00:29<00:03,  1.68s/it]71/99 7.3G 0.07055 0.04351 0.01706 82 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07056 0.04388 0.01706 91 640:  95%|█████████▍| 18/19 [00:33<00:02,  2.61s/it]71/99 7.3G 0.07056 0.04388 0.01706 91 640: 100%|██████████| 19/19 [00:33<00:00,  3.02s/it]71/99 7.3G 0.07056 0.04388 0.01706 91 640: 100%|██████████| 19/19 [00:33<00:00,  1.78s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.58s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.60s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]
                   all         55        256      0.222      0.274      0.171     0.0592
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07505 0.05979 0.01849 103 640:   0%|          | 0/19 [00:00<?, ?it/s]72/99 7.3G 0.07505 0.05979 0.01849 103 640:   5%|▌         | 1/19 [00:00<00:03,  5.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07265 0.0517 0.01821 72 640:   5%|▌         | 1/19 [00:00<00:03,  5.36it/s]  72/99 7.3G 0.07265 0.0517 0.01821 72 640:  11%|█         | 2/19 [00:00<00:03,  5.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07133 0.05104 0.01693 90 640:  11%|█         | 2/19 [00:00<00:03,  5.24it/s]72/99 7.3G 0.07133 0.05104 0.01693 90 640:  16%|█▌        | 3/19 [00:00<00:03,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07217 0.04943 0.01692 87 640:  16%|█▌        | 3/19 [00:00<00:03,  5.02it/s]72/99 7.3G 0.07217 0.04943 0.01692 87 640:  21%|██        | 4/19 [00:00<00:03,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07129 0.04745 0.01688 71 640:  21%|██        | 4/19 [00:01<00:03,  4.88it/s]72/99 7.3G 0.07129 0.04745 0.01688 71 640:  26%|██▋       | 5/19 [00:01<00:02,  4.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07143 0.04808 0.01708 86 640:  26%|██▋       | 5/19 [00:01<00:02,  4.83it/s]72/99 7.3G 0.07143 0.04808 0.01708 86 640:  32%|███▏      | 6/19 [00:01<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07107 0.04801 0.018 83 640:  32%|███▏      | 6/19 [00:01<00:02,  4.74it/s]  72/99 7.3G 0.07107 0.04801 0.018 83 640:  37%|███▋      | 7/19 [00:01<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07056 0.0455 0.01794 43 640:  37%|███▋      | 7/19 [00:01<00:02,  4.74it/s]72/99 7.3G 0.07056 0.0455 0.01794 43 640:  42%|████▏     | 8/19 [00:01<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.06981 0.04392 0.01776 51 640:  42%|████▏     | 8/19 [00:07<00:02,  4.74it/s]72/99 7.3G 0.06981 0.04392 0.01776 51 640:  47%|████▋     | 9/19 [00:07<00:18,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07037 0.04412 0.01736 85 640:  47%|████▋     | 9/19 [00:07<00:18,  1.89s/it]72/99 7.3G 0.07037 0.04412 0.01736 85 640:  53%|█████▎    | 10/19 [00:07<00:12,  1.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07161 0.04794 0.01732 218 640:  53%|█████▎    | 10/19 [00:07<00:12,  1.36s/it]72/99 7.3G 0.07161 0.04794 0.01732 218 640:  58%|█████▊    | 11/19 [00:07<00:07,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07089 0.04695 0.01748 53 640:  58%|█████▊    | 11/19 [00:14<00:07,  1.00it/s] 72/99 7.3G 0.07089 0.04695 0.01748 53 640:  63%|██████▎   | 12/19 [00:14<00:20,  2.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.0706 0.04671 0.01723 78 640:  63%|██████▎   | 12/19 [00:15<00:20,  2.86s/it] 72/99 7.3G 0.0706 0.04671 0.01723 78 640:  68%|██████▊   | 13/19 [00:15<00:12,  2.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07038 0.04613 0.01718 64 640:  68%|██████▊   | 13/19 [00:15<00:12,  2.09s/it]72/99 7.3G 0.07038 0.04613 0.01718 64 640:  74%|███████▎  | 14/19 [00:15<00:08,  1.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07014 0.04541 0.01708 64 640:  74%|███████▎  | 14/19 [00:20<00:08,  1.60s/it]72/99 7.3G 0.07014 0.04541 0.01708 64 640:  79%|███████▉  | 15/19 [00:20<00:10,  2.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07022 0.04512 0.01704 74 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.62s/it]72/99 7.3G 0.07022 0.04512 0.01704 74 640:  84%|████████▍ | 16/19 [00:24<00:08,  2.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07005 0.04456 0.01723 53 640:  84%|████████▍ | 16/19 [00:28<00:08,  2.90s/it]72/99 7.3G 0.07005 0.04456 0.01723 53 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07039 0.04532 0.01739 108 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.33s/it]72/99 7.3G 0.07039 0.04532 0.01739 108 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.0707 0.04535 0.01738 81 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.39s/it]  72/99 7.3G 0.0707 0.04535 0.01738 81 640: 100%|██████████| 19/19 [00:29<00:00,  1.91s/it]72/99 7.3G 0.0707 0.04535 0.01738 81 640: 100%|██████████| 19/19 [00:29<00:00,  1.55s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.83s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  6.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.50s/it]
                   all         55        256      0.226      0.283      0.172     0.0578
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06866 0.0317 0.01951 51 640:   0%|          | 0/19 [00:00<?, ?it/s]73/99 7.3G 0.06866 0.0317 0.01951 51 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06989 0.04137 0.0168 97 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]73/99 7.3G 0.06989 0.04137 0.0168 97 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06766 0.03765 0.01614 52 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]73/99 7.3G 0.06766 0.03765 0.01614 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06775 0.03737 0.01653 61 640:  16%|█▌        | 3/19 [00:00<00:02,  5.75it/s]73/99 7.3G 0.06775 0.03737 0.01653 61 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06785 0.03955 0.01649 80 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]73/99 7.3G 0.06785 0.03955 0.01649 80 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06863 0.04201 0.01697 100 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s]73/99 7.3G 0.06863 0.04201 0.01697 100 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06828 0.04124 0.01783 58 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s] 73/99 7.3G 0.06828 0.04124 0.01783 58 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07006 0.04348 0.01784 150 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]73/99 7.3G 0.07006 0.04348 0.01784 150 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06981 0.04457 0.01789 87 640:  42%|████▏     | 8/19 [00:07<00:01,  5.75it/s] 73/99 7.3G 0.06981 0.04457 0.01789 87 640:  47%|████▋     | 9/19 [00:07<00:19,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.06997 0.04536 0.01763 88 640:  47%|████▋     | 9/19 [00:07<00:19,  1.96s/it]73/99 7.3G 0.06997 0.04536 0.01763 88 640:  53%|█████▎    | 10/19 [00:07<00:13,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07104 0.04496 0.01797 91 640:  53%|█████▎    | 10/19 [00:08<00:13,  1.49s/it]73/99 7.3G 0.07104 0.04496 0.01797 91 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07165 0.04515 0.01804 102 640:  58%|█████▊    | 11/19 [00:15<00:09,  1.13s/it]73/99 7.3G 0.07165 0.04515 0.01804 102 640:  63%|██████▎   | 12/19 [00:15<00:20,  2.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07182 0.04605 0.01799 96 640:  63%|██████▎   | 12/19 [00:18<00:20,  2.95s/it] 73/99 7.3G 0.07182 0.04605 0.01799 96 640:  68%|██████▊   | 13/19 [00:18<00:19,  3.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07232 0.04705 0.01797 102 640:  68%|██████▊   | 13/19 [00:23<00:19,  3.22s/it]73/99 7.3G 0.07232 0.04705 0.01797 102 640:  74%|███████▎  | 14/19 [00:23<00:17,  3.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07219 0.0463 0.01785 65 640:  74%|███████▎  | 14/19 [00:23<00:17,  3.59s/it]  73/99 7.3G 0.07219 0.0463 0.01785 65 640:  79%|███████▉  | 15/19 [00:23<00:10,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.0718 0.04579 0.01803 60 640:  79%|███████▉  | 15/19 [00:25<00:10,  2.63s/it]73/99 7.3G 0.0718 0.04579 0.01803 60 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07235 0.04639 0.01794 133 640:  84%|████████▍ | 16/19 [00:30<00:07,  2.46s/it]73/99 7.3G 0.07235 0.04639 0.01794 133 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07242 0.04694 0.01801 98 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.96s/it] 73/99 7.3G 0.07242 0.04694 0.01801 98 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07268 0.04731 0.01778 103 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.12s/it]73/99 7.3G 0.07268 0.04731 0.01778 103 640: 100%|██████████| 19/19 [00:30<00:00,  1.54s/it]73/99 7.3G 0.07268 0.04731 0.01778 103 640: 100%|██████████| 19/19 [00:30<00:00,  1.60s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  5.79s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
                   all         55        256      0.236      0.297      0.173     0.0587
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07027 0.04724 0.02051 79 640:   0%|          | 0/19 [00:00<?, ?it/s]74/99 7.3G 0.07027 0.04724 0.02051 79 640:   5%|▌         | 1/19 [00:00<00:07,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07362 0.04603 0.0195 91 640:   5%|▌         | 1/19 [00:00<00:07,  2.29it/s] 74/99 7.3G 0.07362 0.04603 0.0195 91 640:  11%|█         | 2/19 [00:00<00:07,  2.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07458 0.04637 0.01906 85 640:  11%|█         | 2/19 [00:01<00:07,  2.40it/s]74/99 7.3G 0.07458 0.04637 0.01906 85 640:  16%|█▌        | 3/19 [00:01<00:06,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07474 0.05161 0.01912 121 640:  16%|█▌        | 3/19 [00:01<00:06,  2.29it/s]74/99 7.3G 0.07474 0.05161 0.01912 121 640:  21%|██        | 4/19 [00:01<00:06,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07497 0.05505 0.01871 118 640:  21%|██        | 4/19 [00:02<00:06,  2.42it/s]74/99 7.3G 0.07497 0.05505 0.01871 118 640:  26%|██▋       | 5/19 [00:02<00:05,  2.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07499 0.05602 0.01862 118 640:  26%|██▋       | 5/19 [00:02<00:05,  2.53it/s]74/99 7.3G 0.07499 0.05602 0.01862 118 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07393 0.05482 0.01818 84 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s] 74/99 7.3G 0.07393 0.05482 0.01818 84 640:  37%|███▋      | 7/19 [00:02<00:04,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07351 0.052 0.0176 61 640:  37%|███▋      | 7/19 [00:03<00:04,  2.64it/s]   74/99 7.3G 0.07351 0.052 0.0176 61 640:  42%|████▏     | 8/19 [00:03<00:04,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07217 0.0494 0.01717 42 640:  42%|████▏     | 8/19 [00:06<00:04,  2.67it/s]74/99 7.3G 0.07217 0.0494 0.01717 42 640:  47%|████▋     | 9/19 [00:06<00:14,  1.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07198 0.04913 0.01739 79 640:  47%|████▋     | 9/19 [00:11<00:14,  1.42s/it]74/99 7.3G 0.07198 0.04913 0.01739 79 640:  53%|█████▎    | 10/19 [00:11<00:20,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07219 0.04954 0.01722 105 640:  53%|█████▎    | 10/19 [00:14<00:20,  2.29s/it]74/99 7.3G 0.07219 0.04954 0.01722 105 640:  58%|█████▊    | 11/19 [00:14<00:22,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07225 0.04939 0.01774 79 640:  58%|█████▊    | 11/19 [00:15<00:22,  2.79s/it] 74/99 7.3G 0.07225 0.04939 0.01774 79 640:  63%|██████▎   | 12/19 [00:15<00:14,  2.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07209 0.04896 0.0181 74 640:  63%|██████▎   | 12/19 [00:20<00:14,  2.06s/it] 74/99 7.3G 0.07209 0.04896 0.0181 74 640:  68%|██████▊   | 13/19 [00:20<00:17,  2.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07172 0.04887 0.01795 78 640:  68%|██████▊   | 13/19 [00:25<00:17,  2.89s/it]74/99 7.3G 0.07172 0.04887 0.01795 78 640:  74%|███████▎  | 14/19 [00:25<00:17,  3.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07158 0.0476 0.01779 54 640:  74%|███████▎  | 14/19 [00:25<00:17,  3.49s/it] 74/99 7.3G 0.07158 0.0476 0.01779 54 640:  79%|███████▉  | 15/19 [00:25<00:09,  2.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07155 0.04765 0.01759 86 640:  79%|███████▉  | 15/19 [00:25<00:09,  2.49s/it]74/99 7.3G 0.07155 0.04765 0.01759 86 640:  84%|████████▍ | 16/19 [00:25<00:05,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07185 0.04753 0.01767 86 640:  84%|████████▍ | 16/19 [00:30<00:05,  1.79s/it]74/99 7.3G 0.07185 0.04753 0.01767 86 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07192 0.04749 0.01759 81 640:  89%|████████▉ | 17/19 [00:34<00:05,  2.86s/it]74/99 7.3G 0.07192 0.04749 0.01759 81 640:  95%|█████████▍| 18/19 [00:34<00:03,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07199 0.0466 0.01762 49 640:  95%|█████████▍| 18/19 [00:37<00:03,  3.15s/it] 74/99 7.3G 0.07199 0.0466 0.01762 49 640: 100%|██████████| 19/19 [00:37<00:00,  3.05s/it]74/99 7.3G 0.07199 0.0466 0.01762 49 640: 100%|██████████| 19/19 [00:37<00:00,  1.97s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.80s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]
                   all         55        256      0.225      0.283      0.164     0.0567
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07065 0.03164 0.01675 60 640:   0%|          | 0/19 [00:00<?, ?it/s]75/99 7.3G 0.07065 0.03164 0.01675 60 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07446 0.05606 0.01683 174 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]75/99 7.3G 0.07446 0.05606 0.01683 174 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07109 0.04848 0.01523 56 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s] 75/99 7.3G 0.07109 0.04848 0.01523 56 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.06888 0.04266 0.01399 42 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]75/99 7.3G 0.06888 0.04266 0.01399 42 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07075 0.04268 0.01484 77 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]75/99 7.3G 0.07075 0.04268 0.01484 77 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07087 0.04394 0.01492 93 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s]75/99 7.3G 0.07087 0.04394 0.01492 93 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07059 0.04244 0.01564 54 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]75/99 7.3G 0.07059 0.04244 0.01564 54 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07178 0.04472 0.01542 153 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]75/99 7.3G 0.07178 0.04472 0.01542 153 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07132 0.04398 0.01606 62 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s] 75/99 7.3G 0.07132 0.04398 0.01606 62 640:  47%|████▋     | 9/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07143 0.0447 0.01633 88 640:  47%|████▋     | 9/19 [00:05<00:01,  5.75it/s] 75/99 7.3G 0.07143 0.0447 0.01633 88 640:  53%|█████▎    | 10/19 [00:05<00:12,  1.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07155 0.04452 0.01703 75 640:  53%|█████▎    | 10/19 [00:10<00:12,  1.34s/it]75/99 7.3G 0.07155 0.04452 0.01703 75 640:  58%|█████▊    | 11/19 [00:10<00:19,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07105 0.04484 0.01687 81 640:  58%|█████▊    | 11/19 [00:11<00:19,  2.40s/it]75/99 7.3G 0.07105 0.04484 0.01687 81 640:  63%|██████▎   | 12/19 [00:11<00:13,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07034 0.04311 0.01688 36 640:  63%|██████▎   | 12/19 [00:11<00:13,  1.89s/it]75/99 7.3G 0.07034 0.04311 0.01688 36 640:  68%|██████▊   | 13/19 [00:11<00:09,  1.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.06997 0.04195 0.01703 40 640:  68%|██████▊   | 13/19 [00:16<00:09,  1.54s/it]75/99 7.3G 0.06997 0.04195 0.01703 40 640:  74%|███████▎  | 14/19 [00:16<00:13,  2.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07009 0.04222 0.01711 71 640:  74%|███████▎  | 14/19 [00:19<00:13,  2.61s/it]75/99 7.3G 0.07009 0.04222 0.01711 71 640:  79%|███████▉  | 15/19 [00:19<00:10,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07051 0.04333 0.01725 112 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.67s/it]75/99 7.3G 0.07051 0.04333 0.01725 112 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07016 0.04285 0.01724 55 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.21s/it] 75/99 7.3G 0.07016 0.04285 0.01724 55 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07018 0.04231 0.01718 56 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.46s/it]75/99 7.3G 0.07018 0.04231 0.01718 56 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07039 0.04245 0.0171 86 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.62s/it] 75/99 7.3G 0.07039 0.04245 0.0171 86 640: 100%|██████████| 19/19 [00:30<00:00,  2.55s/it]75/99 7.3G 0.07039 0.04245 0.0171 86 640: 100%|██████████| 19/19 [00:30<00:00,  1.59s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.46s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.40s/it]
                   all         55        256      0.229      0.301      0.168     0.0578
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.06916 0.03331 0.01425 56 640:   0%|          | 0/19 [00:00<?, ?it/s]76/99 7.3G 0.06916 0.03331 0.01425 56 640:   5%|▌         | 1/19 [00:00<00:09,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07383 0.02994 0.01528 57 640:   5%|▌         | 1/19 [00:01<00:09,  1.83it/s]76/99 7.3G 0.07383 0.02994 0.01528 57 640:  11%|█         | 2/19 [00:01<00:11,  1.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.0742 0.03374 0.01741 69 640:  11%|█         | 2/19 [00:01<00:11,  1.54it/s] 76/99 7.3G 0.0742 0.03374 0.01741 69 640:  16%|█▌        | 3/19 [00:01<00:10,  1.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07354 0.032 0.01829 45 640:  16%|█▌        | 3/19 [00:02<00:10,  1.58it/s] 76/99 7.3G 0.07354 0.032 0.01829 45 640:  21%|██        | 4/19 [00:02<00:09,  1.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07402 0.03717 0.01868 113 640:  21%|██        | 4/19 [00:03<00:09,  1.57it/s]76/99 7.3G 0.07402 0.03717 0.01868 113 640:  26%|██▋       | 5/19 [00:03<00:09,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07487 0.03806 0.01797 100 640:  26%|██▋       | 5/19 [00:03<00:09,  1.50it/s]76/99 7.3G 0.07487 0.03806 0.01797 100 640:  32%|███▏      | 6/19 [00:03<00:08,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07493 0.03923 0.01827 80 640:  32%|███▏      | 6/19 [00:04<00:08,  1.53it/s] 76/99 7.3G 0.07493 0.03923 0.01827 80 640:  37%|███▋      | 7/19 [00:04<00:08,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07459 0.04042 0.01804 83 640:  37%|███▋      | 7/19 [00:05<00:08,  1.50it/s]76/99 7.3G 0.07459 0.04042 0.01804 83 640:  42%|████▏     | 8/19 [00:05<00:06,  1.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07443 0.04195 0.01808 100 640:  42%|████▏     | 8/19 [00:05<00:06,  1.60it/s]76/99 7.3G 0.07443 0.04195 0.01808 100 640:  47%|████▋     | 9/19 [00:05<00:06,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07416 0.04408 0.01794 104 640:  47%|████▋     | 9/19 [00:06<00:06,  1.65it/s]76/99 7.3G 0.07416 0.04408 0.01794 104 640:  53%|█████▎    | 10/19 [00:06<00:05,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.0747 0.04554 0.01801 113 640:  53%|█████▎    | 10/19 [00:06<00:05,  1.64it/s] 76/99 7.3G 0.0747 0.04554 0.01801 113 640:  58%|█████▊    | 11/19 [00:06<00:04,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07491 0.04718 0.01779 125 640:  58%|█████▊    | 11/19 [00:08<00:04,  1.70it/s]76/99 7.3G 0.07491 0.04718 0.01779 125 640:  63%|██████▎   | 12/19 [00:08<00:06,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07472 0.045 0.01739 29 640:  63%|██████▎   | 12/19 [00:14<00:06,  1.03it/s]   76/99 7.3G 0.07472 0.045 0.01739 29 640:  68%|██████▊   | 13/19 [00:14<00:14,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07429 0.04453 0.01714 64 640:  68%|██████▊   | 13/19 [00:15<00:14,  2.45s/it]76/99 7.3G 0.07429 0.04453 0.01714 64 640:  74%|███████▎  | 14/19 [00:15<00:09,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07396 0.04436 0.01698 81 640:  74%|███████▎  | 14/19 [00:18<00:09,  1.86s/it]76/99 7.3G 0.07396 0.04436 0.01698 81 640:  79%|███████▉  | 15/19 [00:18<00:09,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07334 0.04401 0.01675 73 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.27s/it]76/99 7.3G 0.07334 0.04401 0.01675 73 640:  84%|████████▍ | 16/19 [00:22<00:08,  2.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07331 0.04432 0.01677 89 640:  84%|████████▍ | 16/19 [00:22<00:08,  2.86s/it]76/99 7.3G 0.07331 0.04432 0.01677 89 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07304 0.04358 0.01664 56 640:  89%|████████▉ | 17/19 [00:23<00:04,  2.11s/it]76/99 7.3G 0.07304 0.04358 0.01664 56 640:  95%|█████████▍| 18/19 [00:23<00:01,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.0731 0.04367 0.0166 83 640:  95%|█████████▍| 18/19 [00:27<00:01,  1.58s/it]  76/99 7.3G 0.0731 0.04367 0.0166 83 640: 100%|██████████| 19/19 [00:27<00:00,  2.36s/it]76/99 7.3G 0.0731 0.04367 0.0166 83 640: 100%|██████████| 19/19 [00:27<00:00,  1.44s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.63s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.88s/it]
                   all         55        256      0.263      0.314       0.19     0.0684
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07198 0.04598 0.01362 86 640:   0%|          | 0/19 [00:00<?, ?it/s]77/99 7.3G 0.07198 0.04598 0.01362 86 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.0738 0.03803 0.01425 51 640:   5%|▌         | 1/19 [00:01<00:09,  1.82it/s] 77/99 7.3G 0.0738 0.03803 0.01425 51 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07526 0.04632 0.0153 131 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]77/99 7.3G 0.07526 0.04632 0.0153 131 640:  16%|█▌        | 3/19 [00:01<00:06,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07618 0.04574 0.01769 83 640:  16%|█▌        | 3/19 [00:01<00:06,  2.61it/s]77/99 7.3G 0.07618 0.04574 0.01769 83 640:  21%|██        | 4/19 [00:01<00:05,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.0749 0.04196 0.01773 43 640:  21%|██        | 4/19 [00:01<00:05,  2.68it/s] 77/99 7.3G 0.0749 0.04196 0.01773 43 640:  26%|██▋       | 5/19 [00:01<00:04,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07465 0.04169 0.01755 77 640:  26%|██▋       | 5/19 [00:02<00:04,  2.82it/s]77/99 7.3G 0.07465 0.04169 0.01755 77 640:  32%|███▏      | 6/19 [00:02<00:03,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07222 0.03901 0.01645 39 640:  32%|███▏      | 6/19 [00:02<00:03,  3.41it/s]77/99 7.3G 0.07222 0.03901 0.01645 39 640:  37%|███▋      | 7/19 [00:02<00:03,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07171 0.03815 0.01586 55 640:  37%|███▋      | 7/19 [00:02<00:03,  3.93it/s]77/99 7.3G 0.07171 0.03815 0.01586 55 640:  42%|████▏     | 8/19 [00:02<00:02,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07166 0.03723 0.01586 48 640:  42%|████▏     | 8/19 [00:03<00:02,  4.37it/s]77/99 7.3G 0.07166 0.03723 0.01586 48 640:  47%|████▋     | 9/19 [00:03<00:06,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07184 0.03861 0.01644 86 640:  47%|████▋     | 9/19 [00:10<00:06,  1.64it/s]77/99 7.3G 0.07184 0.03861 0.01644 86 640:  53%|█████▎    | 10/19 [00:10<00:21,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07166 0.03953 0.0163 81 640:  53%|█████▎    | 10/19 [00:10<00:21,  2.37s/it] 77/99 7.3G 0.07166 0.03953 0.0163 81 640:  58%|█████▊    | 11/19 [00:10<00:14,  1.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.0718 0.03996 0.01631 83 640:  58%|█████▊    | 11/19 [00:12<00:14,  1.87s/it]77/99 7.3G 0.0718 0.03996 0.01631 83 640:  63%|██████▎   | 12/19 [00:12<00:12,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07124 0.03901 0.01607 46 640:  63%|██████▎   | 12/19 [00:18<00:12,  1.75s/it]77/99 7.3G 0.07124 0.03901 0.01607 46 640:  68%|██████▊   | 13/19 [00:18<00:18,  3.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07105 0.0383 0.01645 46 640:  68%|██████▊   | 13/19 [00:20<00:18,  3.09s/it] 77/99 7.3G 0.07105 0.0383 0.01645 46 640:  74%|███████▎  | 14/19 [00:20<00:13,  2.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07113 0.03915 0.01667 79 640:  74%|███████▎  | 14/19 [00:20<00:13,  2.66s/it]77/99 7.3G 0.07113 0.03915 0.01667 79 640:  79%|███████▉  | 15/19 [00:20<00:07,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07121 0.03876 0.01679 52 640:  79%|███████▉  | 15/19 [00:25<00:07,  1.97s/it]77/99 7.3G 0.07121 0.03876 0.01679 52 640:  84%|████████▍ | 16/19 [00:25<00:08,  2.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07142 0.03922 0.01675 85 640:  84%|████████▍ | 16/19 [00:29<00:08,  2.71s/it]77/99 7.3G 0.07142 0.03922 0.01675 85 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07117 0.0389 0.0166 59 640:  89%|████████▉ | 17/19 [00:32<00:06,  3.15s/it]  77/99 7.3G 0.07117 0.0389 0.0166 59 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07092 0.03886 0.01683 60 640:  95%|█████████▍| 18/19 [00:33<00:03,  3.12s/it]77/99 7.3G 0.07092 0.03886 0.01683 60 640: 100%|██████████| 19/19 [00:33<00:00,  2.43s/it]77/99 7.3G 0.07092 0.03886 0.01683 60 640: 100%|██████████| 19/19 [00:33<00:00,  1.74s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
                   all         55        256      0.248      0.247      0.187     0.0658
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07432 0.06026 0.01863 115 640:   0%|          | 0/19 [00:00<?, ?it/s]78/99 7.3G 0.07432 0.06026 0.01863 115 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07412 0.06149 0.0192 118 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s] 78/99 7.3G 0.07412 0.06149 0.0192 118 640:  11%|█         | 2/19 [00:00<00:03,  4.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07383 0.04968 0.01754 43 640:  11%|█         | 2/19 [00:00<00:03,  4.81it/s]78/99 7.3G 0.07383 0.04968 0.01754 43 640:  16%|█▌        | 3/19 [00:00<00:03,  4.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07247 0.04518 0.01626 59 640:  16%|█▌        | 3/19 [00:00<00:03,  4.04it/s]78/99 7.3G 0.07247 0.04518 0.01626 59 640:  21%|██        | 4/19 [00:00<00:03,  4.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07116 0.04201 0.01544 50 640:  21%|██        | 4/19 [00:01<00:03,  4.57it/s]78/99 7.3G 0.07116 0.04201 0.01544 50 640:  26%|██▋       | 5/19 [00:01<00:02,  4.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07179 0.04344 0.01517 103 640:  26%|██▋       | 5/19 [00:01<00:02,  4.92it/s]78/99 7.3G 0.07179 0.04344 0.01517 103 640:  32%|███▏      | 6/19 [00:01<00:02,  4.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07223 0.04394 0.01524 86 640:  32%|███▏      | 6/19 [00:01<00:02,  4.39it/s] 78/99 7.3G 0.07223 0.04394 0.01524 86 640:  37%|███▋      | 7/19 [00:01<00:03,  3.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07145 0.04492 0.01587 78 640:  37%|███▋      | 7/19 [00:01<00:03,  3.75it/s]78/99 7.3G 0.07145 0.04492 0.01587 78 640:  42%|████▏     | 8/19 [00:01<00:02,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07056 0.04291 0.01568 47 640:  42%|████▏     | 8/19 [00:02<00:02,  3.83it/s]78/99 7.3G 0.07056 0.04291 0.01568 47 640:  47%|████▋     | 9/19 [00:02<00:02,  4.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07064 0.04311 0.01594 77 640:  47%|████▋     | 9/19 [00:09<00:02,  4.28it/s]78/99 7.3G 0.07064 0.04311 0.01594 77 640:  53%|█████▎    | 10/19 [00:09<00:21,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07044 0.04263 0.01595 70 640:  53%|█████▎    | 10/19 [00:09<00:21,  2.39s/it]78/99 7.3G 0.07044 0.04263 0.01595 70 640:  58%|█████▊    | 11/19 [00:09<00:14,  1.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07002 0.04208 0.01577 59 640:  58%|█████▊    | 11/19 [00:10<00:14,  1.83s/it]78/99 7.3G 0.07002 0.04208 0.01577 59 640:  63%|██████▎   | 12/19 [00:10<00:09,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07046 0.04307 0.01605 89 640:  63%|██████▎   | 12/19 [00:16<00:09,  1.39s/it]78/99 7.3G 0.07046 0.04307 0.01605 89 640:  68%|██████▊   | 13/19 [00:16<00:17,  2.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07003 0.0424 0.01582 54 640:  68%|██████▊   | 13/19 [00:20<00:17,  2.87s/it] 78/99 7.3G 0.07003 0.0424 0.01582 54 640:  74%|███████▎  | 14/19 [00:20<00:15,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.06994 0.0427 0.01564 84 640:  74%|███████▎  | 14/19 [00:24<00:15,  3.15s/it]78/99 7.3G 0.06994 0.0427 0.01564 84 640:  79%|███████▉  | 15/19 [00:24<00:13,  3.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.0697 0.04219 0.01561 53 640:  79%|███████▉  | 15/19 [00:25<00:13,  3.48s/it]78/99 7.3G 0.0697 0.04219 0.01561 53 640:  84%|████████▍ | 16/19 [00:25<00:07,  2.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.06997 0.04207 0.0156 72 640:  84%|████████▍ | 16/19 [00:27<00:07,  2.60s/it]78/99 7.3G 0.06997 0.04207 0.0156 72 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07049 0.04255 0.01565 108 640:  89%|████████▉ | 17/19 [00:30<00:04,  2.43s/it]78/99 7.3G 0.07049 0.04255 0.01565 108 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07037 0.04322 0.01551 106 640:  95%|█████████▍| 18/19 [00:31<00:02,  2.77s/it]78/99 7.3G 0.07037 0.04322 0.01551 106 640: 100%|██████████| 19/19 [00:31<00:00,  2.05s/it]78/99 7.3G 0.07037 0.04322 0.01551 106 640: 100%|██████████| 19/19 [00:31<00:00,  1.64s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.01s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.13s/it]
                   all         55        256      0.247       0.26      0.194     0.0702
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.0757 0.04353 0.01382 93 640:   0%|          | 0/19 [00:00<?, ?it/s]79/99 7.3G 0.0757 0.04353 0.01382 93 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07051 0.03609 0.01445 43 640:   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]79/99 7.3G 0.07051 0.03609 0.01445 43 640:  11%|█         | 2/19 [00:00<00:06,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07066 0.03792 0.01412 84 640:  11%|█         | 2/19 [00:01<00:06,  2.62it/s]79/99 7.3G 0.07066 0.03792 0.01412 84 640:  16%|█▌        | 3/19 [00:01<00:06,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07126 0.03807 0.01413 72 640:  16%|█▌        | 3/19 [00:01<00:06,  2.64it/s]79/99 7.3G 0.07126 0.03807 0.01413 72 640:  21%|██        | 4/19 [00:01<00:04,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07297 0.0401 0.01447 89 640:  21%|██        | 4/19 [00:01<00:04,  3.22it/s] 79/99 7.3G 0.07297 0.0401 0.01447 89 640:  26%|██▋       | 5/19 [00:01<00:03,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07298 0.0436 0.0155 111 640:  26%|██▋       | 5/19 [00:01<00:03,  3.82it/s]79/99 7.3G 0.07298 0.0436 0.0155 111 640:  32%|███▏      | 6/19 [00:01<00:03,  4.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07321 0.04415 0.01554 87 640:  32%|███▏      | 6/19 [00:01<00:03,  4.32it/s]79/99 7.3G 0.07321 0.04415 0.01554 87 640:  37%|███▋      | 7/19 [00:01<00:02,  4.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07321 0.04316 0.01585 59 640:  37%|███▋      | 7/19 [00:02<00:02,  4.70it/s]79/99 7.3G 0.07321 0.04316 0.01585 59 640:  42%|████▏     | 8/19 [00:02<00:02,  4.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07327 0.04251 0.01593 64 640:  42%|████▏     | 8/19 [00:02<00:02,  4.99it/s]79/99 7.3G 0.07327 0.04251 0.01593 64 640:  47%|████▋     | 9/19 [00:02<00:01,  5.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07296 0.04291 0.01592 84 640:  47%|████▋     | 9/19 [00:05<00:01,  5.21it/s]79/99 7.3G 0.07296 0.04291 0.01592 84 640:  53%|█████▎    | 10/19 [00:05<00:10,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07244 0.04179 0.01606 50 640:  53%|█████▎    | 10/19 [00:08<00:10,  1.18s/it]79/99 7.3G 0.07244 0.04179 0.01606 50 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07329 0.04272 0.01593 152 640:  58%|█████▊    | 11/19 [00:11<00:12,  1.56s/it]79/99 7.3G 0.07329 0.04272 0.01593 152 640:  63%|██████▎   | 12/19 [00:11<00:15,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07277 0.04257 0.01582 72 640:  63%|██████▎   | 12/19 [00:12<00:15,  2.27s/it] 79/99 7.3G 0.07277 0.04257 0.01582 72 640:  68%|██████▊   | 13/19 [00:12<00:09,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07353 0.04474 0.01605 188 640:  68%|██████▊   | 13/19 [00:16<00:09,  1.63s/it]79/99 7.3G 0.07353 0.04474 0.01605 188 640:  74%|███████▎  | 14/19 [00:16<00:12,  2.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07337 0.04447 0.01606 69 640:  74%|███████▎  | 14/19 [00:23<00:12,  2.52s/it] 79/99 7.3G 0.07337 0.04447 0.01606 69 640:  79%|███████▉  | 15/19 [00:23<00:15,  3.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07353 0.0453 0.01639 103 640:  79%|███████▉  | 15/19 [00:24<00:15,  3.97s/it]79/99 7.3G 0.07353 0.0453 0.01639 103 640:  84%|████████▍ | 16/19 [00:24<00:08,  2.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07281 0.04495 0.01645 58 640:  84%|████████▍ | 16/19 [00:25<00:08,  2.93s/it]79/99 7.3G 0.07281 0.04495 0.01645 58 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07242 0.04472 0.01636 73 640:  89%|████████▉ | 17/19 [00:28<00:04,  2.20s/it]79/99 7.3G 0.07242 0.04472 0.01636 73 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07202 0.04437 0.01653 57 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.60s/it]79/99 7.3G 0.07202 0.04437 0.01653 57 640: 100%|██████████| 19/19 [00:30<00:00,  2.32s/it]79/99 7.3G 0.07202 0.04437 0.01653 57 640: 100%|██████████| 19/19 [00:30<00:00,  1.59s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]
                   all         55        256      0.217      0.264      0.182     0.0631
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.06736 0.0458 0.02125 79 640:   0%|          | 0/19 [00:00<?, ?it/s]80/99 7.3G 0.06736 0.0458 0.02125 79 640:   5%|▌         | 1/19 [00:00<00:03,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07378 0.06831 0.01893 201 640:   5%|▌         | 1/19 [00:00<00:03,  5.59it/s]80/99 7.3G 0.07378 0.06831 0.01893 201 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07323 0.05596 0.01781 58 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s] 80/99 7.3G 0.07323 0.05596 0.01781 58 640:  16%|█▌        | 3/19 [00:00<00:03,  4.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07168 0.05012 0.01721 55 640:  16%|█▌        | 3/19 [00:01<00:03,  4.09it/s]80/99 7.3G 0.07168 0.05012 0.01721 55 640:  21%|██        | 4/19 [00:01<00:06,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07101 0.05182 0.01689 106 640:  21%|██        | 4/19 [00:02<00:06,  2.23it/s]80/99 7.3G 0.07101 0.05182 0.01689 106 640:  26%|██▋       | 5/19 [00:02<00:08,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07024 0.04847 0.01702 49 640:  26%|██▋       | 5/19 [00:03<00:08,  1.70it/s] 80/99 7.3G 0.07024 0.04847 0.01702 49 640:  32%|███▏      | 6/19 [00:03<00:08,  1.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07122 0.05036 0.01639 132 640:  32%|███▏      | 6/19 [00:03<00:08,  1.56it/s]80/99 7.3G 0.07122 0.05036 0.01639 132 640:  37%|███▋      | 7/19 [00:03<00:07,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07169 0.04969 0.01715 75 640:  37%|███▋      | 7/19 [00:04<00:07,  1.64it/s] 80/99 7.3G 0.07169 0.04969 0.01715 75 640:  42%|████▏     | 8/19 [00:04<00:07,  1.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07137 0.04951 0.01702 87 640:  42%|████▏     | 8/19 [00:04<00:07,  1.56it/s]80/99 7.3G 0.07137 0.04951 0.01702 87 640:  47%|████▋     | 9/19 [00:04<00:06,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07151 0.04819 0.01676 77 640:  47%|████▋     | 9/19 [00:05<00:06,  1.64it/s]80/99 7.3G 0.07151 0.04819 0.01676 77 640:  53%|█████▎    | 10/19 [00:05<00:05,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07149 0.04685 0.01659 61 640:  53%|█████▎    | 10/19 [00:09<00:05,  1.65it/s]80/99 7.3G 0.07149 0.04685 0.01659 61 640:  58%|█████▊    | 11/19 [00:09<00:14,  1.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07146 0.04721 0.01656 100 640:  58%|█████▊    | 11/19 [00:15<00:14,  1.81s/it]80/99 7.3G 0.07146 0.04721 0.01656 100 640:  63%|██████▎   | 12/19 [00:15<00:20,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07235 0.04773 0.01677 123 640:  63%|██████▎   | 12/19 [00:15<00:20,  2.96s/it]80/99 7.3G 0.07235 0.04773 0.01677 123 640:  68%|██████▊   | 13/19 [00:15<00:13,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07227 0.04688 0.01661 71 640:  68%|██████▊   | 13/19 [00:16<00:13,  2.17s/it] 80/99 7.3G 0.07227 0.04688 0.01661 71 640:  74%|███████▎  | 14/19 [00:16<00:08,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07215 0.04681 0.01631 77 640:  74%|███████▎  | 14/19 [00:21<00:08,  1.63s/it]80/99 7.3G 0.07215 0.04681 0.01631 77 640:  79%|███████▉  | 15/19 [00:21<00:10,  2.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07234 0.04752 0.01637 96 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.58s/it]80/99 7.3G 0.07234 0.04752 0.01637 96 640:  84%|████████▍ | 16/19 [00:24<00:08,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07214 0.04677 0.01648 53 640:  84%|████████▍ | 16/19 [00:27<00:08,  2.79s/it]80/99 7.3G 0.07214 0.04677 0.01648 53 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07209 0.04578 0.01621 59 640:  89%|████████▉ | 17/19 [00:28<00:05,  2.92s/it]80/99 7.3G 0.07209 0.04578 0.01621 59 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07195 0.0451 0.0165 48 640:  95%|█████████▍| 18/19 [00:31<00:02,  2.23s/it]  80/99 7.3G 0.07195 0.0451 0.0165 48 640: 100%|██████████| 19/19 [00:31<00:00,  2.44s/it]80/99 7.3G 0.07195 0.0451 0.0165 48 640: 100%|██████████| 19/19 [00:31<00:00,  1.64s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.58s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.43s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]
                   all         55        256      0.222      0.309      0.146     0.0542
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.06789 0.03168 0.01191 57 640:   0%|          | 0/19 [00:00<?, ?it/s]81/99 7.3G 0.06789 0.03168 0.01191 57 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.06657 0.03138 0.01275 54 640:   5%|▌         | 1/19 [00:00<00:06,  2.79it/s]81/99 7.3G 0.06657 0.03138 0.01275 54 640:  11%|█         | 2/19 [00:00<00:06,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.06947 0.03712 0.01517 87 640:  11%|█         | 2/19 [00:01<00:06,  2.77it/s]81/99 7.3G 0.06947 0.03712 0.01517 87 640:  16%|█▌        | 3/19 [00:01<00:06,  2.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07051 0.04264 0.01573 104 640:  16%|█▌        | 3/19 [00:01<00:06,  2.37it/s]81/99 7.3G 0.07051 0.04264 0.01573 104 640:  21%|██        | 4/19 [00:01<00:08,  1.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07101 0.04523 0.01544 108 640:  21%|██        | 4/19 [00:02<00:08,  1.87it/s]81/99 7.3G 0.07101 0.04523 0.01544 108 640:  26%|██▋       | 5/19 [00:02<00:07,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07206 0.04834 0.01519 141 640:  26%|██▋       | 5/19 [00:03<00:07,  1.78it/s]81/99 7.3G 0.07206 0.04834 0.01519 141 640:  32%|███▏      | 6/19 [00:03<00:07,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07215 0.04525 0.01664 49 640:  32%|███▏      | 6/19 [00:03<00:07,  1.83it/s] 81/99 7.3G 0.07215 0.04525 0.01664 49 640:  37%|███▋      | 7/19 [00:03<00:06,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.0723 0.0456 0.01678 80 640:  37%|███▋      | 7/19 [00:04<00:06,  1.82it/s]  81/99 7.3G 0.0723 0.0456 0.01678 80 640:  42%|████▏     | 8/19 [00:04<00:06,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07297 0.04565 0.01678 87 640:  42%|████▏     | 8/19 [00:05<00:06,  1.81it/s]81/99 7.3G 0.07297 0.04565 0.01678 87 640:  47%|████▋     | 9/19 [00:05<00:06,  1.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07313 0.04469 0.01713 67 640:  47%|████▋     | 9/19 [00:05<00:06,  1.43it/s]81/99 7.3G 0.07313 0.04469 0.01713 67 640:  53%|█████▎    | 10/19 [00:05<00:06,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.0725 0.04461 0.01688 72 640:  53%|█████▎    | 10/19 [00:06<00:06,  1.40it/s] 81/99 7.3G 0.0725 0.04461 0.01688 72 640:  58%|█████▊    | 11/19 [00:06<00:05,  1.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07293 0.04523 0.01663 89 640:  58%|█████▊    | 11/19 [00:12<00:05,  1.36it/s]81/99 7.3G 0.07293 0.04523 0.01663 89 640:  63%|██████▎   | 12/19 [00:12<00:15,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07302 0.04549 0.01643 96 640:  63%|██████▎   | 12/19 [00:14<00:15,  2.26s/it]81/99 7.3G 0.07302 0.04549 0.01643 96 640:  68%|██████▊   | 13/19 [00:14<00:12,  2.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07253 0.04597 0.01631 77 640:  68%|██████▊   | 13/19 [00:20<00:12,  2.09s/it]81/99 7.3G 0.07253 0.04597 0.01631 77 640:  74%|███████▎  | 14/19 [00:20<00:16,  3.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07248 0.04528 0.01629 63 640:  74%|███████▎  | 14/19 [00:21<00:16,  3.28s/it]81/99 7.3G 0.07248 0.04528 0.01629 63 640:  79%|███████▉  | 15/19 [00:21<00:10,  2.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07198 0.04481 0.01619 64 640:  79%|███████▉  | 15/19 [00:24<00:10,  2.60s/it]81/99 7.3G 0.07198 0.04481 0.01619 64 640:  84%|████████▍ | 16/19 [00:24<00:08,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07169 0.04415 0.01601 55 640:  84%|████████▍ | 16/19 [00:27<00:08,  2.67s/it]81/99 7.3G 0.07169 0.04415 0.01601 55 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07156 0.04413 0.01599 68 640:  89%|████████▉ | 17/19 [00:27<00:05,  2.79s/it]81/99 7.3G 0.07156 0.04413 0.01599 68 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07129 0.04348 0.0161 53 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.00s/it] 81/99 7.3G 0.07129 0.04348 0.0161 53 640: 100%|██████████| 19/19 [00:27<00:00,  1.51s/it]81/99 7.3G 0.07129 0.04348 0.0161 53 640: 100%|██████████| 19/19 [00:27<00:00,  1.45s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]
                   all         55        256      0.228      0.302      0.165     0.0576
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07582 0.04042 0.01306 78 640:   0%|          | 0/19 [00:00<?, ?it/s]82/99 7.3G 0.07582 0.04042 0.01306 78 640:   5%|▌         | 1/19 [00:00<00:04,  4.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07071 0.03864 0.01392 64 640:   5%|▌         | 1/19 [00:00<00:04,  4.39it/s]82/99 7.3G 0.07071 0.03864 0.01392 64 640:  11%|█         | 2/19 [00:00<00:03,  4.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.0704 0.04448 0.0133 110 640:  11%|█         | 2/19 [00:00<00:03,  4.44it/s] 82/99 7.3G 0.0704 0.04448 0.0133 110 640:  16%|█▌        | 3/19 [00:00<00:03,  4.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07041 0.04445 0.0149 76 640:  16%|█▌        | 3/19 [00:00<00:03,  4.45it/s]82/99 7.3G 0.07041 0.04445 0.0149 76 640:  21%|██        | 4/19 [00:00<00:03,  4.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07001 0.04495 0.016 80 640:  21%|██        | 4/19 [00:01<00:03,  4.42it/s] 82/99 7.3G 0.07001 0.04495 0.016 80 640:  26%|██▋       | 5/19 [00:01<00:03,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06927 0.04413 0.01555 65 640:  26%|██▋       | 5/19 [00:02<00:03,  3.55it/s]82/99 7.3G 0.06927 0.04413 0.01555 65 640:  32%|███▏      | 6/19 [00:02<00:05,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07014 0.04312 0.01543 79 640:  32%|███▏      | 6/19 [00:02<00:05,  2.30it/s]82/99 7.3G 0.07014 0.04312 0.01543 79 640:  37%|███▋      | 7/19 [00:02<00:05,  2.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07014 0.04261 0.01517 70 640:  37%|███▋      | 7/19 [00:02<00:05,  2.27it/s]82/99 7.3G 0.07014 0.04261 0.01517 70 640:  42%|████▏     | 8/19 [00:02<00:04,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07031 0.04231 0.01528 64 640:  42%|████▏     | 8/19 [00:06<00:04,  2.39it/s]82/99 7.3G 0.07031 0.04231 0.01528 64 640:  47%|████▋     | 9/19 [00:06<00:15,  1.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07022 0.04302 0.01526 91 640:  47%|████▋     | 9/19 [00:07<00:15,  1.54s/it]82/99 7.3G 0.07022 0.04302 0.01526 91 640:  53%|█████▎    | 10/19 [00:07<00:12,  1.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06892 0.04132 0.01526 36 640:  53%|█████▎    | 10/19 [00:13<00:12,  1.42s/it]82/99 7.3G 0.06892 0.04132 0.01526 36 640:  58%|█████▊    | 11/19 [00:13<00:20,  2.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.0693 0.04216 0.01506 106 640:  58%|█████▊    | 11/19 [00:15<00:20,  2.56s/it]82/99 7.3G 0.0693 0.04216 0.01506 106 640:  63%|██████▎   | 12/19 [00:15<00:16,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06981 0.04461 0.0153 137 640:  63%|██████▎   | 12/19 [00:17<00:16,  2.40s/it]82/99 7.3G 0.06981 0.04461 0.0153 137 640:  68%|██████▊   | 13/19 [00:17<00:14,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06958 0.04391 0.01514 62 640:  68%|██████▊   | 13/19 [00:22<00:14,  2.40s/it]82/99 7.3G 0.06958 0.04391 0.01514 62 640:  74%|███████▎  | 14/19 [00:22<00:15,  3.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06956 0.04387 0.01529 74 640:  74%|███████▎  | 14/19 [00:22<00:15,  3.05s/it]82/99 7.3G 0.06956 0.04387 0.01529 74 640:  79%|███████▉  | 15/19 [00:22<00:08,  2.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06977 0.04405 0.01533 86 640:  79%|███████▉  | 15/19 [00:23<00:08,  2.19s/it]82/99 7.3G 0.06977 0.04405 0.01533 86 640:  84%|████████▍ | 16/19 [00:23<00:05,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06949 0.0442 0.01528 77 640:  84%|████████▍ | 16/19 [00:29<00:05,  1.96s/it] 82/99 7.3G 0.06949 0.0442 0.01528 77 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06949 0.04419 0.01513 80 640:  89%|████████▉ | 17/19 [00:30<00:06,  3.16s/it]82/99 7.3G 0.06949 0.04419 0.01513 80 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.06961 0.04365 0.015 67 640:  95%|█████████▍| 18/19 [00:35<00:02,  2.44s/it]  82/99 7.3G 0.06961 0.04365 0.015 67 640: 100%|██████████| 19/19 [00:35<00:00,  3.19s/it]82/99 7.3G 0.06961 0.04365 0.015 67 640: 100%|██████████| 19/19 [00:35<00:00,  1.86s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.12s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]
                   all         55        256      0.241      0.312      0.179     0.0598
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07339 0.05801 0.01699 99 640:   0%|          | 0/19 [00:00<?, ?it/s]83/99 7.3G 0.07339 0.05801 0.01699 99 640:   5%|▌         | 1/19 [00:00<00:10,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07273 0.04676 0.01571 70 640:   5%|▌         | 1/19 [00:00<00:10,  1.70it/s]83/99 7.3G 0.07273 0.04676 0.01571 70 640:  11%|█         | 2/19 [00:00<00:07,  2.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07237 0.04494 0.01586 80 640:  11%|█         | 2/19 [00:01<00:07,  2.15it/s]83/99 7.3G 0.07237 0.04494 0.01586 80 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07363 0.04412 0.01588 90 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]83/99 7.3G 0.07363 0.04412 0.01588 90 640:  21%|██        | 4/19 [00:01<00:05,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07278 0.04488 0.01547 91 640:  21%|██        | 4/19 [00:01<00:05,  2.74it/s]83/99 7.3G 0.07278 0.04488 0.01547 91 640:  26%|██▋       | 5/19 [00:01<00:05,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07194 0.04276 0.01681 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.73it/s]83/99 7.3G 0.07194 0.04276 0.01681 56 640:  32%|███▏      | 6/19 [00:02<00:04,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07203 0.04428 0.01803 91 640:  32%|███▏      | 6/19 [00:02<00:04,  2.66it/s]83/99 7.3G 0.07203 0.04428 0.01803 91 640:  37%|███▋      | 7/19 [00:02<00:03,  3.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07132 0.04488 0.01768 88 640:  37%|███▋      | 7/19 [00:02<00:03,  3.06it/s]83/99 7.3G 0.07132 0.04488 0.01768 88 640:  42%|████▏     | 8/19 [00:02<00:03,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.06998 0.043 0.01723 43 640:  42%|████▏     | 8/19 [00:02<00:03,  3.46it/s]  83/99 7.3G 0.06998 0.043 0.01723 43 640:  47%|████▋     | 9/19 [00:02<00:02,  3.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07095 0.04627 0.01743 149 640:  47%|████▋     | 9/19 [00:06<00:02,  3.76it/s]83/99 7.3G 0.07095 0.04627 0.01743 149 640:  53%|█████▎    | 10/19 [00:06<00:11,  1.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07141 0.0467 0.0173 99 640:  53%|█████▎    | 10/19 [00:10<00:11,  1.32s/it]   83/99 7.3G 0.07141 0.0467 0.0173 99 640:  58%|█████▊    | 11/19 [00:10<00:16,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07116 0.04513 0.01703 44 640:  58%|█████▊    | 11/19 [00:10<00:16,  2.01s/it]83/99 7.3G 0.07116 0.04513 0.01703 44 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07136 0.04484 0.01679 79 640:  63%|██████▎   | 12/19 [00:13<00:10,  1.45s/it]83/99 7.3G 0.07136 0.04484 0.01679 79 640:  68%|██████▊   | 13/19 [00:13<00:10,  1.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07174 0.04775 0.01691 167 640:  68%|██████▊   | 13/19 [00:18<00:10,  1.80s/it]83/99 7.3G 0.07174 0.04775 0.01691 167 640:  74%|███████▎  | 14/19 [00:18<00:14,  2.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07176 0.04742 0.017 81 640:  74%|███████▎  | 14/19 [00:19<00:14,  2.89s/it]   83/99 7.3G 0.07176 0.04742 0.017 81 640:  79%|███████▉  | 15/19 [00:19<00:09,  2.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07244 0.04691 0.01706 104 640:  79%|███████▉  | 15/19 [00:24<00:09,  2.35s/it]83/99 7.3G 0.07244 0.04691 0.01706 104 640:  84%|████████▍ | 16/19 [00:24<00:09,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07265 0.04768 0.01717 132 640:  84%|████████▍ | 16/19 [00:25<00:09,  3.16s/it]83/99 7.3G 0.07265 0.04768 0.01717 132 640:  89%|████████▉ | 17/19 [00:25<00:05,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07287 0.04817 0.01737 84 640:  89%|████████▉ | 17/19 [00:29<00:05,  2.63s/it] 83/99 7.3G 0.07287 0.04817 0.01737 84 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07243 0.04754 0.01724 58 640:  95%|█████████▍| 18/19 [00:32<00:02,  2.78s/it]83/99 7.3G 0.07243 0.04754 0.01724 58 640: 100%|██████████| 19/19 [00:32<00:00,  3.07s/it]83/99 7.3G 0.07243 0.04754 0.01724 58 640: 100%|██████████| 19/19 [00:32<00:00,  1.73s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.81s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.63s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]
                   all         55        256      0.213      0.299      0.167     0.0618
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.06864 0.03849 0.01471 66 640:   0%|          | 0/19 [00:00<?, ?it/s]84/99 7.3G 0.06864 0.03849 0.01471 66 640:   5%|▌         | 1/19 [00:00<00:03,  5.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07117 0.04603 0.01602 100 640:   5%|▌         | 1/19 [00:00<00:03,  5.09it/s]84/99 7.3G 0.07117 0.04603 0.01602 100 640:  11%|█         | 2/19 [00:00<00:05,  3.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0724 0.04826 0.01655 94 640:  11%|█         | 2/19 [00:00<00:05,  3.24it/s]  84/99 7.3G 0.0724 0.04826 0.01655 94 640:  16%|█▌        | 3/19 [00:00<00:05,  3.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07163 0.04554 0.0182 60 640:  16%|█▌        | 3/19 [00:01<00:05,  3.06it/s]84/99 7.3G 0.07163 0.04554 0.0182 60 640:  21%|██        | 4/19 [00:01<00:04,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07019 0.04093 0.01788 36 640:  21%|██        | 4/19 [00:01<00:04,  3.23it/s]84/99 7.3G 0.07019 0.04093 0.01788 36 640:  26%|██▋       | 5/19 [00:01<00:03,  3.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.06856 0.03909 0.01709 49 640:  26%|██▋       | 5/19 [00:01<00:03,  3.73it/s]84/99 7.3G 0.06856 0.03909 0.01709 49 640:  32%|███▏      | 6/19 [00:01<00:04,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.06893 0.03736 0.0166 46 640:  32%|███▏      | 6/19 [00:02<00:04,  3.12it/s] 84/99 7.3G 0.06893 0.03736 0.0166 46 640:  37%|███▋      | 7/19 [00:02<00:03,  3.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0698 0.04216 0.017 124 640:  37%|███▋      | 7/19 [00:02<00:03,  3.37it/s] 84/99 7.3G 0.0698 0.04216 0.017 124 640:  42%|████▏     | 8/19 [00:02<00:02,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.06922 0.04211 0.01625 75 640:  42%|████▏     | 8/19 [00:02<00:02,  3.88it/s]84/99 7.3G 0.06922 0.04211 0.01625 75 640:  47%|████▋     | 9/19 [00:02<00:02,  4.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.06913 0.04146 0.01611 53 640:  47%|████▋     | 9/19 [00:02<00:02,  4.32it/s]84/99 7.3G 0.06913 0.04146 0.01611 53 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0701 0.04525 0.01646 171 640:  53%|█████▎    | 10/19 [00:08<00:01,  4.61it/s]84/99 7.3G 0.0701 0.04525 0.01646 171 640:  58%|█████▊    | 11/19 [00:08<00:14,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07019 0.04622 0.01624 106 640:  58%|█████▊    | 11/19 [00:09<00:14,  1.86s/it]84/99 7.3G 0.07019 0.04622 0.01624 106 640:  63%|██████▎   | 12/19 [00:09<00:11,  1.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07024 0.04611 0.01635 86 640:  63%|██████▎   | 12/19 [00:14<00:11,  1.65s/it] 84/99 7.3G 0.07024 0.04611 0.01635 86 640:  68%|██████▊   | 13/19 [00:14<00:16,  2.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07023 0.0456 0.01636 71 640:  68%|██████▊   | 13/19 [00:16<00:16,  2.78s/it] 84/99 7.3G 0.07023 0.0456 0.01636 71 640:  74%|███████▎  | 14/19 [00:16<00:12,  2.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07036 0.04623 0.01636 97 640:  74%|███████▎  | 14/19 [00:18<00:12,  2.46s/it]84/99 7.3G 0.07036 0.04623 0.01636 97 640:  79%|███████▉  | 15/19 [00:18<00:09,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07007 0.04571 0.01624 66 640:  79%|███████▉  | 15/19 [00:21<00:09,  2.27s/it]84/99 7.3G 0.07007 0.04571 0.01624 66 640:  84%|████████▍ | 16/19 [00:21<00:07,  2.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07052 0.04602 0.01607 114 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.65s/it]84/99 7.3G 0.07052 0.04602 0.01607 114 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07054 0.04593 0.0161 80 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.07s/it]  84/99 7.3G 0.07054 0.04593 0.0161 80 640:  95%|█████████▍| 18/19 [00:24<00:02,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07063 0.04519 0.01621 52 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.00s/it]84/99 7.3G 0.07063 0.04519 0.01621 52 640: 100%|██████████| 19/19 [00:29<00:00,  2.90s/it]84/99 7.3G 0.07063 0.04519 0.01621 52 640: 100%|██████████| 19/19 [00:29<00:00,  1.55s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.68s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]
                   all         55        256      0.232      0.294      0.166     0.0578
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07741 0.05559 0.01887 105 640:   0%|          | 0/19 [00:00<?, ?it/s]85/99 7.3G 0.07741 0.05559 0.01887 105 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07563 0.06289 0.01862 137 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]85/99 7.3G 0.07563 0.06289 0.01862 137 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07613 0.06238 0.01769 129 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]85/99 7.3G 0.07613 0.06238 0.01769 129 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07503 0.05603 0.01714 63 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s] 85/99 7.3G 0.07503 0.05603 0.01714 63 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0735 0.05194 0.01629 61 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s] 85/99 7.3G 0.0735 0.05194 0.01629 61 640:  26%|██▋       | 5/19 [00:00<00:02,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07209 0.05062 0.01655 71 640:  26%|██▋       | 5/19 [00:01<00:02,  5.50it/s]85/99 7.3G 0.07209 0.05062 0.01655 71 640:  32%|███▏      | 6/19 [00:01<00:03,  4.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07184 0.04683 0.01706 37 640:  32%|███▏      | 6/19 [00:01<00:03,  4.33it/s]85/99 7.3G 0.07184 0.04683 0.01706 37 640:  37%|███▋      | 7/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0716 0.04685 0.01682 79 640:  37%|███▋      | 7/19 [00:01<00:02,  4.71it/s] 85/99 7.3G 0.0716 0.04685 0.01682 79 640:  42%|████▏     | 8/19 [00:01<00:02,  5.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0721 0.0484 0.01663 114 640:  42%|████▏     | 8/19 [00:01<00:02,  5.00it/s]85/99 7.3G 0.0721 0.0484 0.01663 114 640:  47%|████▋     | 9/19 [00:01<00:01,  5.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07256 0.05317 0.01645 190 640:  47%|████▋     | 9/19 [00:06<00:01,  5.19it/s]85/99 7.3G 0.07256 0.05317 0.01645 190 640:  53%|█████▎    | 10/19 [00:06<00:13,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07183 0.05191 0.01634 68 640:  53%|█████▎    | 10/19 [00:09<00:13,  1.49s/it] 85/99 7.3G 0.07183 0.05191 0.01634 68 640:  58%|█████▊    | 11/19 [00:09<00:15,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07183 0.05028 0.01676 56 640:  58%|█████▊    | 11/19 [00:11<00:15,  1.98s/it]85/99 7.3G 0.07183 0.05028 0.01676 56 640:  63%|██████▎   | 12/19 [00:11<00:14,  2.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07125 0.04868 0.01629 52 640:  63%|██████▎   | 12/19 [00:15<00:14,  2.08s/it]85/99 7.3G 0.07125 0.04868 0.01629 52 640:  68%|██████▊   | 13/19 [00:15<00:15,  2.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07097 0.0481 0.01636 63 640:  68%|██████▊   | 13/19 [00:15<00:15,  2.52s/it] 85/99 7.3G 0.07097 0.0481 0.01636 63 640:  74%|███████▎  | 14/19 [00:15<00:09,  1.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07102 0.04882 0.01644 106 640:  74%|███████▎  | 14/19 [00:19<00:09,  1.90s/it]85/99 7.3G 0.07102 0.04882 0.01644 106 640:  79%|███████▉  | 15/19 [00:19<00:10,  2.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07112 0.04839 0.01633 82 640:  79%|███████▉  | 15/19 [00:22<00:10,  2.52s/it] 85/99 7.3G 0.07112 0.04839 0.01633 82 640:  84%|████████▍ | 16/19 [00:22<00:08,  2.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07153 0.04957 0.0163 122 640:  84%|████████▍ | 16/19 [00:23<00:08,  2.73s/it]85/99 7.3G 0.07153 0.04957 0.0163 122 640:  89%|████████▉ | 17/19 [00:23<00:04,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07156 0.04844 0.01629 56 640:  89%|████████▉ | 17/19 [00:28<00:04,  2.10s/it]85/99 7.3G 0.07156 0.04844 0.01629 56 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07121 0.04839 0.01613 85 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.90s/it]85/99 7.3G 0.07121 0.04839 0.01613 85 640: 100%|██████████| 19/19 [00:30<00:00,  2.88s/it]85/99 7.3G 0.07121 0.04839 0.01613 85 640: 100%|██████████| 19/19 [00:30<00:00,  1.63s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.43s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]
                   all         55        256      0.235      0.233      0.181     0.0626
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07568 0.03177 0.01542 64 640:   0%|          | 0/19 [00:00<?, ?it/s]86/99 7.3G 0.07568 0.03177 0.01542 64 640:   5%|▌         | 1/19 [00:00<00:06,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.0742 0.04021 0.01509 83 640:   5%|▌         | 1/19 [00:00<00:06,  2.66it/s] 86/99 7.3G 0.0742 0.04021 0.01509 83 640:  11%|█         | 2/19 [00:00<00:05,  3.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07133 0.0371 0.01452 56 640:  11%|█         | 2/19 [00:00<00:05,  3.13it/s]86/99 7.3G 0.07133 0.0371 0.01452 56 640:  16%|█▌        | 3/19 [00:00<00:04,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07069 0.03793 0.01444 75 640:  16%|█▌        | 3/19 [00:01<00:04,  3.86it/s]86/99 7.3G 0.07069 0.03793 0.01444 75 640:  21%|██        | 4/19 [00:01<00:03,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07043 0.03852 0.01555 73 640:  21%|██        | 4/19 [00:01<00:03,  4.27it/s]86/99 7.3G 0.07043 0.03852 0.01555 73 640:  26%|██▋       | 5/19 [00:01<00:03,  4.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07108 0.04168 0.0158 94 640:  26%|██▋       | 5/19 [00:01<00:03,  4.09it/s] 86/99 7.3G 0.07108 0.04168 0.0158 94 640:  32%|███▏      | 6/19 [00:01<00:04,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07116 0.04196 0.01639 81 640:  32%|███▏      | 6/19 [00:02<00:04,  3.11it/s]86/99 7.3G 0.07116 0.04196 0.01639 81 640:  37%|███▋      | 7/19 [00:02<00:04,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07142 0.04219 0.01599 73 640:  37%|███▋      | 7/19 [00:02<00:04,  2.98it/s]86/99 7.3G 0.07142 0.04219 0.01599 73 640:  42%|████▏     | 8/19 [00:02<00:03,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.0715 0.04471 0.01598 109 640:  42%|████▏     | 8/19 [00:04<00:03,  3.04it/s]86/99 7.3G 0.0715 0.04471 0.01598 109 640:  47%|████▋     | 9/19 [00:04<00:09,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07127 0.04445 0.01573 82 640:  47%|████▋     | 9/19 [00:06<00:09,  1.01it/s]86/99 7.3G 0.07127 0.04445 0.01573 82 640:  53%|█████▎    | 10/19 [00:06<00:09,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07156 0.04492 0.01596 90 640:  53%|█████▎    | 10/19 [00:06<00:09,  1.10s/it]86/99 7.3G 0.07156 0.04492 0.01596 90 640:  58%|█████▊    | 11/19 [00:06<00:06,  1.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07149 0.04438 0.01624 69 640:  58%|█████▊    | 11/19 [00:10<00:06,  1.16it/s]86/99 7.3G 0.07149 0.04438 0.01624 69 640:  63%|██████▎   | 12/19 [00:10<00:12,  1.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07136 0.04539 0.01618 105 640:  63%|██████▎   | 12/19 [00:15<00:12,  1.84s/it]86/99 7.3G 0.07136 0.04539 0.01618 105 640:  68%|██████▊   | 13/19 [00:15<00:16,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07183 0.04482 0.01619 70 640:  68%|██████▊   | 13/19 [00:15<00:16,  2.83s/it] 86/99 7.3G 0.07183 0.04482 0.01619 70 640:  74%|███████▎  | 14/19 [00:15<00:10,  2.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07155 0.04381 0.01607 51 640:  74%|███████▎  | 14/19 [00:20<00:10,  2.04s/it]86/99 7.3G 0.07155 0.04381 0.01607 51 640:  79%|███████▉  | 15/19 [00:20<00:11,  2.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.0713 0.04368 0.01626 71 640:  79%|███████▉  | 15/19 [00:23<00:11,  2.91s/it] 86/99 7.3G 0.0713 0.04368 0.01626 71 640:  84%|████████▍ | 16/19 [00:23<00:08,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07217 0.04412 0.01623 140 640:  84%|████████▍ | 16/19 [00:28<00:08,  2.96s/it]86/99 7.3G 0.07217 0.04412 0.01623 140 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07225 0.04399 0.01637 78 640:  89%|████████▉ | 17/19 [00:28<00:06,  3.40s/it] 86/99 7.3G 0.07225 0.04399 0.01637 78 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07186 0.04344 0.01609 68 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.46s/it]86/99 7.3G 0.07186 0.04344 0.01609 68 640: 100%|██████████| 19/19 [00:28<00:00,  1.77s/it]86/99 7.3G 0.07186 0.04344 0.01609 68 640: 100%|██████████| 19/19 [00:28<00:00,  1.52s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.28s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.71s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.69s/it]
                   all         55        256      0.235      0.265      0.182     0.0589
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.06901 0.02641 0.01227 41 640:   0%|          | 0/19 [00:00<?, ?it/s]87/99 7.3G 0.06901 0.02641 0.01227 41 640:   5%|▌         | 1/19 [00:00<00:14,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.06985 0.03414 0.0113 85 640:   5%|▌         | 1/19 [00:01<00:14,  1.22it/s] 87/99 7.3G 0.06985 0.03414 0.0113 85 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.0695 0.04134 0.01274 93 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]87/99 7.3G 0.0695 0.04134 0.01274 93 640:  16%|█▌        | 3/19 [00:01<00:06,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07223 0.04376 0.01416 111 640:  16%|█▌        | 3/19 [00:01<00:06,  2.59it/s]87/99 7.3G 0.07223 0.04376 0.01416 111 640:  21%|██        | 4/19 [00:01<00:04,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.073 0.04449 0.01473 76 640:  21%|██        | 4/19 [00:01<00:04,  3.15it/s]   87/99 7.3G 0.073 0.04449 0.01473 76 640:  26%|██▋       | 5/19 [00:01<00:04,  3.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07236 0.04343 0.01458 70 640:  26%|██▋       | 5/19 [00:02<00:04,  3.49it/s]87/99 7.3G 0.07236 0.04343 0.01458 70 640:  32%|███▏      | 6/19 [00:02<00:04,  3.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07323 0.04525 0.01488 101 640:  32%|███▏      | 6/19 [00:02<00:04,  3.05it/s]87/99 7.3G 0.07323 0.04525 0.01488 101 640:  37%|███▋      | 7/19 [00:02<00:04,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07298 0.04437 0.01535 69 640:  37%|███▋      | 7/19 [00:03<00:04,  2.83it/s] 87/99 7.3G 0.07298 0.04437 0.01535 69 640:  42%|████▏     | 8/19 [00:03<00:04,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07199 0.04284 0.01524 47 640:  42%|████▏     | 8/19 [00:04<00:04,  2.30it/s]87/99 7.3G 0.07199 0.04284 0.01524 47 640:  47%|████▋     | 9/19 [00:04<00:06,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07128 0.04129 0.01528 41 640:  47%|████▋     | 9/19 [00:09<00:06,  1.46it/s]87/99 7.3G 0.07128 0.04129 0.01528 41 640:  53%|█████▎    | 10/19 [00:09<00:17,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07215 0.04116 0.01542 94 640:  53%|█████▎    | 10/19 [00:10<00:17,  1.96s/it]87/99 7.3G 0.07215 0.04116 0.01542 94 640:  58%|█████▊    | 11/19 [00:10<00:12,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07224 0.04177 0.01551 89 640:  58%|█████▊    | 11/19 [00:14<00:12,  1.61s/it]87/99 7.3G 0.07224 0.04177 0.01551 89 640:  63%|██████▎   | 12/19 [00:14<00:16,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07149 0.04022 0.01545 32 640:  63%|██████▎   | 12/19 [00:16<00:16,  2.33s/it]87/99 7.3G 0.07149 0.04022 0.01545 32 640:  68%|██████▊   | 13/19 [00:16<00:14,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07153 0.03968 0.01555 63 640:  68%|██████▊   | 13/19 [00:21<00:14,  2.37s/it]87/99 7.3G 0.07153 0.03968 0.01555 63 640:  74%|███████▎  | 14/19 [00:21<00:15,  3.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.0712 0.03899 0.01567 50 640:  74%|███████▎  | 14/19 [00:21<00:15,  3.02s/it] 87/99 7.3G 0.0712 0.03899 0.01567 50 640:  79%|███████▉  | 15/19 [00:21<00:09,  2.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07137 0.03899 0.01564 66 640:  79%|███████▉  | 15/19 [00:22<00:09,  2.30s/it]87/99 7.3G 0.07137 0.03899 0.01564 66 640:  84%|████████▍ | 16/19 [00:22<00:05,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07117 0.03924 0.01574 71 640:  84%|████████▍ | 16/19 [00:26<00:05,  1.76s/it]87/99 7.3G 0.07117 0.03924 0.01574 71 640:  89%|████████▉ | 17/19 [00:26<00:04,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07111 0.03992 0.01587 97 640:  89%|████████▉ | 17/19 [00:30<00:04,  2.37s/it]87/99 7.3G 0.07111 0.03992 0.01587 97 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07122 0.04015 0.01591 77 640:  95%|█████████▍| 18/19 [00:32<00:03,  3.16s/it]87/99 7.3G 0.07122 0.04015 0.01591 77 640: 100%|██████████| 19/19 [00:32<00:00,  2.52s/it]87/99 7.3G 0.07122 0.04015 0.01591 77 640: 100%|██████████| 19/19 [00:32<00:00,  1.68s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  5.98s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:14<00:00,  7.17s/it]
                   all         55        256      0.203      0.267      0.158     0.0568
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.0733 0.0645 0.01439 116 640:   0%|          | 0/19 [00:00<?, ?it/s]88/99 7.3G 0.0733 0.0645 0.01439 116 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07709 0.05154 0.01655 85 640:   5%|▌         | 1/19 [00:01<00:09,  1.82it/s]88/99 7.3G 0.07709 0.05154 0.01655 85 640:  11%|█         | 2/19 [00:01<00:09,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07707 0.04641 0.01534 76 640:  11%|█         | 2/19 [00:01<00:09,  1.81it/s]88/99 7.3G 0.07707 0.04641 0.01534 76 640:  16%|█▌        | 3/19 [00:01<00:09,  1.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07473 0.04265 0.01471 52 640:  16%|█▌        | 3/19 [00:02<00:09,  1.72it/s]88/99 7.3G 0.07473 0.04265 0.01471 52 640:  21%|██        | 4/19 [00:02<00:07,  1.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07343 0.04235 0.01464 84 640:  21%|██        | 4/19 [00:02<00:07,  1.88it/s]88/99 7.3G 0.07343 0.04235 0.01464 84 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07295 0.04147 0.01512 60 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s]88/99 7.3G 0.07295 0.04147 0.01512 60 640:  32%|███▏      | 6/19 [00:02<00:04,  2.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07236 0.04216 0.01498 84 640:  32%|███▏      | 6/19 [00:02<00:04,  2.91it/s]88/99 7.3G 0.07236 0.04216 0.01498 84 640:  37%|███▋      | 7/19 [00:02<00:03,  3.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07155 0.0403 0.0149 47 640:  37%|███▋      | 7/19 [00:03<00:03,  3.28it/s]  88/99 7.3G 0.07155 0.0403 0.0149 47 640:  42%|████▏     | 8/19 [00:03<00:03,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07082 0.03978 0.01502 62 640:  42%|████▏     | 8/19 [00:06<00:03,  3.65it/s]88/99 7.3G 0.07082 0.03978 0.01502 62 640:  47%|████▋     | 9/19 [00:06<00:11,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07109 0.04089 0.01573 87 640:  47%|████▋     | 9/19 [00:07<00:11,  1.14s/it]88/99 7.3G 0.07109 0.04089 0.01573 87 640:  53%|█████▎    | 10/19 [00:07<00:10,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07137 0.04345 0.01559 122 640:  53%|█████▎    | 10/19 [00:11<00:10,  1.21s/it]88/99 7.3G 0.07137 0.04345 0.01559 122 640:  58%|█████▊    | 11/19 [00:11<00:17,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.0717 0.04441 0.0157 98 640:  58%|█████▊    | 11/19 [00:12<00:17,  2.15s/it]   88/99 7.3G 0.0717 0.04441 0.0157 98 640:  63%|██████▎   | 12/19 [00:12<00:11,  1.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07158 0.04464 0.01559 89 640:  63%|██████▎   | 12/19 [00:14<00:11,  1.65s/it]88/99 7.3G 0.07158 0.04464 0.01559 89 640:  68%|██████▊   | 13/19 [00:14<00:11,  1.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07125 0.04453 0.0155 69 640:  68%|██████▊   | 13/19 [00:17<00:11,  1.87s/it] 88/99 7.3G 0.07125 0.04453 0.0155 69 640:  74%|███████▎  | 14/19 [00:17<00:10,  2.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07119 0.04393 0.01544 67 640:  74%|███████▎  | 14/19 [00:20<00:10,  2.05s/it]88/99 7.3G 0.07119 0.04393 0.01544 67 640:  79%|███████▉  | 15/19 [00:20<00:10,  2.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07149 0.04447 0.01563 101 640:  79%|███████▉  | 15/19 [00:22<00:10,  2.57s/it]88/99 7.3G 0.07149 0.04447 0.01563 101 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07143 0.04462 0.01574 75 640:  84%|████████▍ | 16/19 [00:27<00:07,  2.43s/it] 88/99 7.3G 0.07143 0.04462 0.01574 75 640:  89%|████████▉ | 17/19 [00:27<00:06,  3.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07124 0.04453 0.01597 70 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.06s/it]88/99 7.3G 0.07124 0.04453 0.01597 70 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07112 0.04344 0.01591 42 640:  95%|█████████▍| 18/19 [00:33<00:02,  2.83s/it]88/99 7.3G 0.07112 0.04344 0.01591 42 640: 100%|██████████| 19/19 [00:33<00:00,  3.24s/it]88/99 7.3G 0.07112 0.04344 0.01591 42 640: 100%|██████████| 19/19 [00:33<00:00,  1.79s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.62s/it]
                   all         55        256      0.189      0.248      0.164     0.0579
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.0732 0.04473 0.01675 68 640:   0%|          | 0/19 [00:00<?, ?it/s]89/99 7.3G 0.0732 0.04473 0.01675 68 640:   5%|▌         | 1/19 [00:00<00:03,  4.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07114 0.04016 0.01655 63 640:   5%|▌         | 1/19 [00:00<00:03,  4.90it/s]89/99 7.3G 0.07114 0.04016 0.01655 63 640:  11%|█         | 2/19 [00:00<00:03,  5.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.06937 0.04334 0.0157 91 640:  11%|█         | 2/19 [00:00<00:03,  5.30it/s] 89/99 7.3G 0.06937 0.04334 0.0157 91 640:  16%|█▌        | 3/19 [00:00<00:03,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07144 0.04706 0.01607 112 640:  16%|█▌        | 3/19 [00:00<00:03,  4.22it/s]89/99 7.3G 0.07144 0.04706 0.01607 112 640:  21%|██        | 4/19 [00:00<00:03,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07249 0.04686 0.01652 98 640:  21%|██        | 4/19 [00:01<00:03,  3.97it/s] 89/99 7.3G 0.07249 0.04686 0.01652 98 640:  26%|██▋       | 5/19 [00:01<00:04,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07118 0.04522 0.01597 67 640:  26%|██▋       | 5/19 [00:01<00:04,  3.46it/s]89/99 7.3G 0.07118 0.04522 0.01597 67 640:  32%|███▏      | 6/19 [00:01<00:04,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07031 0.04588 0.01551 87 640:  32%|███▏      | 6/19 [00:02<00:04,  3.23it/s]89/99 7.3G 0.07031 0.04588 0.01551 87 640:  37%|███▋      | 7/19 [00:02<00:03,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07042 0.04538 0.0156 62 640:  37%|███▋      | 7/19 [00:02<00:03,  3.12it/s] 89/99 7.3G 0.07042 0.04538 0.0156 62 640:  42%|████▏     | 8/19 [00:02<00:03,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.06993 0.04379 0.01565 54 640:  42%|████▏     | 8/19 [00:02<00:03,  3.04it/s]89/99 7.3G 0.06993 0.04379 0.01565 54 640:  47%|████▋     | 9/19 [00:02<00:03,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.06973 0.04241 0.01569 53 640:  47%|████▋     | 9/19 [00:03<00:03,  2.60it/s]89/99 7.3G 0.06973 0.04241 0.01569 53 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07041 0.04356 0.01617 108 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.10it/s]89/99 7.3G 0.07041 0.04356 0.01617 108 640:  58%|█████▊    | 11/19 [00:05<00:07,  1.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07121 0.04456 0.01618 137 640:  58%|█████▊    | 11/19 [00:10<00:07,  1.12it/s]89/99 7.3G 0.07121 0.04456 0.01618 137 640:  63%|██████▎   | 12/19 [00:10<00:15,  2.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07149 0.04522 0.01635 100 640:  63%|██████▎   | 12/19 [00:11<00:15,  2.20s/it]89/99 7.3G 0.07149 0.04522 0.01635 100 640:  68%|██████▊   | 13/19 [00:11<00:11,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07154 0.0455 0.0168 74 640:  68%|██████▊   | 13/19 [00:16<00:11,  1.91s/it]   89/99 7.3G 0.07154 0.0455 0.0168 74 640:  74%|███████▎  | 14/19 [00:16<00:14,  2.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07174 0.04517 0.01663 66 640:  74%|███████▎  | 14/19 [00:18<00:14,  2.89s/it]89/99 7.3G 0.07174 0.04517 0.01663 66 640:  79%|███████▉  | 15/19 [00:18<00:09,  2.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07194 0.04495 0.01646 90 640:  79%|███████▉  | 15/19 [00:23<00:09,  2.41s/it]89/99 7.3G 0.07194 0.04495 0.01646 90 640:  84%|████████▍ | 16/19 [00:23<00:09,  3.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07159 0.04452 0.01641 66 640:  84%|████████▍ | 16/19 [00:23<00:09,  3.28s/it]89/99 7.3G 0.07159 0.04452 0.01641 66 640:  89%|████████▉ | 17/19 [00:23<00:04,  2.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07134 0.04475 0.01661 74 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.35s/it]89/99 7.3G 0.07134 0.04475 0.01661 74 640:  95%|█████████▍| 18/19 [00:27<00:02,  2.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07107 0.04389 0.01635 46 640:  95%|█████████▍| 18/19 [00:29<00:02,  2.66s/it]89/99 7.3G 0.07107 0.04389 0.01635 46 640: 100%|██████████| 19/19 [00:29<00:00,  2.46s/it]89/99 7.3G 0.07107 0.04389 0.01635 46 640: 100%|██████████| 19/19 [00:29<00:00,  1.53s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:10<00:10, 10.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.40s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.32s/it]
                   all         55        256      0.191      0.229      0.158     0.0559
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08148 0.05251 0.01875 117 640:   0%|          | 0/19 [00:00<?, ?it/s]90/99 7.3G 0.08148 0.05251 0.01875 117 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07942 0.07195 0.0201 149 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s] 90/99 7.3G 0.07942 0.07195 0.0201 149 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07735 0.06032 0.0191 62 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s] 90/99 7.3G 0.07735 0.06032 0.0191 62 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07804 0.06216 0.01787 173 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]90/99 7.3G 0.07804 0.06216 0.01787 173 640:  21%|██        | 4/19 [00:00<00:02,  5.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07632 0.05566 0.01676 50 640:  21%|██        | 4/19 [00:00<00:02,  5.41it/s] 90/99 7.3G 0.07632 0.05566 0.01676 50 640:  26%|██▋       | 5/19 [00:00<00:02,  5.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0756 0.05326 0.01724 67 640:  26%|██▋       | 5/19 [00:01<00:02,  5.15it/s] 90/99 7.3G 0.0756 0.05326 0.01724 67 640:  32%|███▏      | 6/19 [00:01<00:02,  4.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07514 0.05118 0.01731 68 640:  32%|███▏      | 6/19 [00:01<00:02,  4.89it/s]90/99 7.3G 0.07514 0.05118 0.01731 68 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07487 0.04935 0.01679 76 640:  37%|███▋      | 7/19 [00:01<00:03,  3.96it/s]90/99 7.3G 0.07487 0.04935 0.01679 76 640:  42%|████▏     | 8/19 [00:01<00:02,  4.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0739 0.04711 0.01633 50 640:  42%|████▏     | 8/19 [00:03<00:02,  4.21it/s] 90/99 7.3G 0.0739 0.04711 0.01633 50 640:  47%|████▋     | 9/19 [00:03<00:08,  1.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07377 0.04808 0.01616 105 640:  47%|████▋     | 9/19 [00:04<00:08,  1.23it/s]90/99 7.3G 0.07377 0.04808 0.01616 105 640:  53%|█████▎    | 10/19 [00:04<00:07,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07325 0.04746 0.01634 67 640:  53%|█████▎    | 10/19 [00:11<00:07,  1.22it/s] 90/99 7.3G 0.07325 0.04746 0.01634 67 640:  58%|█████▊    | 11/19 [00:11<00:21,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07304 0.04765 0.01671 90 640:  58%|█████▊    | 11/19 [00:12<00:21,  2.67s/it]90/99 7.3G 0.07304 0.04765 0.01671 90 640:  63%|██████▎   | 12/19 [00:12<00:14,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07225 0.0464 0.01633 50 640:  63%|██████▎   | 12/19 [00:16<00:14,  2.07s/it] 90/99 7.3G 0.07225 0.0464 0.01633 50 640:  68%|██████▊   | 13/19 [00:16<00:16,  2.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07196 0.04524 0.01666 46 640:  68%|██████▊   | 13/19 [00:17<00:16,  2.67s/it]90/99 7.3G 0.07196 0.04524 0.01666 46 640:  74%|███████▎  | 14/19 [00:17<00:10,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07188 0.04483 0.01664 68 640:  74%|███████▎  | 14/19 [00:19<00:10,  2.15s/it]90/99 7.3G 0.07188 0.04483 0.01664 68 640:  79%|███████▉  | 15/19 [00:19<00:08,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07184 0.04425 0.01651 60 640:  79%|███████▉  | 15/19 [00:20<00:08,  2.07s/it]90/99 7.3G 0.07184 0.04425 0.01651 60 640:  84%|████████▍ | 16/19 [00:20<00:05,  1.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07125 0.04368 0.01664 58 640:  84%|████████▍ | 16/19 [00:24<00:05,  1.88s/it]90/99 7.3G 0.07125 0.04368 0.01664 58 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07138 0.04376 0.01682 85 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.38s/it]90/99 7.3G 0.07138 0.04376 0.01682 85 640:  95%|█████████▍| 18/19 [00:24<00:01,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07137 0.04385 0.0169 86 640:  95%|█████████▍| 18/19 [00:33<00:01,  1.75s/it] 90/99 7.3G 0.07137 0.04385 0.0169 86 640: 100%|██████████| 19/19 [00:33<00:00,  3.96s/it]90/99 7.3G 0.07137 0.04385 0.0169 86 640: 100%|██████████| 19/19 [00:33<00:00,  1.76s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:15<00:15, 15.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.55s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.90s/it]
                   all         55        256      0.196      0.231      0.167     0.0593
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07114 0.04882 0.01593 83 640:   0%|          | 0/19 [00:00<?, ?it/s]91/99 7.3G 0.07114 0.04882 0.01593 83 640:   5%|▌         | 1/19 [00:00<00:06,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06944 0.04259 0.01514 59 640:   5%|▌         | 1/19 [00:00<00:06,  2.72it/s]91/99 7.3G 0.06944 0.04259 0.01514 59 640:  11%|█         | 2/19 [00:00<00:08,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06849 0.04354 0.0152 71 640:  11%|█         | 2/19 [00:01<00:08,  1.98it/s] 91/99 7.3G 0.06849 0.04354 0.0152 71 640:  16%|█▌        | 3/19 [00:01<00:08,  1.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06799 0.03962 0.0163 47 640:  16%|█▌        | 3/19 [00:01<00:08,  1.96it/s]91/99 7.3G 0.06799 0.03962 0.0163 47 640:  21%|██        | 4/19 [00:01<00:07,  2.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06906 0.03957 0.01778 62 640:  21%|██        | 4/19 [00:02<00:07,  2.09it/s]91/99 7.3G 0.06906 0.03957 0.01778 62 640:  26%|██▋       | 5/19 [00:02<00:06,  2.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06838 0.03792 0.0172 51 640:  26%|██▋       | 5/19 [00:02<00:06,  2.31it/s] 91/99 7.3G 0.06838 0.03792 0.0172 51 640:  32%|███▏      | 6/19 [00:02<00:05,  2.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06805 0.03845 0.01684 73 640:  32%|███▏      | 6/19 [00:02<00:05,  2.48it/s]91/99 7.3G 0.06805 0.03845 0.01684 73 640:  37%|███▋      | 7/19 [00:02<00:04,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06852 0.03958 0.01731 81 640:  37%|███▋      | 7/19 [00:03<00:04,  2.60it/s]91/99 7.3G 0.06852 0.03958 0.01731 81 640:  42%|████▏     | 8/19 [00:03<00:04,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06894 0.03849 0.01781 48 640:  42%|████▏     | 8/19 [00:03<00:04,  2.69it/s]91/99 7.3G 0.06894 0.03849 0.01781 48 640:  47%|████▋     | 9/19 [00:03<00:03,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0693 0.03908 0.01776 77 640:  47%|████▋     | 9/19 [00:05<00:03,  2.74it/s] 91/99 7.3G 0.0693 0.03908 0.01776 77 640:  53%|█████▎    | 10/19 [00:05<00:07,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07 0.04051 0.0177 91 640:  53%|█████▎    | 10/19 [00:05<00:07,  1.22it/s]   91/99 7.3G 0.07 0.04051 0.0177 91 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06991 0.04116 0.01772 76 640:  58%|█████▊    | 11/19 [00:09<00:04,  1.61it/s]91/99 7.3G 0.06991 0.04116 0.01772 76 640:  63%|██████▎   | 12/19 [00:09<00:10,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0698 0.04084 0.01769 60 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.49s/it] 91/99 7.3G 0.0698 0.04084 0.01769 60 640:  68%|██████▊   | 13/19 [00:10<00:08,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.06984 0.04051 0.018 65 640:  68%|██████▊   | 13/19 [00:12<00:08,  1.39s/it] 91/99 7.3G 0.06984 0.04051 0.018 65 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07011 0.03978 0.01776 55 640:  74%|███████▎  | 14/19 [00:13<00:08,  1.70s/it]91/99 7.3G 0.07011 0.03978 0.01776 55 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07076 0.04063 0.01767 114 640:  79%|███████▉  | 15/19 [00:23<00:05,  1.34s/it]91/99 7.3G 0.07076 0.04063 0.01767 114 640:  84%|████████▍ | 16/19 [00:23<00:12,  4.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07107 0.04114 0.01749 101 640:  84%|████████▍ | 16/19 [00:24<00:12,  4.11s/it]91/99 7.3G 0.07107 0.04114 0.01749 101 640:  89%|████████▉ | 17/19 [00:24<00:06,  3.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07061 0.0402 0.01744 37 640:  89%|████████▉ | 17/19 [00:29<00:06,  3.01s/it]  91/99 7.3G 0.07061 0.0402 0.01744 37 640:  95%|█████████▍| 18/19 [00:29<00:03,  3.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07093 0.04016 0.01745 79 640:  95%|█████████▍| 18/19 [00:30<00:03,  3.70s/it]91/99 7.3G 0.07093 0.04016 0.01745 79 640: 100%|██████████| 19/19 [00:30<00:00,  2.77s/it]91/99 7.3G 0.07093 0.04016 0.01745 79 640: 100%|██████████| 19/19 [00:30<00:00,  1.59s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.72s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.51s/it]
                   all         55        256      0.201      0.229      0.167     0.0567
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06892 0.03085 0.01364 50 640:   0%|          | 0/19 [00:00<?, ?it/s]92/99 7.3G 0.06892 0.03085 0.01364 50 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07448 0.04169 0.0168 88 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s] 92/99 7.3G 0.07448 0.04169 0.0168 88 640:  11%|█         | 2/19 [00:00<00:05,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07366 0.03965 0.01713 59 640:  11%|█         | 2/19 [00:00<00:05,  3.19it/s]92/99 7.3G 0.07366 0.03965 0.01713 59 640:  16%|█▌        | 3/19 [00:00<00:04,  3.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07394 0.04838 0.01889 120 640:  16%|█▌        | 3/19 [00:01<00:04,  3.67it/s]92/99 7.3G 0.07394 0.04838 0.01889 120 640:  21%|██        | 4/19 [00:01<00:03,  3.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07382 0.04565 0.01979 64 640:  21%|██        | 4/19 [00:01<00:03,  3.77it/s] 92/99 7.3G 0.07382 0.04565 0.01979 64 640:  26%|██▋       | 5/19 [00:01<00:04,  3.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07361 0.04906 0.01956 118 640:  26%|██▋       | 5/19 [00:01<00:04,  3.31it/s]92/99 7.3G 0.07361 0.04906 0.01956 118 640:  32%|███▏      | 6/19 [00:01<00:04,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07379 0.04929 0.02001 76 640:  32%|███▏      | 6/19 [00:02<00:04,  3.09it/s] 92/99 7.3G 0.07379 0.04929 0.02001 76 640:  37%|███▋      | 7/19 [00:02<00:04,  2.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07329 0.0473 0.01951 60 640:  37%|███▋      | 7/19 [00:02<00:04,  2.99it/s] 92/99 7.3G 0.07329 0.0473 0.01951 60 640:  42%|████▏     | 8/19 [00:02<00:03,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07213 0.04523 0.01863 47 640:  42%|████▏     | 8/19 [00:02<00:03,  2.92it/s]92/99 7.3G 0.07213 0.04523 0.01863 47 640:  47%|████▋     | 9/19 [00:02<00:03,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07081 0.04352 0.01814 42 640:  47%|████▋     | 9/19 [00:03<00:03,  3.29it/s]92/99 7.3G 0.07081 0.04352 0.01814 42 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07103 0.04312 0.01788 81 640:  53%|█████▎    | 10/19 [00:06<00:03,  2.73it/s]92/99 7.3G 0.07103 0.04312 0.01788 81 640:  58%|█████▊    | 11/19 [00:06<00:10,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.0703 0.04247 0.01741 56 640:  58%|█████▊    | 11/19 [00:07<00:10,  1.27s/it] 92/99 7.3G 0.0703 0.04247 0.01741 56 640:  63%|██████▎   | 12/19 [00:07<00:07,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06997 0.04271 0.01749 76 640:  63%|██████▎   | 12/19 [00:15<00:07,  1.11s/it]92/99 7.3G 0.06997 0.04271 0.01749 76 640:  68%|██████▊   | 13/19 [00:15<00:19,  3.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.0699 0.04198 0.01799 54 640:  68%|██████▊   | 13/19 [00:16<00:19,  3.32s/it] 92/99 7.3G 0.0699 0.04198 0.01799 54 640:  74%|███████▎  | 14/19 [00:16<00:12,  2.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06943 0.0415 0.01779 51 640:  74%|███████▎  | 14/19 [00:19<00:12,  2.47s/it]92/99 7.3G 0.06943 0.0415 0.01779 51 640:  79%|███████▉  | 15/19 [00:19<00:11,  2.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06894 0.04058 0.01745 44 640:  79%|███████▉  | 15/19 [00:20<00:11,  2.78s/it]92/99 7.3G 0.06894 0.04058 0.01745 44 640:  84%|████████▍ | 16/19 [00:20<00:06,  2.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.0686 0.04012 0.01729 52 640:  84%|████████▍ | 16/19 [00:24<00:06,  2.11s/it] 92/99 7.3G 0.0686 0.04012 0.01729 52 640:  89%|████████▉ | 17/19 [00:24<00:05,  2.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06871 0.04078 0.01752 91 640:  89%|████████▉ | 17/19 [00:24<00:05,  2.70s/it]92/99 7.3G 0.06871 0.04078 0.01752 91 640:  95%|█████████▍| 18/19 [00:24<00:01,  1.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.06867 0.04104 0.01756 81 640:  95%|█████████▍| 18/19 [00:28<00:01,  1.96s/it]92/99 7.3G 0.06867 0.04104 0.01756 81 640: 100%|██████████| 19/19 [00:28<00:00,  2.46s/it]92/99 7.3G 0.06867 0.04104 0.01756 81 640: 100%|██████████| 19/19 [00:28<00:00,  1.49s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.84s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it]
                   all         55        256      0.202      0.223      0.163     0.0581
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06712 0.03744 0.01901 59 640:   0%|          | 0/19 [00:00<?, ?it/s]93/99 7.3G 0.06712 0.03744 0.01901 59 640:   5%|▌         | 1/19 [00:00<00:06,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07072 0.03805 0.01677 74 640:   5%|▌         | 1/19 [00:00<00:06,  2.84it/s]93/99 7.3G 0.07072 0.03805 0.01677 74 640:  11%|█         | 2/19 [00:00<00:05,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07026 0.03944 0.01535 79 640:  11%|█         | 2/19 [00:00<00:05,  2.86it/s]93/99 7.3G 0.07026 0.03944 0.01535 79 640:  16%|█▌        | 3/19 [00:00<00:04,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06812 0.03759 0.0156 49 640:  16%|█▌        | 3/19 [00:01<00:04,  3.29it/s] 93/99 7.3G 0.06812 0.03759 0.0156 49 640:  21%|██        | 4/19 [00:01<00:03,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06985 0.03896 0.01521 90 640:  21%|██        | 4/19 [00:01<00:03,  3.96it/s]93/99 7.3G 0.06985 0.03896 0.01521 90 640:  26%|██▋       | 5/19 [00:01<00:03,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07002 0.04048 0.01495 90 640:  26%|██▋       | 5/19 [00:01<00:03,  4.40it/s]93/99 7.3G 0.07002 0.04048 0.01495 90 640:  32%|███▏      | 6/19 [00:01<00:02,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07069 0.04526 0.01493 158 640:  32%|███▏      | 6/19 [00:01<00:02,  4.69it/s]93/99 7.3G 0.07069 0.04526 0.01493 158 640:  37%|███▋      | 7/19 [00:01<00:03,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.0704 0.04411 0.01486 70 640:  37%|███▋      | 7/19 [00:02<00:03,  3.92it/s]  93/99 7.3G 0.0704 0.04411 0.01486 70 640:  42%|████▏     | 8/19 [00:02<00:03,  3.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06996 0.04257 0.01521 53 640:  42%|████▏     | 8/19 [00:02<00:03,  3.36it/s]93/99 7.3G 0.06996 0.04257 0.01521 53 640:  47%|████▋     | 9/19 [00:02<00:03,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06999 0.04392 0.01497 102 640:  47%|████▋     | 9/19 [00:08<00:03,  2.79it/s]93/99 7.3G 0.06999 0.04392 0.01497 102 640:  53%|█████▎    | 10/19 [00:08<00:18,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.06954 0.04433 0.01508 80 640:  53%|█████▎    | 10/19 [00:09<00:18,  2.10s/it] 93/99 7.3G 0.06954 0.04433 0.01508 80 640:  58%|█████▊    | 11/19 [00:09<00:13,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07011 0.04522 0.0153 89 640:  58%|█████▊    | 11/19 [00:12<00:13,  1.64s/it] 93/99 7.3G 0.07011 0.04522 0.0153 89 640:  63%|██████▎   | 12/19 [00:12<00:14,  2.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07005 0.04484 0.01543 70 640:  63%|██████▎   | 12/19 [00:13<00:14,  2.06s/it]93/99 7.3G 0.07005 0.04484 0.01543 70 640:  68%|██████▊   | 13/19 [00:13<00:09,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07019 0.04483 0.01542 80 640:  68%|██████▊   | 13/19 [00:14<00:09,  1.64s/it]93/99 7.3G 0.07019 0.04483 0.01542 80 640:  74%|███████▎  | 14/19 [00:14<00:08,  1.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07087 0.04593 0.01558 153 640:  74%|███████▎  | 14/19 [00:15<00:08,  1.66s/it]93/99 7.3G 0.07087 0.04593 0.01558 153 640:  79%|███████▉  | 15/19 [00:15<00:05,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07098 0.04495 0.01545 57 640:  79%|███████▉  | 15/19 [00:18<00:05,  1.49s/it] 93/99 7.3G 0.07098 0.04495 0.01545 57 640:  84%|████████▍ | 16/19 [00:18<00:05,  1.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07098 0.04397 0.01518 55 640:  84%|████████▍ | 16/19 [00:18<00:05,  1.73s/it]93/99 7.3G 0.07098 0.04397 0.01518 55 640:  89%|████████▉ | 17/19 [00:18<00:02,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07058 0.043 0.01487 46 640:  89%|████████▉ | 17/19 [00:28<00:02,  1.37s/it]  93/99 7.3G 0.07058 0.043 0.01487 46 640:  95%|█████████▍| 18/19 [00:28<00:04,  4.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07043 0.04265 0.0151 64 640:  95%|█████████▍| 18/19 [00:28<00:04,  4.00s/it]93/99 7.3G 0.07043 0.04265 0.0151 64 640: 100%|██████████| 19/19 [00:28<00:00,  2.86s/it]93/99 7.3G 0.07043 0.04265 0.0151 64 640: 100%|██████████| 19/19 [00:28<00:00,  1.53s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:18<00:18, 18.32s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:20<00:00,  8.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:20<00:00, 10.10s/it]
                   all         55        256      0.218      0.223      0.171     0.0628
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06225 0.03361 0.01466 61 640:   0%|          | 0/19 [00:00<?, ?it/s]94/99 7.3G 0.06225 0.03361 0.01466 61 640:   5%|▌         | 1/19 [00:00<00:06,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06547 0.03795 0.01706 71 640:   5%|▌         | 1/19 [00:00<00:06,  2.57it/s]94/99 7.3G 0.06547 0.03795 0.01706 71 640:  11%|█         | 2/19 [00:00<00:07,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06582 0.03797 0.01624 67 640:  11%|█         | 2/19 [00:01<00:07,  2.18it/s]94/99 7.3G 0.06582 0.03797 0.01624 67 640:  16%|█▌        | 3/19 [00:01<00:06,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.0674 0.04079 0.01703 77 640:  16%|█▌        | 3/19 [00:01<00:06,  2.42it/s] 94/99 7.3G 0.0674 0.04079 0.01703 77 640:  21%|██        | 4/19 [00:01<00:06,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.0677 0.03961 0.01767 58 640:  21%|██        | 4/19 [00:02<00:06,  2.20it/s]94/99 7.3G 0.0677 0.03961 0.01767 58 640:  26%|██▋       | 5/19 [00:02<00:06,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06771 0.04244 0.01666 100 640:  26%|██▋       | 5/19 [00:02<00:06,  2.08it/s]94/99 7.3G 0.06771 0.04244 0.01666 100 640:  32%|███▏      | 6/19 [00:02<00:05,  2.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06876 0.04382 0.01709 91 640:  32%|███▏      | 6/19 [00:03<00:05,  2.27it/s] 94/99 7.3G 0.06876 0.04382 0.01709 91 640:  37%|███▋      | 7/19 [00:03<00:05,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06973 0.04611 0.01695 110 640:  37%|███▋      | 7/19 [00:03<00:05,  2.26it/s]94/99 7.3G 0.06973 0.04611 0.01695 110 640:  42%|████▏     | 8/19 [00:03<00:05,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07034 0.04802 0.01722 110 640:  42%|████▏     | 8/19 [00:04<00:05,  1.93it/s]94/99 7.3G 0.07034 0.04802 0.01722 110 640:  47%|████▋     | 9/19 [00:04<00:05,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07006 0.04669 0.01819 56 640:  47%|████▋     | 9/19 [00:05<00:05,  1.84it/s] 94/99 7.3G 0.07006 0.04669 0.01819 56 640:  53%|█████▎    | 10/19 [00:05<00:05,  1.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06983 0.04572 0.01807 51 640:  53%|█████▎    | 10/19 [00:05<00:05,  1.60it/s]94/99 7.3G 0.06983 0.04572 0.01807 51 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06947 0.04555 0.01771 72 640:  58%|█████▊    | 11/19 [00:06<00:04,  1.69it/s]94/99 7.3G 0.06947 0.04555 0.01771 72 640:  63%|██████▎   | 12/19 [00:06<00:04,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06956 0.04578 0.01755 82 640:  63%|██████▎   | 12/19 [00:08<00:04,  1.70it/s]94/99 7.3G 0.06956 0.04578 0.01755 82 640:  68%|██████▊   | 13/19 [00:08<00:06,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06949 0.04555 0.01732 68 640:  68%|██████▊   | 13/19 [00:09<00:06,  1.04s/it]94/99 7.3G 0.06949 0.04555 0.01732 68 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06964 0.04609 0.01727 93 640:  74%|███████▎  | 14/19 [00:17<00:04,  1.01it/s]94/99 7.3G 0.06964 0.04609 0.01727 93 640:  79%|███████▉  | 15/19 [00:17<00:12,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06933 0.0451 0.01773 46 640:  79%|███████▉  | 15/19 [00:17<00:12,  3.07s/it] 94/99 7.3G 0.06933 0.0451 0.01773 46 640:  84%|████████▍ | 16/19 [00:17<00:06,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.0695 0.0451 0.01789 74 640:  84%|████████▍ | 16/19 [00:22<00:06,  2.24s/it] 94/99 7.3G 0.0695 0.0451 0.01789 74 640:  89%|████████▉ | 17/19 [00:22<00:06,  3.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06984 0.04464 0.01786 81 640:  89%|████████▉ | 17/19 [00:23<00:06,  3.20s/it]94/99 7.3G 0.06984 0.04464 0.01786 81 640:  95%|█████████▍| 18/19 [00:23<00:02,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06966 0.0441 0.01777 54 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.33s/it] 94/99 7.3G 0.06966 0.0441 0.01777 54 640: 100%|██████████| 19/19 [00:26<00:00,  2.67s/it]94/99 7.3G 0.06966 0.0441 0.01777 54 640: 100%|██████████| 19/19 [00:26<00:00,  1.40s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.24s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.33s/it]
                   all         55        256      0.219      0.226      0.166      0.057
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06157 0.0372 0.01456 64 640:   0%|          | 0/19 [00:00<?, ?it/s]95/99 7.3G 0.06157 0.0372 0.01456 64 640:   5%|▌         | 1/19 [00:00<00:06,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06473 0.03101 0.01595 41 640:   5%|▌         | 1/19 [00:00<00:06,  2.84it/s]95/99 7.3G 0.06473 0.03101 0.01595 41 640:  11%|█         | 2/19 [00:00<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06879 0.03316 0.01731 79 640:  11%|█         | 2/19 [00:01<00:05,  2.85it/s]95/99 7.3G 0.06879 0.03316 0.01731 79 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06906 0.03414 0.01657 63 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]95/99 7.3G 0.06906 0.03414 0.01657 63 640:  21%|██        | 4/19 [00:01<00:04,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07002 0.03652 0.01635 87 640:  21%|██        | 4/19 [00:01<00:04,  3.12it/s]95/99 7.3G 0.07002 0.03652 0.01635 87 640:  26%|██▋       | 5/19 [00:01<00:04,  3.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06973 0.03728 0.01592 70 640:  26%|██▋       | 5/19 [00:02<00:04,  3.06it/s]95/99 7.3G 0.06973 0.03728 0.01592 70 640:  32%|███▏      | 6/19 [00:02<00:04,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.06959 0.03648 0.01599 50 640:  32%|███▏      | 6/19 [00:02<00:04,  2.79it/s]95/99 7.3G 0.06959 0.03648 0.01599 50 640:  37%|███▋      | 7/19 [00:02<00:04,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07054 0.03912 0.01592 109 640:  37%|███▋      | 7/19 [00:02<00:04,  2.88it/s]95/99 7.3G 0.07054 0.03912 0.01592 109 640:  42%|████▏     | 8/19 [00:02<00:03,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07057 0.04074 0.01666 88 640:  42%|████▏     | 8/19 [00:03<00:03,  2.86it/s] 95/99 7.3G 0.07057 0.04074 0.01666 88 640:  47%|████▋     | 9/19 [00:03<00:04,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07097 0.04017 0.01673 59 640:  47%|████▋     | 9/19 [00:05<00:04,  2.11it/s]95/99 7.3G 0.07097 0.04017 0.01673 59 640:  53%|█████▎    | 10/19 [00:05<00:08,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07127 0.04191 0.01674 115 640:  53%|█████▎    | 10/19 [00:06<00:08,  1.08it/s]95/99 7.3G 0.07127 0.04191 0.01674 115 640:  58%|█████▊    | 11/19 [00:06<00:07,  1.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07103 0.04209 0.01707 77 640:  58%|█████▊    | 11/19 [00:14<00:07,  1.09it/s] 95/99 7.3G 0.07103 0.04209 0.01707 77 640:  63%|██████▎   | 12/19 [00:14<00:20,  2.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07076 0.0409 0.01678 50 640:  63%|██████▎   | 12/19 [00:14<00:20,  2.99s/it] 95/99 7.3G 0.07076 0.0409 0.01678 50 640:  68%|██████▊   | 13/19 [00:15<00:14,  2.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07105 0.04292 0.01673 133 640:  68%|██████▊   | 13/19 [00:17<00:14,  2.36s/it]95/99 7.3G 0.07105 0.04292 0.01673 133 640:  74%|███████▎  | 14/19 [00:17<00:12,  2.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07042 0.04258 0.0165 58 640:  74%|███████▎  | 14/19 [00:18<00:12,  2.51s/it]  95/99 7.3G 0.07042 0.04258 0.0165 58 640:  79%|███████▉  | 15/19 [00:18<00:08,  2.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07057 0.04255 0.01693 65 640:  79%|███████▉  | 15/19 [00:20<00:08,  2.02s/it]95/99 7.3G 0.07057 0.04255 0.01693 65 640:  84%|████████▍ | 16/19 [00:20<00:05,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07152 0.04374 0.01696 159 640:  84%|████████▍ | 16/19 [00:23<00:05,  1.91s/it]95/99 7.3G 0.07152 0.04374 0.01696 159 640:  89%|████████▉ | 17/19 [00:23<00:04,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.0714 0.04388 0.01688 92 640:  89%|████████▉ | 17/19 [00:24<00:04,  2.17s/it]  95/99 7.3G 0.0714 0.04388 0.01688 92 640:  95%|█████████▍| 18/19 [00:24<00:01,  1.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07114 0.04399 0.01669 74 640:  95%|█████████▍| 18/19 [00:25<00:01,  1.80s/it]95/99 7.3G 0.07114 0.04399 0.01669 74 640: 100%|██████████| 19/19 [00:25<00:00,  1.66s/it]95/99 7.3G 0.07114 0.04399 0.01669 74 640: 100%|██████████| 19/19 [00:25<00:00,  1.34s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:15<00:15, 15.71s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.62s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.98s/it]
                   all         55        256      0.201      0.223      0.166     0.0587
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06638 0.03462 0.01661 54 640:   0%|          | 0/19 [00:00<?, ?it/s]96/99 7.3G 0.06638 0.03462 0.01661 54 640:   5%|▌         | 1/19 [00:00<00:12,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06562 0.03475 0.01535 58 640:   5%|▌         | 1/19 [00:01<00:12,  1.46it/s]96/99 7.3G 0.06562 0.03475 0.01535 58 640:  11%|█         | 2/19 [00:01<00:12,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06584 0.0375 0.01609 74 640:  11%|█         | 2/19 [00:01<00:12,  1.40it/s] 96/99 7.3G 0.06584 0.0375 0.01609 74 640:  16%|█▌        | 3/19 [00:01<00:09,  1.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06859 0.04224 0.01723 94 640:  16%|█▌        | 3/19 [00:02<00:09,  1.72it/s]96/99 7.3G 0.06859 0.04224 0.01723 94 640:  21%|██        | 4/19 [00:02<00:07,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06935 0.0433 0.01761 84 640:  21%|██        | 4/19 [00:02<00:07,  2.05it/s] 96/99 7.3G 0.06935 0.0433 0.01761 84 640:  26%|██▋       | 5/19 [00:02<00:07,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06853 0.04629 0.01751 93 640:  26%|██▋       | 5/19 [00:03<00:07,  1.99it/s]96/99 7.3G 0.06853 0.04629 0.01751 93 640:  32%|███▏      | 6/19 [00:03<00:05,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06887 0.04777 0.01713 105 640:  32%|███▏      | 6/19 [00:03<00:05,  2.21it/s]96/99 7.3G 0.06887 0.04777 0.01713 105 640:  37%|███▋      | 7/19 [00:03<00:05,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06866 0.04534 0.0167 44 640:  37%|███▋      | 7/19 [00:03<00:05,  2.38it/s]  96/99 7.3G 0.06866 0.04534 0.0167 44 640:  42%|████▏     | 8/19 [00:03<00:04,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06946 0.04771 0.01635 136 640:  42%|████▏     | 8/19 [00:09<00:04,  2.43it/s]96/99 7.3G 0.06946 0.04771 0.01635 136 640:  47%|████▋     | 9/19 [00:09<00:20,  2.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06859 0.04555 0.01657 42 640:  47%|████▋     | 9/19 [00:09<00:20,  2.05s/it] 96/99 7.3G 0.06859 0.04555 0.01657 42 640:  53%|█████▎    | 10/19 [00:09<00:13,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06843 0.04403 0.01639 47 640:  53%|█████▎    | 10/19 [00:13<00:13,  1.48s/it]96/99 7.3G 0.06843 0.04403 0.01639 47 640:  58%|█████▊    | 11/19 [00:13<00:17,  2.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06868 0.04443 0.01647 85 640:  58%|█████▊    | 11/19 [00:13<00:17,  2.20s/it]96/99 7.3G 0.06868 0.04443 0.01647 85 640:  63%|██████▎   | 12/19 [00:13<00:11,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06929 0.04618 0.01621 144 640:  63%|██████▎   | 12/19 [00:15<00:11,  1.67s/it]96/99 7.3G 0.06929 0.04618 0.01621 144 640:  68%|██████▊   | 13/19 [00:15<00:09,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06946 0.04544 0.01618 76 640:  68%|██████▊   | 13/19 [00:18<00:09,  1.56s/it] 96/99 7.3G 0.06946 0.04544 0.01618 76 640:  74%|███████▎  | 14/19 [00:18<00:11,  2.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.06986 0.04683 0.01648 123 640:  74%|███████▎  | 14/19 [00:19<00:11,  2.22s/it]96/99 7.3G 0.06986 0.04683 0.01648 123 640:  79%|███████▉  | 15/19 [00:19<00:06,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07062 0.04726 0.01663 149 640:  79%|███████▉  | 15/19 [00:20<00:06,  1.71s/it]96/99 7.3G 0.07062 0.04726 0.01663 149 640:  84%|████████▍ | 16/19 [00:20<00:04,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07054 0.04783 0.01658 98 640:  84%|████████▍ | 16/19 [00:30<00:04,  1.48s/it] 96/99 7.3G 0.07054 0.04783 0.01658 98 640:  89%|████████▉ | 17/19 [00:30<00:08,  4.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07072 0.04784 0.01639 85 640:  89%|████████▉ | 17/19 [00:30<00:08,  4.10s/it]96/99 7.3G 0.07072 0.04784 0.01639 85 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07084 0.04859 0.01639 117 640:  95%|█████████▍| 18/19 [00:36<00:02,  2.93s/it]96/99 7.3G 0.07084 0.04859 0.01639 117 640: 100%|██████████| 19/19 [00:36<00:00,  3.77s/it]96/99 7.3G 0.07084 0.04859 0.01639 117 640: 100%|██████████| 19/19 [00:36<00:00,  1.93s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:15<00:15, 15.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  6.92s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.16s/it]
                   all         55        256      0.209      0.221      0.173     0.0611
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07795 0.08294 0.01974 162 640:   0%|          | 0/19 [00:00<?, ?it/s]97/99 7.3G 0.07795 0.08294 0.01974 162 640:   5%|▌         | 1/19 [00:00<00:03,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07043 0.05723 0.01737 49 640:   5%|▌         | 1/19 [00:00<00:03,  4.71it/s] 97/99 7.3G 0.07043 0.05723 0.01737 49 640:  11%|█         | 2/19 [00:00<00:04,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07088 0.05365 0.01546 85 640:  11%|█         | 2/19 [00:00<00:04,  3.74it/s]97/99 7.3G 0.07088 0.05365 0.01546 85 640:  16%|█▌        | 3/19 [00:00<00:05,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07184 0.05405 0.01534 110 640:  16%|█▌        | 3/19 [00:01<00:05,  3.15it/s]97/99 7.3G 0.07184 0.05405 0.01534 110 640:  21%|██        | 4/19 [00:01<00:05,  2.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07118 0.05178 0.01524 74 640:  21%|██        | 4/19 [00:01<00:05,  2.97it/s] 97/99 7.3G 0.07118 0.05178 0.01524 74 640:  26%|██▋       | 5/19 [00:01<00:04,  2.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07129 0.05257 0.01504 112 640:  26%|██▋       | 5/19 [00:02<00:04,  2.89it/s]97/99 7.3G 0.07129 0.05257 0.01504 112 640:  32%|███▏      | 6/19 [00:02<00:04,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07221 0.04997 0.01551 62 640:  32%|███▏      | 6/19 [00:02<00:04,  2.80it/s] 97/99 7.3G 0.07221 0.04997 0.01551 62 640:  37%|███▋      | 7/19 [00:02<00:05,  2.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.0728 0.04751 0.01552 54 640:  37%|███▋      | 7/19 [00:03<00:05,  2.15it/s] 97/99 7.3G 0.0728 0.04751 0.01552 54 640:  42%|████▏     | 8/19 [00:03<00:05,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07333 0.04792 0.01606 124 640:  42%|████▏     | 8/19 [00:03<00:05,  2.14it/s]97/99 7.3G 0.07333 0.04792 0.01606 124 640:  47%|████▋     | 9/19 [00:03<00:04,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07339 0.04733 0.01607 78 640:  47%|████▋     | 9/19 [00:04<00:04,  2.05it/s] 97/99 7.3G 0.07339 0.04733 0.01607 78 640:  53%|█████▎    | 10/19 [00:04<00:05,  1.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07297 0.04752 0.01621 80 640:  53%|█████▎    | 10/19 [00:06<00:05,  1.74it/s]97/99 7.3G 0.07297 0.04752 0.01621 80 640:  58%|█████▊    | 11/19 [00:06<00:09,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07382 0.05098 0.01623 214 640:  58%|█████▊    | 11/19 [00:07<00:09,  1.16s/it]97/99 7.3G 0.07382 0.05098 0.01623 214 640:  63%|██████▎   | 12/19 [00:07<00:07,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07286 0.04933 0.01636 52 640:  63%|██████▎   | 12/19 [00:08<00:07,  1.05s/it] 97/99 7.3G 0.07286 0.04933 0.01636 52 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07223 0.0494 0.01638 77 640:  68%|██████▊   | 13/19 [00:15<00:05,  1.02it/s] 97/99 7.3G 0.07223 0.0494 0.01638 77 640:  74%|███████▎  | 14/19 [00:15<00:14,  2.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07256 0.04924 0.01674 92 640:  74%|███████▎  | 14/19 [00:16<00:14,  2.87s/it]97/99 7.3G 0.07256 0.04924 0.01674 92 640:  79%|███████▉  | 15/19 [00:16<00:08,  2.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07255 0.04924 0.01662 85 640:  79%|███████▉  | 15/19 [00:21<00:08,  2.12s/it]97/99 7.3G 0.07255 0.04924 0.01662 85 640:  84%|████████▍ | 16/19 [00:21<00:09,  3.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07203 0.04785 0.01633 46 640:  84%|████████▍ | 16/19 [00:21<00:09,  3.00s/it]97/99 7.3G 0.07203 0.04785 0.01633 46 640:  89%|████████▉ | 17/19 [00:21<00:04,  2.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07224 0.04761 0.01636 72 640:  89%|████████▉ | 17/19 [00:25<00:04,  2.16s/it]97/99 7.3G 0.07224 0.04761 0.01636 72 640:  95%|█████████▍| 18/19 [00:25<00:02,  2.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07182 0.04737 0.01638 69 640:  95%|█████████▍| 18/19 [00:30<00:02,  2.62s/it]97/99 7.3G 0.07182 0.04737 0.01638 69 640: 100%|██████████| 19/19 [00:30<00:00,  3.31s/it]97/99 7.3G 0.07182 0.04737 0.01638 69 640: 100%|██████████| 19/19 [00:30<00:00,  1.58s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.21s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.77s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.73s/it]
                   all         55        256      0.208      0.214       0.17     0.0601
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06127 0.04623 0.01658 74 640:   0%|          | 0/19 [00:00<?, ?it/s]98/99 7.3G 0.06127 0.04623 0.01658 74 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06641 0.04297 0.01612 68 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]98/99 7.3G 0.06641 0.04297 0.01612 68 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06619 0.03894 0.017 55 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]  98/99 7.3G 0.06619 0.03894 0.017 55 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06886 0.04008 0.01665 83 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]98/99 7.3G 0.06886 0.04008 0.01665 83 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06953 0.03827 0.01629 51 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]98/99 7.3G 0.06953 0.03827 0.01629 51 640:  26%|██▋       | 5/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06947 0.03864 0.01593 70 640:  26%|██▋       | 5/19 [00:01<00:02,  5.73it/s]98/99 7.3G 0.06947 0.03864 0.01593 70 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06955 0.03912 0.01612 67 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]98/99 7.3G 0.06955 0.03912 0.01612 67 640:  37%|███▋      | 7/19 [00:01<00:02,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06977 0.0397 0.01614 83 640:  37%|███▋      | 7/19 [00:01<00:02,  5.57it/s] 98/99 7.3G 0.06977 0.0397 0.01614 83 640:  42%|████▏     | 8/19 [00:01<00:02,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07003 0.04122 0.01636 104 640:  42%|████▏     | 8/19 [00:02<00:02,  3.82it/s]98/99 7.3G 0.07003 0.04122 0.01636 104 640:  47%|████▋     | 9/19 [00:02<00:04,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06962 0.04143 0.01613 74 640:  47%|████▋     | 9/19 [00:03<00:04,  2.21it/s] 98/99 7.3G 0.06962 0.04143 0.01613 74 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07013 0.04173 0.01578 108 640:  53%|█████▎    | 10/19 [00:08<00:04,  2.03it/s]98/99 7.3G 0.07013 0.04173 0.01578 108 640:  58%|█████▊    | 11/19 [00:08<00:16,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06992 0.04146 0.01599 66 640:  58%|█████▊    | 11/19 [00:08<00:16,  2.01s/it] 98/99 7.3G 0.06992 0.04146 0.01599 66 640:  63%|██████▎   | 12/19 [00:08<00:10,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07002 0.04121 0.01604 65 640:  63%|██████▎   | 12/19 [00:11<00:10,  1.45s/it]98/99 7.3G 0.07002 0.04121 0.01604 65 640:  68%|██████▊   | 13/19 [00:11<00:11,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07029 0.04156 0.01623 87 640:  68%|██████▊   | 13/19 [00:12<00:11,  1.99s/it]98/99 7.3G 0.07029 0.04156 0.01623 87 640:  74%|███████▎  | 14/19 [00:12<00:07,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07074 0.04147 0.01623 77 640:  74%|███████▎  | 14/19 [00:13<00:07,  1.52s/it]98/99 7.3G 0.07074 0.04147 0.01623 77 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07058 0.04169 0.016 77 640:  79%|███████▉  | 15/19 [00:19<00:05,  1.25s/it]  98/99 7.3G 0.07058 0.04169 0.016 77 640:  84%|████████▍ | 16/19 [00:19<00:08,  2.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07042 0.04153 0.01594 76 640:  84%|████████▍ | 16/19 [00:20<00:08,  2.78s/it]98/99 7.3G 0.07042 0.04153 0.01594 76 640:  89%|████████▉ | 17/19 [00:20<00:04,  2.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07036 0.04225 0.01595 88 640:  89%|████████▉ | 17/19 [00:20<00:04,  2.22s/it]98/99 7.3G 0.07036 0.04225 0.01595 88 640:  95%|█████████▍| 18/19 [00:20<00:01,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.0705 0.04202 0.01597 69 640:  95%|█████████▍| 18/19 [00:28<00:01,  1.68s/it] 98/99 7.3G 0.0705 0.04202 0.01597 69 640: 100%|██████████| 19/19 [00:28<00:00,  3.48s/it]98/99 7.3G 0.0705 0.04202 0.01597 69 640: 100%|██████████| 19/19 [00:28<00:00,  1.49s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.23s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.17s/it]
                   all         55        256      0.205      0.213      0.174     0.0617
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06696 0.03919 0.01522 62 640:   0%|          | 0/19 [00:00<?, ?it/s]99/99 7.3G 0.06696 0.03919 0.01522 62 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06867 0.04347 0.01545 87 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]99/99 7.3G 0.06867 0.04347 0.01545 87 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07116 0.04843 0.01616 106 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]99/99 7.3G 0.07116 0.04843 0.01616 106 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07006 0.04667 0.01576 69 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s] 99/99 7.3G 0.07006 0.04667 0.01576 69 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.0704 0.0466 0.01615 79 640:  21%|██        | 4/19 [00:00<00:02,  5.72it/s]  99/99 7.3G 0.0704 0.0466 0.01615 79 640:  26%|██▋       | 5/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.0704 0.0458 0.01629 71 640:  26%|██▋       | 5/19 [00:01<00:02,  5.73it/s]99/99 7.3G 0.0704 0.0458 0.01629 71 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06996 0.04346 0.01687 44 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s]99/99 7.3G 0.06996 0.04346 0.01687 44 640:  37%|███▋      | 7/19 [00:01<00:02,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06911 0.04139 0.01709 44 640:  37%|███▋      | 7/19 [00:01<00:02,  4.34it/s]99/99 7.3G 0.06911 0.04139 0.01709 44 640:  42%|████▏     | 8/19 [00:01<00:02,  4.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06802 0.03924 0.01785 36 640:  42%|████▏     | 8/19 [00:01<00:02,  4.33it/s]99/99 7.3G 0.06802 0.03924 0.01785 36 640:  47%|████▋     | 9/19 [00:01<00:02,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06879 0.042 0.01763 133 640:  47%|████▋     | 9/19 [00:04<00:02,  4.56it/s] 99/99 7.3G 0.06879 0.042 0.01763 133 640:  53%|█████▎    | 10/19 [00:04<00:10,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06943 0.04509 0.01762 151 640:  53%|█████▎    | 10/19 [00:05<00:10,  1.13s/it]99/99 7.3G 0.06943 0.04509 0.01762 151 640:  58%|█████▊    | 11/19 [00:05<00:06,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07005 0.0455 0.01741 106 640:  58%|█████▊    | 11/19 [00:05<00:06,  1.19it/s] 99/99 7.3G 0.07005 0.0455 0.01741 106 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.0702 0.04605 0.01776 85 640:  63%|██████▎   | 12/19 [00:14<00:04,  1.51it/s] 99/99 7.3G 0.0702 0.04605 0.01776 85 640:  68%|██████▊   | 13/19 [00:14<00:18,  3.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07006 0.04501 0.01768 56 640:  68%|██████▊   | 13/19 [00:14<00:18,  3.08s/it]99/99 7.3G 0.07006 0.04501 0.01768 56 640:  74%|███████▎  | 14/19 [00:14<00:12,  2.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06998 0.04434 0.01793 55 640:  74%|███████▎  | 14/19 [00:16<00:12,  2.43s/it]99/99 7.3G 0.06998 0.04434 0.01793 55 640:  79%|███████▉  | 15/19 [00:16<00:08,  2.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.06997 0.04455 0.0177 85 640:  79%|███████▉  | 15/19 [00:22<00:08,  2.05s/it] 99/99 7.3G 0.06997 0.04455 0.0177 85 640:  84%|████████▍ | 16/19 [00:22<00:09,  3.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07036 0.04572 0.01787 117 640:  84%|████████▍ | 16/19 [00:22<00:09,  3.26s/it]99/99 7.3G 0.07036 0.04572 0.01787 117 640:  89%|████████▉ | 17/19 [00:22<00:04,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07032 0.04589 0.01768 90 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.39s/it] 99/99 7.3G 0.07032 0.04589 0.01768 90 640:  95%|█████████▍| 18/19 [00:27<00:03,  3.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07079 0.0467 0.01751 146 640:  95%|█████████▍| 18/19 [00:28<00:03,  3.23s/it]99/99 7.3G 0.07079 0.0467 0.01751 146 640: 100%|██████████| 19/19 [00:28<00:00,  2.33s/it]99/99 7.3G 0.07079 0.0467 0.01751 146 640: 100%|██████████| 19/19 [00:28<00:00,  1.47s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.82s/it]
                   all         55        256      0.218      0.218       0.18     0.0628
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
[34m[1mtrain_dp: [0mweights=yolov5s.pt, cfg=, data=/mnt/bst/hxu10/hxu10/chanti/dataset/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, device=6, multi_scale=False, single_cls=False, optimizer=SGD, workers=8, project=runs/train/train_dp, name=noise_0.2, exist_ok=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, dp=True, noise_multiplier=0.2, max_grad_norm=5.0, delta=1e-05
[34m[1mgithub: [0mup to date with https://github.com/ultralytics/yolov5 ✅
YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.5 torch-2.6.0+cu124 CUDA:6 (NVIDIA A100-SXM4-80GB, 81154MiB)

[34m[1mhyperparameters: [0mlr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0005, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
Overriding model.yaml nc=80 with nc=3

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs

Transferred 342/349 layers from yolov5s.pt
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
[34m[1mAMP: [0mchecks passed ✅
[34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s][34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s]
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
Privacy Engine attached: Noise Multiplier=0.2, Max Grad Norm=5.0
/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=amp and not opt.dp)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to [1mruns/train/train_dp/noise_0.24[0m
Starting training for 100 epochs...
✅ Model is now private: False
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1141 0.03617 0.03665 62 640:   0%|          | 0/19 [00:33<?, ?it/s]0/99 7.18G 0.1141 0.03617 0.03665 62 640:   5%|▌         | 1/19 [00:33<09:58, 33.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1141 0.03664 0.03689 61 640:   5%|▌         | 1/19 [00:33<09:58, 33.25s/it]0/99 7.18G 0.1141 0.03664 0.03689 61 640:  11%|█         | 2/19 [00:33<03:54, 13.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1135 0.03516 0.03683 52 640:  11%|█         | 2/19 [00:33<03:54, 13.80s/it]0/99 7.18G 0.1135 0.03516 0.03683 52 640:  16%|█▌        | 3/19 [00:33<02:01,  7.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1137 0.03535 0.03693 61 640:  16%|█▌        | 3/19 [00:33<02:01,  7.58s/it]0/99 7.18G 0.1137 0.03535 0.03693 61 640:  21%|██        | 4/19 [00:33<01:10,  4.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1133 0.0361 0.03682 66 640:  21%|██        | 4/19 [00:34<01:10,  4.73s/it] 0/99 7.18G 0.1133 0.0361 0.03682 66 640:  26%|██▋       | 5/19 [00:34<00:45,  3.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1133 0.04029 0.03681 175 640:  26%|██▋       | 5/19 [00:34<00:45,  3.21s/it]0/99 7.18G 0.1133 0.04029 0.03681 175 640:  32%|███▏      | 6/19 [00:34<00:29,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1129 0.03985 0.03683 69 640:  32%|███▏      | 6/19 [00:35<00:29,  2.24s/it] 0/99 7.18G 0.1129 0.03985 0.03683 69 640:  37%|███▋      | 7/19 [00:35<00:19,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1132 0.04215 0.03689 148 640:  37%|███▋      | 7/19 [00:35<00:19,  1.62s/it]0/99 7.18G 0.1132 0.04215 0.03689 148 640:  42%|████▏     | 8/19 [00:35<00:13,  1.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1127 0.04258 0.0369 88 640:  42%|████▏     | 8/19 [00:49<00:13,  1.22s/it]  0/99 7.18G 0.1127 0.04258 0.0369 88 640:  47%|████▋     | 9/19 [00:49<00:52,  5.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1123 0.04149 0.0369 45 640:  47%|████▋     | 9/19 [00:49<00:52,  5.30s/it]0/99 7.18G 0.1123 0.04149 0.0369 45 640:  53%|█████▎    | 10/19 [00:49<00:33,  3.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1118 0.04107 0.03691 63 640:  53%|█████▎    | 10/19 [00:50<00:33,  3.72s/it]0/99 7.18G 0.1118 0.04107 0.03691 63 640:  58%|█████▊    | 11/19 [00:50<00:21,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1118 0.04221 0.03689 134 640:  58%|█████▊    | 11/19 [00:50<00:21,  2.63s/it]0/99 7.18G 0.1118 0.04221 0.03689 134 640:  63%|██████▎   | 12/19 [00:50<00:13,  1.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1113 0.0418 0.03686 58 640:  63%|██████▎   | 12/19 [00:50<00:13,  1.88s/it]  0/99 7.18G 0.1113 0.0418 0.03686 58 640:  68%|██████▊   | 13/19 [00:50<00:08,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.111 0.04142 0.0368 59 640:  68%|██████▊   | 13/19 [00:50<00:08,  1.37s/it] 0/99 7.18G 0.111 0.04142 0.0368 59 640:  74%|███████▎  | 14/19 [00:50<00:05,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1106 0.04136 0.03675 67 640:  74%|███████▎  | 14/19 [00:51<00:05,  1.07s/it]0/99 7.18G 0.1106 0.04136 0.03675 67 640:  79%|███████▉  | 15/19 [00:51<00:03,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1106 0.04177 0.03677 107 640:  79%|███████▉  | 15/19 [00:51<00:03,  1.05it/s]0/99 7.18G 0.1106 0.04177 0.03677 107 640:  84%|████████▍ | 16/19 [00:51<00:02,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1103 0.04161 0.03674 71 640:  84%|████████▍ | 16/19 [01:14<00:02,  1.40it/s] 0/99 7.18G 0.1103 0.04161 0.03674 71 640:  89%|████████▉ | 17/19 [01:14<00:14,  7.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1101 0.04131 0.03669 61 640:  89%|████████▉ | 17/19 [01:14<00:14,  7.20s/it]0/99 7.18G 0.1101 0.04131 0.03669 61 640:  95%|█████████▍| 18/19 [01:14<00:05,  5.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1099 0.04122 0.03667 72 640:  95%|█████████▍| 18/19 [01:15<00:05,  5.13s/it]0/99 7.18G 0.1099 0.04122 0.03667 72 640: 100%|██████████| 19/19 [01:15<00:00,  3.82s/it]0/99 7.18G 0.1099 0.04122 0.03667 72 640: 100%|██████████| 19/19 [01:15<00:00,  3.95s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.06s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  5.09s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]
                   all         55        256    0.00119      0.213    0.00119   0.000281
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1033 0.04456 0.03545 74 640:   0%|          | 0/19 [00:02<?, ?it/s]1/99 7.2G 0.1033 0.04456 0.03545 74 640:   5%|▌         | 1/19 [00:02<00:40,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1059 0.0439 0.03581 87 640:   5%|▌         | 1/19 [00:02<00:40,  2.26s/it] 1/99 7.2G 0.1059 0.0439 0.03581 87 640:  11%|█         | 2/19 [00:02<00:19,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1052 0.04362 0.036 89 640:  11%|█         | 2/19 [00:03<00:19,  1.15s/it] 1/99 7.2G 0.1052 0.04362 0.036 89 640:  16%|█▌        | 3/19 [00:03<00:13,  1.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.105 0.04162 0.0362 65 640:  16%|█▌        | 3/19 [00:03<00:13,  1.18it/s]1/99 7.2G 0.105 0.04162 0.0362 65 640:  21%|██        | 4/19 [00:03<00:08,  1.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1048 0.04214 0.03629 85 640:  21%|██        | 4/19 [00:03<00:08,  1.68it/s]1/99 7.2G 0.1048 0.04214 0.03629 85 640:  26%|██▋       | 5/19 [00:03<00:06,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1044 0.04142 0.03626 66 640:  26%|██▋       | 5/19 [00:03<00:06,  2.21it/s]1/99 7.2G 0.1044 0.04142 0.03626 66 640:  32%|███▏      | 6/19 [00:03<00:04,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1044 0.04079 0.03624 63 640:  32%|███▏      | 6/19 [00:04<00:04,  2.79it/s]1/99 7.2G 0.1044 0.04079 0.03624 63 640:  37%|███▋      | 7/19 [00:04<00:04,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.104 0.04021 0.03616 55 640:  37%|███▋      | 7/19 [00:04<00:04,  2.98it/s] 1/99 7.2G 0.104 0.04021 0.03616 55 640:  42%|████▏     | 8/19 [00:04<00:03,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1037 0.0404 0.0362 73 640:  42%|████▏     | 8/19 [00:04<00:03,  2.95it/s] 1/99 7.2G 0.1037 0.0404 0.0362 73 640:  47%|████▋     | 9/19 [00:04<00:03,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1039 0.0427 0.03614 143 640:  47%|████▋     | 9/19 [00:05<00:03,  2.56it/s]1/99 7.2G 0.1039 0.0427 0.03614 143 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1037 0.04149 0.03615 43 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.83it/s]1/99 7.2G 0.1037 0.04149 0.03615 43 640:  58%|█████▊    | 11/19 [00:05<00:02,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1041 0.04398 0.03613 181 640:  58%|█████▊    | 11/19 [00:06<00:02,  2.83it/s]1/99 7.2G 0.1041 0.04398 0.03613 181 640:  63%|██████▎   | 12/19 [00:06<00:02,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1038 0.04453 0.03612 95 640:  63%|██████▎   | 12/19 [00:06<00:02,  2.42it/s] 1/99 7.2G 0.1038 0.04453 0.03612 95 640:  68%|██████▊   | 13/19 [00:06<00:03,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1037 0.04487 0.03606 92 640:  68%|██████▊   | 13/19 [00:22<00:03,  1.80it/s]1/99 7.2G 0.1037 0.04487 0.03606 92 640:  74%|███████▎  | 14/19 [00:22<00:25,  5.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1035 0.04453 0.03601 70 640:  74%|███████▎  | 14/19 [00:22<00:25,  5.02s/it]1/99 7.2G 0.1035 0.04453 0.03601 70 640:  79%|███████▉  | 15/19 [00:22<00:14,  3.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1033 0.04412 0.03599 63 640:  79%|███████▉  | 15/19 [00:22<00:14,  3.56s/it]1/99 7.2G 0.1033 0.04412 0.03599 63 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.103 0.04391 0.03599 66 640:  84%|████████▍ | 16/19 [00:22<00:07,  2.54s/it] 1/99 7.2G 0.103 0.04391 0.03599 66 640:  89%|████████▉ | 17/19 [00:22<00:03,  1.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1028 0.04345 0.03593 58 640:  89%|████████▉ | 17/19 [00:25<00:03,  1.83s/it]1/99 7.2G 0.1028 0.04345 0.03593 58 640:  95%|█████████▍| 18/19 [00:25<00:02,  2.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1026 0.04314 0.03589 60 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.05s/it]1/99 7.2G 0.1026 0.04314 0.03589 60 640: 100%|██████████| 19/19 [00:26<00:00,  1.88s/it]1/99 7.2G 0.1026 0.04314 0.03589 60 640: 100%|██████████| 19/19 [00:26<00:00,  1.41s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:12<00:12, 12.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  5.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.85s/it]
                   all         55        256    0.00153      0.304    0.00157   0.000332
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1087 0.05048 0.03454 144 640:   0%|          | 0/19 [00:02<?, ?it/s]2/99 7.21G 0.1087 0.05048 0.03454 144 640:   5%|▌         | 1/19 [00:02<00:38,  2.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1042 0.05192 0.03473 107 640:   5%|▌         | 1/19 [00:02<00:38,  2.17s/it]2/99 7.21G 0.1042 0.05192 0.03473 107 640:  11%|█         | 2/19 [00:02<00:20,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1022 0.04791 0.0349 66 640:  11%|█         | 2/19 [00:02<00:20,  1.23s/it]  2/99 7.21G 0.1022 0.04791 0.0349 66 640:  16%|█▌        | 3/19 [00:02<00:12,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1007 0.04627 0.0346 68 640:  16%|█▌        | 3/19 [00:03<00:12,  1.31it/s]2/99 7.21G 0.1007 0.04627 0.0346 68 640:  21%|██        | 4/19 [00:03<00:08,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1001 0.04632 0.03465 76 640:  21%|██        | 4/19 [00:03<00:08,  1.84it/s]2/99 7.21G 0.1001 0.04632 0.03465 76 640:  26%|██▋       | 5/19 [00:03<00:05,  2.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09981 0.04462 0.03455 61 640:  26%|██▋       | 5/19 [00:03<00:05,  2.37it/s]2/99 7.21G 0.09981 0.04462 0.03455 61 640:  32%|███▏      | 6/19 [00:03<00:04,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1002 0.04552 0.03454 106 640:  32%|███▏      | 6/19 [00:03<00:04,  2.75it/s]2/99 7.21G 0.1002 0.04552 0.03454 106 640:  37%|███▋      | 7/19 [00:03<00:04,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1003 0.04512 0.03445 79 640:  37%|███▋      | 7/19 [00:04<00:04,  2.80it/s] 2/99 7.21G 0.1003 0.04512 0.03445 79 640:  42%|████▏     | 8/19 [00:04<00:03,  3.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1 0.04464 0.03441 69 640:  42%|████▏     | 8/19 [00:04<00:03,  3.24it/s]   2/99 7.21G 0.1 0.04464 0.03441 69 640:  47%|████▋     | 9/19 [00:04<00:03,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09968 0.04354 0.03432 55 640:  47%|████▋     | 9/19 [00:05<00:03,  3.29it/s]2/99 7.22G 0.09968 0.04354 0.03432 55 640:  53%|█████▎    | 10/19 [00:05<00:04,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09965 0.04339 0.03428 76 640:  53%|█████▎    | 10/19 [00:21<00:04,  1.94it/s]2/99 7.22G 0.09965 0.04339 0.03428 76 640:  58%|█████▊    | 11/19 [00:21<00:41,  5.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09922 0.0418 0.03416 35 640:  58%|█████▊    | 11/19 [00:21<00:41,  5.21s/it] 2/99 7.22G 0.09922 0.0418 0.03416 35 640:  63%|██████▎   | 12/19 [00:21<00:25,  3.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09903 0.04129 0.03419 55 640:  63%|██████▎   | 12/19 [00:21<00:25,  3.68s/it]2/99 7.22G 0.09903 0.04129 0.03419 55 640:  68%|██████▊   | 13/19 [00:21<00:15,  2.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.0991 0.04227 0.03416 101 640:  68%|██████▊   | 13/19 [00:21<00:15,  2.62s/it]2/99 7.22G 0.0991 0.04227 0.03416 101 640:  74%|███████▎  | 14/19 [00:21<00:09,  1.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09875 0.04194 0.0341 58 640:  74%|███████▎  | 14/19 [00:21<00:09,  1.88s/it] 2/99 7.22G 0.09875 0.04194 0.0341 58 640:  79%|███████▉  | 15/19 [00:21<00:05,  1.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.0984 0.0425 0.03409 78 640:  79%|███████▉  | 15/19 [00:22<00:05,  1.36s/it] 2/99 7.22G 0.0984 0.0425 0.03409 78 640:  84%|████████▍ | 16/19 [00:22<00:03,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09869 0.04456 0.03404 194 640:  84%|████████▍ | 16/19 [00:27<00:03,  1.25s/it]2/99 7.22G 0.09869 0.04456 0.03404 194 640:  89%|████████▉ | 17/19 [00:27<00:04,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09856 0.04497 0.03406 86 640:  89%|████████▉ | 17/19 [00:28<00:04,  2.10s/it] 2/99 7.22G 0.09856 0.04497 0.03406 86 640:  95%|█████████▍| 18/19 [00:28<00:01,  1.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.22G 0.09869 0.04495 0.03405 82 640:  95%|█████████▍| 18/19 [00:45<00:01,  1.87s/it]2/99 7.22G 0.09869 0.04495 0.03405 82 640: 100%|██████████| 19/19 [00:45<00:00,  6.46s/it]2/99 7.22G 0.09869 0.04495 0.03405 82 640: 100%|██████████| 19/19 [00:45<00:00,  2.40s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:11<00:11, 11.37s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  4.77s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:11<00:00,  5.76s/it]
                   all         55        256    0.00224       0.31    0.00305   0.000622
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09362 0.03941 0.03304 59 640:   0%|          | 0/19 [00:02<?, ?it/s]3/99 7.23G 0.09362 0.03941 0.03304 59 640:   5%|▌         | 1/19 [00:02<00:51,  2.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09469 0.04118 0.03355 73 640:   5%|▌         | 1/19 [00:03<00:51,  2.86s/it]3/99 7.23G 0.09469 0.04118 0.03355 73 640:  11%|█         | 2/19 [00:03<00:25,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09571 0.042 0.03353 84 640:  11%|█         | 2/19 [00:04<00:25,  1.49s/it]  3/99 7.23G 0.09571 0.042 0.03353 84 640:  16%|█▌        | 3/19 [00:04<00:17,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09566 0.04349 0.03385 75 640:  16%|█▌        | 3/19 [00:04<00:17,  1.11s/it]3/99 7.23G 0.09566 0.04349 0.03385 75 640:  21%|██        | 4/19 [00:04<00:15,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09559 0.04312 0.03372 66 640:  21%|██        | 4/19 [00:05<00:15,  1.03s/it]3/99 7.23G 0.09559 0.04312 0.03372 66 640:  26%|██▋       | 5/19 [00:05<00:13,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09536 0.04153 0.03364 58 640:  26%|██▋       | 5/19 [00:06<00:13,  1.02it/s]3/99 7.23G 0.09536 0.04153 0.03364 58 640:  32%|███▏      | 6/19 [00:06<00:10,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09594 0.04427 0.03362 119 640:  32%|███▏      | 6/19 [00:06<00:10,  1.22it/s]3/99 7.23G 0.09594 0.04427 0.03362 119 640:  37%|███▋      | 7/19 [00:06<00:07,  1.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09554 0.04407 0.03344 67 640:  37%|███▋      | 7/19 [00:06<00:07,  1.51it/s] 3/99 7.23G 0.09554 0.04407 0.03344 67 640:  42%|████▏     | 8/19 [00:06<00:05,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09556 0.04459 0.03366 80 640:  42%|████▏     | 8/19 [00:07<00:05,  1.86it/s]3/99 7.23G 0.09556 0.04459 0.03366 80 640:  47%|████▋     | 9/19 [00:07<00:04,  2.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09515 0.04415 0.03365 59 640:  47%|████▋     | 9/19 [00:07<00:04,  2.02it/s]3/99 7.23G 0.09515 0.04415 0.03365 59 640:  53%|█████▎    | 10/19 [00:07<00:04,  2.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09523 0.04431 0.03355 91 640:  53%|█████▎    | 10/19 [00:07<00:04,  2.20it/s]3/99 7.23G 0.09523 0.04431 0.03355 91 640:  58%|█████▊    | 11/19 [00:07<00:03,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09519 0.04531 0.0335 93 640:  58%|█████▊    | 11/19 [00:08<00:03,  2.54it/s] 3/99 7.23G 0.09519 0.04531 0.0335 93 640:  63%|██████▎   | 12/19 [00:08<00:02,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09471 0.04456 0.0334 50 640:  63%|██████▎   | 12/19 [00:08<00:02,  2.84it/s]3/99 7.23G 0.09471 0.04456 0.0334 50 640:  68%|██████▊   | 13/19 [00:08<00:01,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09444 0.04475 0.03327 75 640:  68%|██████▊   | 13/19 [00:08<00:01,  3.09it/s]3/99 7.23G 0.09444 0.04475 0.03327 75 640:  74%|███████▎  | 14/19 [00:08<00:01,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09432 0.04527 0.0332 85 640:  74%|███████▎  | 14/19 [00:09<00:01,  2.86it/s] 3/99 7.23G 0.09432 0.04527 0.0332 85 640:  79%|███████▉  | 15/19 [00:09<00:01,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09436 0.04556 0.03314 88 640:  79%|███████▉  | 15/19 [00:24<00:01,  2.78it/s]3/99 7.23G 0.09436 0.04556 0.03314 88 640:  84%|████████▍ | 16/19 [00:24<00:14,  4.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09363 0.04492 0.03303 45 640:  84%|████████▍ | 16/19 [00:25<00:14,  4.78s/it]3/99 7.23G 0.09363 0.04492 0.03303 45 640:  89%|████████▉ | 17/19 [00:25<00:07,  3.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09359 0.04463 0.03298 66 640:  89%|████████▉ | 17/19 [00:26<00:07,  3.61s/it]3/99 7.23G 0.09359 0.04463 0.03298 66 640:  95%|█████████▍| 18/19 [00:26<00:02,  2.96s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09347 0.04496 0.03301 83 640:  95%|█████████▍| 18/19 [00:28<00:02,  2.96s/it]3/99 7.23G 0.09347 0.04496 0.03301 83 640: 100%|██████████| 19/19 [00:28<00:00,  2.49s/it]3/99 7.23G 0.09347 0.04496 0.03301 83 640: 100%|██████████| 19/19 [00:28<00:00,  1.48s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.45s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.95s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.78s/it]
                   all         55        256    0.00821       0.38    0.00945    0.00264
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09241 0.04263 0.03052 69 640:   0%|          | 0/19 [00:00<?, ?it/s]4/99 7.25G 0.09241 0.04263 0.03052 69 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09085 0.04213 0.0327 65 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s] 4/99 7.25G 0.09085 0.04213 0.0327 65 640:  11%|█         | 2/19 [00:00<00:07,  2.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09063 0.04418 0.0328 77 640:  11%|█         | 2/19 [00:01<00:07,  2.34it/s]4/99 7.25G 0.09063 0.04418 0.0328 77 640:  16%|█▌        | 3/19 [00:01<00:06,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.0907 0.04094 0.03253 47 640:  16%|█▌        | 3/19 [00:01<00:06,  2.44it/s]4/99 7.25G 0.0907 0.04094 0.03253 47 640:  21%|██        | 4/19 [00:01<00:05,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09055 0.04085 0.0327 57 640:  21%|██        | 4/19 [00:02<00:05,  2.52it/s]4/99 7.25G 0.09055 0.04085 0.0327 57 640:  26%|██▋       | 5/19 [00:02<00:05,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.08953 0.03944 0.0323 46 640:  26%|██▋       | 5/19 [00:02<00:05,  2.58it/s]4/99 7.25G 0.08953 0.03944 0.0323 46 640:  32%|███▏      | 6/19 [00:02<00:05,  2.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09048 0.04149 0.03214 102 640:  32%|███▏      | 6/19 [00:03<00:05,  2.24it/s]4/99 7.25G 0.09048 0.04149 0.03214 102 640:  37%|███▋      | 7/19 [00:03<00:06,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09027 0.04146 0.03199 65 640:  37%|███▋      | 7/19 [00:03<00:06,  1.99it/s] 4/99 7.25G 0.09027 0.04146 0.03199 65 640:  42%|████▏     | 8/19 [00:03<00:06,  1.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09031 0.04395 0.03208 99 640:  42%|████▏     | 8/19 [00:04<00:06,  1.76it/s]4/99 7.25G 0.09031 0.04395 0.03208 99 640:  47%|████▋     | 9/19 [00:04<00:05,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09061 0.04417 0.0321 74 640:  47%|████▋     | 9/19 [00:04<00:05,  1.82it/s] 4/99 7.25G 0.09061 0.04417 0.0321 74 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09079 0.045 0.03202 87 640:  53%|█████▎    | 10/19 [00:05<00:04,  1.97it/s] 4/99 7.25G 0.09079 0.045 0.03202 87 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09083 0.04672 0.03193 108 640:  58%|█████▊    | 11/19 [00:06<00:03,  2.06it/s]4/99 7.25G 0.09083 0.04672 0.03193 108 640:  63%|██████▎   | 12/19 [00:06<00:03,  1.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09091 0.04639 0.03205 73 640:  63%|██████▎   | 12/19 [00:18<00:03,  1.79it/s] 4/99 7.25G 0.09091 0.04639 0.03205 73 640:  68%|██████▊   | 13/19 [00:18<00:24,  4.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09073 0.04538 0.03215 50 640:  68%|██████▊   | 13/19 [00:18<00:24,  4.04s/it]4/99 7.25G 0.09073 0.04538 0.03215 50 640:  74%|███████▎  | 14/19 [00:18<00:15,  3.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09119 0.04702 0.03206 146 640:  74%|███████▎  | 14/19 [00:19<00:15,  3.07s/it]4/99 7.25G 0.09119 0.04702 0.03206 146 640:  79%|███████▉  | 15/19 [00:19<00:09,  2.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09131 0.04736 0.03207 96 640:  79%|███████▉  | 15/19 [00:20<00:09,  2.39s/it] 4/99 7.25G 0.09131 0.04736 0.03207 96 640:  84%|████████▍ | 16/19 [00:20<00:06,  2.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09145 0.04873 0.03204 121 640:  84%|████████▍ | 16/19 [00:22<00:06,  2.02s/it]4/99 7.25G 0.09145 0.04873 0.03204 121 640:  89%|████████▉ | 17/19 [00:22<00:03,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09127 0.04875 0.03208 75 640:  89%|████████▉ | 17/19 [00:23<00:03,  1.97s/it] 4/99 7.25G 0.09127 0.04875 0.03208 75 640:  95%|█████████▍| 18/19 [00:23<00:01,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09106 0.04913 0.03202 94 640:  95%|█████████▍| 18/19 [00:24<00:01,  1.63s/it]4/99 7.25G 0.09106 0.04913 0.03202 94 640: 100%|██████████| 19/19 [00:24<00:00,  1.31s/it]4/99 7.25G 0.09106 0.04913 0.03202 94 640: 100%|██████████| 19/19 [00:24<00:00,  1.27s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:13<00:13, 13.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  6.44s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:15<00:00,  7.55s/it]
                   all         55        256    0.00595      0.374     0.0111    0.00315
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08731 0.0399 0.03154 55 640:   0%|          | 0/19 [00:00<?, ?it/s]5/99 7.25G 0.08731 0.0399 0.03154 55 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08787 0.04062 0.0316 62 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]5/99 7.25G 0.08787 0.04062 0.0316 62 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09122 0.04477 0.03133 131 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]5/99 7.25G 0.09122 0.04477 0.03133 131 640:  16%|█▌        | 3/19 [00:00<00:02,  5.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09104 0.0463 0.03167 89 640:  16%|█▌        | 3/19 [00:00<00:02,  5.41it/s]  5/99 7.25G 0.09104 0.0463 0.03167 89 640:  21%|██        | 4/19 [00:00<00:03,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09024 0.04364 0.03138 48 640:  21%|██        | 4/19 [00:01<00:03,  3.96it/s]5/99 7.25G 0.09024 0.04364 0.03138 48 640:  26%|██▋       | 5/19 [00:01<00:03,  3.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08981 0.04449 0.03118 73 640:  26%|██▋       | 5/19 [00:01<00:03,  3.59it/s]5/99 7.25G 0.08981 0.04449 0.03118 73 640:  32%|███▏      | 6/19 [00:01<00:03,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09163 0.04967 0.03116 190 640:  32%|███▏      | 6/19 [00:01<00:03,  4.11it/s]5/99 7.25G 0.09163 0.04967 0.03116 190 640:  37%|███▋      | 7/19 [00:01<00:02,  4.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09242 0.05008 0.03138 100 640:  37%|███▋      | 7/19 [00:01<00:02,  4.53it/s]5/99 7.25G 0.09242 0.05008 0.03138 100 640:  42%|████▏     | 8/19 [00:01<00:02,  4.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09203 0.04974 0.03132 72 640:  42%|████▏     | 8/19 [00:02<00:02,  4.39it/s] 5/99 7.25G 0.09203 0.04974 0.03132 72 640:  47%|████▋     | 9/19 [00:02<00:02,  4.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09208 0.04804 0.0312 56 640:  47%|████▋     | 9/19 [00:09<00:02,  4.10it/s] 5/99 7.25G 0.09208 0.04804 0.0312 56 640:  53%|█████▎    | 10/19 [00:09<00:22,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09197 0.04771 0.03142 66 640:  53%|█████▎    | 10/19 [00:10<00:22,  2.45s/it]5/99 7.25G 0.09197 0.04771 0.03142 66 640:  58%|█████▊    | 11/19 [00:10<00:14,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09162 0.04747 0.03157 68 640:  58%|█████▊    | 11/19 [00:10<00:14,  1.86s/it]5/99 7.25G 0.09162 0.04747 0.03157 68 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09206 0.04876 0.0317 123 640:  63%|██████▎   | 12/19 [00:11<00:10,  1.45s/it]5/99 7.25G 0.09206 0.04876 0.0317 123 640:  68%|██████▊   | 13/19 [00:11<00:07,  1.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09193 0.04952 0.03155 101 640:  68%|██████▊   | 13/19 [00:13<00:07,  1.20s/it]5/99 7.25G 0.09193 0.04952 0.03155 101 640:  74%|███████▎  | 14/19 [00:13<00:07,  1.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09138 0.04888 0.03155 56 640:  74%|███████▎  | 14/19 [00:15<00:07,  1.41s/it] 5/99 7.25G 0.09138 0.04888 0.03155 56 640:  79%|███████▉  | 15/19 [00:15<00:07,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09094 0.0487 0.03154 66 640:  79%|███████▉  | 15/19 [00:16<00:07,  1.86s/it] 5/99 7.25G 0.09094 0.0487 0.03154 66 640:  84%|████████▍ | 16/19 [00:16<00:04,  1.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09073 0.04929 0.03147 96 640:  84%|████████▍ | 16/19 [00:22<00:04,  1.46s/it]5/99 7.25G 0.09073 0.04929 0.03147 96 640:  89%|████████▉ | 17/19 [00:22<00:05,  2.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09113 0.0499 0.03144 126 640:  89%|████████▉ | 17/19 [00:30<00:05,  2.90s/it]5/99 7.25G 0.09113 0.0499 0.03144 126 640:  95%|█████████▍| 18/19 [00:30<00:04,  4.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09107 0.04996 0.03141 87 640:  95%|█████████▍| 18/19 [00:31<00:04,  4.47s/it]5/99 7.25G 0.09107 0.04996 0.03141 87 640: 100%|██████████| 19/19 [00:31<00:00,  3.37s/it]5/99 7.25G 0.09107 0.04996 0.03141 87 640: 100%|██████████| 19/19 [00:31<00:00,  1.67s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.53s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]
                   all         55        256    0.00591       0.41     0.0139    0.00305
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09337 0.04841 0.03137 96 640:   0%|          | 0/19 [00:00<?, ?it/s]6/99 7.25G 0.09337 0.04841 0.03137 96 640:   5%|▌         | 1/19 [00:00<00:05,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09318 0.04762 0.03142 83 640:   5%|▌         | 1/19 [00:00<00:05,  3.21it/s]6/99 7.25G 0.09318 0.04762 0.03142 83 640:  11%|█         | 2/19 [00:00<00:05,  3.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09236 0.04977 0.03198 90 640:  11%|█         | 2/19 [00:01<00:05,  3.02it/s]6/99 7.25G 0.09236 0.04977 0.03198 90 640:  16%|█▌        | 3/19 [00:01<00:05,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08982 0.04684 0.03211 53 640:  16%|█▌        | 3/19 [00:01<00:05,  2.96it/s]6/99 7.25G 0.08982 0.04684 0.03211 53 640:  21%|██        | 4/19 [00:01<00:05,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08971 0.04595 0.03205 67 640:  21%|██        | 4/19 [00:01<00:05,  2.92it/s]6/99 7.25G 0.08971 0.04595 0.03205 67 640:  26%|██▋       | 5/19 [00:01<00:04,  2.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09045 0.05156 0.03185 145 640:  26%|██▋       | 5/19 [00:01<00:04,  2.91it/s]6/99 7.25G 0.09045 0.05156 0.03185 145 640:  32%|███▏      | 6/19 [00:01<00:04,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.0898 0.05047 0.03177 63 640:  32%|███▏      | 6/19 [00:02<00:04,  3.21it/s]  6/99 7.25G 0.0898 0.05047 0.03177 63 640:  37%|███▋      | 7/19 [00:02<00:03,  3.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09024 0.05193 0.03155 123 640:  37%|███▋      | 7/19 [00:02<00:03,  3.75it/s]6/99 7.25G 0.09024 0.05193 0.03155 123 640:  42%|████▏     | 8/19 [00:02<00:02,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09012 0.05254 0.03172 98 640:  42%|████▏     | 8/19 [00:02<00:02,  4.22it/s] 6/99 7.25G 0.09012 0.05254 0.03172 98 640:  47%|████▋     | 9/19 [00:02<00:02,  4.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09027 0.05174 0.03177 73 640:  47%|████▋     | 9/19 [00:02<00:02,  4.60it/s]6/99 7.25G 0.09027 0.05174 0.03177 73 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09032 0.05176 0.03158 87 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.34it/s]6/99 7.25G 0.09032 0.05176 0.03158 87 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08983 0.05236 0.0315 97 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.15it/s] 6/99 7.25G 0.08983 0.05236 0.0315 97 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.0893 0.05101 0.03155 48 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.53it/s]6/99 7.25G 0.0893 0.05101 0.03155 48 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08934 0.05229 0.03147 114 640:  68%|██████▊   | 13/19 [00:08<00:01,  4.81it/s]6/99 7.25G 0.08934 0.05229 0.03147 114 640:  74%|███████▎  | 14/19 [00:08<00:08,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08933 0.05238 0.03132 89 640:  74%|███████▎  | 14/19 [00:20<00:08,  1.79s/it] 6/99 7.25G 0.08933 0.05238 0.03132 89 640:  79%|███████▉  | 15/19 [00:20<00:18,  4.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08913 0.0517 0.03111 61 640:  79%|███████▉  | 15/19 [00:20<00:18,  4.63s/it] 6/99 7.25G 0.08913 0.0517 0.03111 61 640:  84%|████████▍ | 16/19 [00:20<00:10,  3.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08886 0.05083 0.03107 50 640:  84%|████████▍ | 16/19 [00:21<00:10,  3.38s/it]6/99 7.25G 0.08886 0.05083 0.03107 50 640:  89%|████████▉ | 17/19 [00:21<00:05,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08878 0.05124 0.03109 90 640:  89%|████████▉ | 17/19 [00:22<00:05,  2.63s/it]6/99 7.25G 0.08878 0.05124 0.03109 90 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.08878 0.0516 0.03104 89 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.07s/it] 6/99 7.25G 0.08878 0.0516 0.03104 89 640: 100%|██████████| 19/19 [00:22<00:00,  1.61s/it]6/99 7.25G 0.08878 0.0516 0.03104 89 640: 100%|██████████| 19/19 [00:22<00:00,  1.19s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.70s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]
                   all         55        256    0.00513      0.406     0.0165    0.00449
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08648 0.04287 0.02862 69 640:   0%|          | 0/19 [00:00<?, ?it/s]7/99 7.25G 0.08648 0.04287 0.02862 69 640:   5%|▌         | 1/19 [00:00<00:03,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08921 0.05368 0.0293 124 640:   5%|▌         | 1/19 [00:00<00:03,  5.70it/s]7/99 7.25G 0.08921 0.05368 0.0293 124 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08807 0.05743 0.02979 101 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]7/99 7.25G 0.08807 0.05743 0.02979 101 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08798 0.05451 0.03003 72 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s] 7/99 7.25G 0.08798 0.05451 0.03003 72 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08964 0.06222 0.03005 187 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]7/99 7.25G 0.08964 0.06222 0.03005 187 640:  26%|██▋       | 5/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08831 0.05697 0.02966 46 640:  26%|██▋       | 5/19 [00:01<00:02,  5.73it/s] 7/99 7.25G 0.08831 0.05697 0.02966 46 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08843 0.0562 0.02953 85 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s] 7/99 7.25G 0.08843 0.0562 0.02953 85 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08924 0.05341 0.02917 59 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]7/99 7.25G 0.08924 0.05341 0.02917 59 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08998 0.05479 0.02897 143 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]7/99 7.25G 0.08998 0.05479 0.02897 143 640:  47%|████▋     | 9/19 [00:01<00:01,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08975 0.05211 0.02898 43 640:  47%|████▋     | 9/19 [00:01<00:01,  5.74it/s] 7/99 7.25G 0.08975 0.05211 0.02898 43 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08874 0.05027 0.02896 43 640:  53%|█████▎    | 10/19 [00:04<00:01,  5.75it/s]7/99 7.25G 0.08874 0.05027 0.02896 43 640:  58%|█████▊    | 11/19 [00:04<00:08,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08819 0.04937 0.0288 56 640:  58%|█████▊    | 11/19 [00:14<00:08,  1.07s/it] 7/99 7.25G 0.08819 0.04937 0.0288 56 640:  63%|██████▎   | 12/19 [00:14<00:25,  3.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08853 0.05134 0.02881 132 640:  63%|██████▎   | 12/19 [00:14<00:25,  3.57s/it]7/99 7.25G 0.08853 0.05134 0.02881 132 640:  68%|██████▊   | 13/19 [00:14<00:15,  2.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08847 0.05069 0.02871 67 640:  68%|██████▊   | 13/19 [00:16<00:15,  2.66s/it] 7/99 7.25G 0.08847 0.05069 0.02871 67 640:  74%|███████▎  | 14/19 [00:16<00:11,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08843 0.05092 0.02888 84 640:  74%|███████▎  | 14/19 [00:16<00:11,  2.28s/it]7/99 7.25G 0.08843 0.05092 0.02888 84 640:  79%|███████▉  | 15/19 [00:16<00:07,  1.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0881 0.05054 0.02908 64 640:  79%|███████▉  | 15/19 [00:17<00:07,  1.84s/it] 7/99 7.25G 0.0881 0.05054 0.02908 64 640:  84%|████████▍ | 16/19 [00:17<00:04,  1.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08852 0.05077 0.0291 113 640:  84%|████████▍ | 16/19 [00:18<00:04,  1.40s/it]7/99 7.25G 0.08852 0.05077 0.0291 113 640:  89%|████████▉ | 17/19 [00:18<00:02,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0878 0.04964 0.0292 38 640:  89%|████████▉ | 17/19 [00:19<00:02,  1.39s/it]  7/99 7.25G 0.0878 0.04964 0.0292 38 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.08733 0.04938 0.02916 59 640:  95%|█████████▍| 18/19 [00:22<00:01,  1.10s/it]7/99 7.25G 0.08733 0.04938 0.02916 59 640: 100%|██████████| 19/19 [00:22<00:00,  1.65s/it]7/99 7.25G 0.08733 0.04938 0.02916 59 640: 100%|██████████| 19/19 [00:22<00:00,  1.16s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.02s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.97s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
                   all         55        256    0.00626      0.469     0.0247    0.00741
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08921 0.05973 0.03139 101 640:   0%|          | 0/19 [00:00<?, ?it/s]8/99 7.25G 0.08921 0.05973 0.03139 101 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08403 0.0446 0.03041 38 640:   5%|▌         | 1/19 [00:00<00:03,  5.63it/s]  8/99 7.25G 0.08403 0.0446 0.03041 38 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08401 0.04324 0.03086 58 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]8/99 7.25G 0.08401 0.04324 0.03086 58 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08354 0.04159 0.03057 53 640:  16%|█▌        | 3/19 [00:00<00:02,  5.71it/s]8/99 7.25G 0.08354 0.04159 0.03057 53 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.083 0.04133 0.03037 59 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]  8/99 7.25G 0.083 0.04133 0.03037 59 640:  26%|██▋       | 5/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08588 0.04458 0.03016 146 640:  26%|██▋       | 5/19 [00:01<00:02,  5.72it/s]8/99 7.25G 0.08588 0.04458 0.03016 146 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08626 0.04488 0.03036 76 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s] 8/99 7.25G 0.08626 0.04488 0.03036 76 640:  37%|███▋      | 7/19 [00:01<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08623 0.04418 0.03015 66 640:  37%|███▋      | 7/19 [00:01<00:02,  5.70it/s]8/99 7.25G 0.08623 0.04418 0.03015 66 640:  42%|████▏     | 8/19 [00:01<00:01,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08703 0.05119 0.03011 209 640:  42%|████▏     | 8/19 [00:08<00:01,  5.71it/s]8/99 7.25G 0.08703 0.05119 0.03011 209 640:  47%|████▋     | 9/19 [00:08<00:22,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08648 0.04908 0.02989 45 640:  47%|████▋     | 9/19 [00:08<00:22,  2.26s/it] 8/99 7.25G 0.08648 0.04908 0.02989 45 640:  53%|█████▎    | 10/19 [00:08<00:15,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08629 0.0487 0.02988 68 640:  53%|█████▎    | 10/19 [00:09<00:15,  1.72s/it] 8/99 7.25G 0.08629 0.0487 0.02988 68 640:  58%|█████▊    | 11/19 [00:09<00:11,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08719 0.05003 0.02987 153 640:  58%|█████▊    | 11/19 [00:10<00:11,  1.38s/it]8/99 7.25G 0.08719 0.05003 0.02987 153 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08713 0.04927 0.03009 68 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.19s/it] 8/99 7.25G 0.08713 0.04927 0.03009 68 640:  68%|██████▊   | 13/19 [00:10<00:05,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08718 0.04973 0.03013 88 640:  68%|██████▊   | 13/19 [00:11<00:05,  1.01it/s]8/99 7.25G 0.08718 0.04973 0.03013 88 640:  74%|███████▎  | 14/19 [00:11<00:04,  1.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08722 0.04926 0.0301 69 640:  74%|███████▎  | 14/19 [00:11<00:04,  1.16it/s] 8/99 7.25G 0.08722 0.04926 0.0301 69 640:  79%|███████▉  | 15/19 [00:11<00:03,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08729 0.04901 0.02998 76 640:  79%|███████▉  | 15/19 [00:14<00:03,  1.31it/s]8/99 7.25G 0.08729 0.04901 0.02998 76 640:  84%|████████▍ | 16/19 [00:14<00:03,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08764 0.04946 0.02996 111 640:  84%|████████▍ | 16/19 [00:18<00:03,  1.23s/it]8/99 7.25G 0.08764 0.04946 0.02996 111 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08754 0.04917 0.02992 69 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.15s/it] 8/99 7.25G 0.08754 0.04917 0.02992 69 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08721 0.04814 0.02981 41 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.61s/it]8/99 7.25G 0.08721 0.04814 0.02981 41 640: 100%|██████████| 19/19 [00:18<00:00,  1.22s/it]8/99 7.25G 0.08721 0.04814 0.02981 41 640: 100%|██████████| 19/19 [00:18<00:00,  1.00it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.18s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.03s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
                   all         55        256    0.00668       0.52     0.0337    0.00835
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08551 0.04634 0.02971 75 640:   0%|          | 0/19 [00:02<?, ?it/s]9/99 7.26G 0.08551 0.04634 0.02971 75 640:   5%|▌         | 1/19 [00:02<00:51,  2.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.0857 0.06057 0.03026 121 640:   5%|▌         | 1/19 [00:03<00:51,  2.84s/it]9/99 7.26G 0.0857 0.06057 0.03026 121 640:  11%|█         | 2/19 [00:03<00:25,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.0862 0.05636 0.02995 80 640:  11%|█         | 2/19 [00:03<00:25,  1.48s/it] 9/99 7.26G 0.0862 0.05636 0.02995 80 640:  16%|█▌        | 3/19 [00:03<00:17,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08557 0.05221 0.02991 58 640:  16%|█▌        | 3/19 [00:04<00:17,  1.07s/it]9/99 7.26G 0.08557 0.05221 0.02991 58 640:  21%|██        | 4/19 [00:04<00:13,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08645 0.04868 0.03017 58 640:  21%|██        | 4/19 [00:05<00:13,  1.08it/s]9/99 7.26G 0.08645 0.04868 0.03017 58 640:  26%|██▋       | 5/19 [00:05<00:11,  1.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08518 0.04793 0.0299 62 640:  26%|██▋       | 5/19 [00:06<00:11,  1.18it/s] 9/99 7.26G 0.08518 0.04793 0.0299 62 640:  32%|███▏      | 6/19 [00:06<00:10,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08589 0.04923 0.0294 100 640:  32%|███▏      | 6/19 [00:06<00:10,  1.26it/s]9/99 7.26G 0.08589 0.04923 0.0294 100 640:  37%|███▋      | 7/19 [00:06<00:09,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08662 0.04906 0.02921 99 640:  37%|███▋      | 7/19 [00:07<00:09,  1.31it/s]9/99 7.26G 0.08662 0.04906 0.02921 99 640:  42%|████▏     | 8/19 [00:07<00:08,  1.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08641 0.04742 0.0295 53 640:  42%|████▏     | 8/19 [00:08<00:08,  1.35it/s] 9/99 7.26G 0.08641 0.04742 0.0295 53 640:  47%|████▋     | 9/19 [00:08<00:07,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08573 0.04565 0.02958 42 640:  47%|████▋     | 9/19 [00:08<00:07,  1.38it/s]9/99 7.26G 0.08573 0.04565 0.02958 42 640:  53%|█████▎    | 10/19 [00:08<00:06,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08562 0.04469 0.02964 50 640:  53%|█████▎    | 10/19 [00:09<00:06,  1.45it/s]9/99 7.26G 0.08562 0.04469 0.02964 50 640:  58%|█████▊    | 11/19 [00:09<00:04,  1.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08514 0.04516 0.02993 72 640:  58%|█████▊    | 11/19 [00:09<00:04,  1.71it/s]9/99 7.26G 0.08514 0.04516 0.02993 72 640:  63%|██████▎   | 12/19 [00:09<00:03,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08497 0.04695 0.03011 108 640:  63%|██████▎   | 12/19 [00:09<00:03,  1.94it/s]9/99 7.26G 0.08497 0.04695 0.03011 108 640:  68%|██████▊   | 13/19 [00:09<00:02,  2.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08554 0.04681 0.03018 74 640:  68%|██████▊   | 13/19 [00:10<00:02,  2.13it/s] 9/99 7.26G 0.08554 0.04681 0.03018 74 640:  74%|███████▎  | 14/19 [00:10<00:02,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08583 0.04769 0.03003 105 640:  74%|███████▎  | 14/19 [00:10<00:02,  2.29it/s]9/99 7.26G 0.08583 0.04769 0.03003 105 640:  79%|███████▉  | 15/19 [00:10<00:01,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08611 0.04748 0.02982 81 640:  79%|███████▉  | 15/19 [00:10<00:01,  2.44it/s] 9/99 7.26G 0.08611 0.04748 0.02982 81 640:  84%|████████▍ | 16/19 [00:10<00:01,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08589 0.04683 0.02965 54 640:  84%|████████▍ | 16/19 [00:10<00:01,  2.95it/s]9/99 7.26G 0.08589 0.04683 0.02965 54 640:  89%|████████▉ | 17/19 [00:10<00:00,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08608 0.04617 0.02959 58 640:  89%|████████▉ | 17/19 [00:11<00:00,  3.46it/s]9/99 7.26G 0.08608 0.04617 0.02959 58 640:  95%|█████████▍| 18/19 [00:11<00:00,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08615 0.04605 0.02951 70 640:  95%|█████████▍| 18/19 [00:11<00:00,  3.92it/s]9/99 7.26G 0.08615 0.04605 0.02951 70 640: 100%|██████████| 19/19 [00:11<00:00,  4.32it/s]9/99 7.26G 0.08615 0.04605 0.02951 70 640: 100%|██████████| 19/19 [00:11<00:00,  1.69it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.08s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]
                   all         55        256    0.00623       0.48     0.0309    0.00837
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08394 0.04744 0.02806 65 640:   0%|          | 0/19 [00:02<?, ?it/s]10/99 7.28G 0.08394 0.04744 0.02806 65 640:   5%|▌         | 1/19 [00:02<00:43,  2.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.0848 0.04102 0.02697 53 640:   5%|▌         | 1/19 [00:03<00:43,  2.42s/it] 10/99 7.28G 0.0848 0.04102 0.02697 53 640:  11%|█         | 2/19 [00:03<00:23,  1.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08433 0.03828 0.02737 51 640:  11%|█         | 2/19 [00:03<00:23,  1.41s/it]10/99 7.28G 0.08433 0.03828 0.02737 51 640:  16%|█▌        | 3/19 [00:03<00:17,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08249 0.03923 0.0281 55 640:  16%|█▌        | 3/19 [00:04<00:17,  1.08s/it] 10/99 7.28G 0.08249 0.03923 0.0281 55 640:  21%|██        | 4/19 [00:04<00:13,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08284 0.04028 0.02801 74 640:  21%|██        | 4/19 [00:05<00:13,  1.08it/s]10/99 7.28G 0.08284 0.04028 0.02801 74 640:  26%|██▋       | 5/19 [00:05<00:10,  1.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08427 0.04075 0.02843 78 640:  26%|██▋       | 5/19 [00:05<00:10,  1.27it/s]10/99 7.28G 0.08427 0.04075 0.02843 78 640:  32%|███▏      | 6/19 [00:05<00:09,  1.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08477 0.04151 0.02841 76 640:  32%|███▏      | 6/19 [00:06<00:09,  1.32it/s]10/99 7.28G 0.08477 0.04151 0.02841 76 640:  37%|███▋      | 7/19 [00:06<00:08,  1.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08413 0.04014 0.02822 43 640:  37%|███▋      | 7/19 [00:07<00:08,  1.36it/s]10/99 7.28G 0.08413 0.04014 0.02822 43 640:  42%|████▏     | 8/19 [00:07<00:07,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08411 0.04005 0.02878 61 640:  42%|████▏     | 8/19 [00:07<00:07,  1.39it/s]10/99 7.28G 0.08411 0.04005 0.02878 61 640:  47%|████▋     | 9/19 [00:07<00:06,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08438 0.04007 0.02876 63 640:  47%|████▋     | 9/19 [00:07<00:06,  1.66it/s]10/99 7.28G 0.08438 0.04007 0.02876 63 640:  53%|█████▎    | 10/19 [00:07<00:04,  1.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08486 0.04004 0.02894 58 640:  53%|█████▎    | 10/19 [00:08<00:04,  1.91it/s]10/99 7.28G 0.08486 0.04004 0.02894 58 640:  58%|█████▊    | 11/19 [00:08<00:03,  2.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08576 0.04159 0.02878 148 640:  58%|█████▊    | 11/19 [00:08<00:03,  2.13it/s]10/99 7.28G 0.08576 0.04159 0.02878 148 640:  63%|██████▎   | 12/19 [00:08<00:03,  2.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08511 0.04172 0.02891 61 640:  63%|██████▎   | 12/19 [00:08<00:03,  2.31it/s] 10/99 7.28G 0.08511 0.04172 0.02891 61 640:  68%|██████▊   | 13/19 [00:08<00:02,  2.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08544 0.04237 0.02887 94 640:  68%|██████▊   | 13/19 [00:09<00:02,  2.45it/s]10/99 7.28G 0.08544 0.04237 0.02887 94 640:  74%|███████▎  | 14/19 [00:09<00:01,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08521 0.04137 0.02892 38 640:  74%|███████▎  | 14/19 [00:09<00:01,  2.57it/s]10/99 7.28G 0.08521 0.04137 0.02892 38 640:  79%|███████▉  | 15/19 [00:09<00:01,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08487 0.04101 0.02913 52 640:  79%|███████▉  | 15/19 [00:09<00:01,  2.65it/s]10/99 7.28G 0.08487 0.04101 0.02913 52 640:  84%|████████▍ | 16/19 [00:09<00:01,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08465 0.04054 0.02934 46 640:  84%|████████▍ | 16/19 [00:09<00:01,  2.95it/s]10/99 7.28G 0.08465 0.04054 0.02934 46 640:  89%|████████▉ | 17/19 [00:09<00:00,  3.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08497 0.04107 0.02928 92 640:  89%|████████▉ | 17/19 [00:10<00:00,  3.45it/s]10/99 7.28G 0.08497 0.04107 0.02928 92 640:  95%|█████████▍| 18/19 [00:10<00:00,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08518 0.042 0.02925 107 640:  95%|█████████▍| 18/19 [00:10<00:00,  3.93it/s] 10/99 7.28G 0.08518 0.042 0.02925 107 640: 100%|██████████| 19/19 [00:10<00:00,  4.33it/s]10/99 7.28G 0.08518 0.042 0.02925 107 640: 100%|██████████| 19/19 [00:10<00:00,  1.84it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.61s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]
                   all         55        256    0.00691      0.563     0.0461     0.0116
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08099 0.03708 0.03001 52 640:   0%|          | 0/19 [00:00<?, ?it/s]11/99 7.28G 0.08099 0.03708 0.03001 52 640:   5%|▌         | 1/19 [00:00<00:06,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08074 0.03531 0.028 48 640:   5%|▌         | 1/19 [00:00<00:06,  2.86it/s]  11/99 7.28G 0.08074 0.03531 0.028 48 640:  11%|█         | 2/19 [00:00<00:05,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.083 0.0426 0.02875 92 640:  11%|█         | 2/19 [00:01<00:05,  2.87it/s] 11/99 7.28G 0.083 0.0426 0.02875 92 640:  16%|█▌        | 3/19 [00:01<00:05,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.0839 0.03872 0.02928 41 640:  16%|█▌        | 3/19 [00:01<00:05,  2.83it/s]11/99 7.28G 0.0839 0.03872 0.02928 41 640:  21%|██        | 4/19 [00:01<00:06,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08433 0.03928 0.02952 66 640:  21%|██        | 4/19 [00:02<00:06,  2.38it/s]11/99 7.28G 0.08433 0.03928 0.02952 66 640:  26%|██▋       | 5/19 [00:02<00:06,  2.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08456 0.04163 0.02905 91 640:  26%|██▋       | 5/19 [00:02<00:06,  2.16it/s]11/99 7.28G 0.08456 0.04163 0.02905 91 640:  32%|███▏      | 6/19 [00:02<00:06,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08467 0.04431 0.02923 95 640:  32%|███▏      | 6/19 [00:02<00:06,  2.10it/s]11/99 7.28G 0.08467 0.04431 0.02923 95 640:  37%|███▋      | 7/19 [00:02<00:05,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08522 0.04614 0.02907 106 640:  37%|███▋      | 7/19 [00:03<00:05,  2.32it/s]11/99 7.28G 0.08522 0.04614 0.02907 106 640:  42%|████▏     | 8/19 [00:03<00:04,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08518 0.04634 0.02891 73 640:  42%|████▏     | 8/19 [00:03<00:04,  2.47it/s] 11/99 7.28G 0.08518 0.04634 0.02891 73 640:  47%|████▋     | 9/19 [00:03<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08507 0.0465 0.02895 71 640:  47%|████▋     | 9/19 [00:04<00:03,  2.57it/s] 11/99 7.28G 0.08507 0.0465 0.02895 71 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08503 0.04805 0.02879 101 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.66it/s]11/99 7.28G 0.08503 0.04805 0.02879 101 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08491 0.04691 0.02917 49 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.72it/s] 11/99 7.28G 0.08491 0.04691 0.02917 49 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08497 0.04719 0.02932 77 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.76it/s]11/99 7.28G 0.08497 0.04719 0.02932 77 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08504 0.04725 0.0291 79 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.79it/s] 11/99 7.28G 0.08504 0.04725 0.0291 79 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08486 0.04606 0.02881 42 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.81it/s]11/99 7.28G 0.08486 0.04606 0.02881 42 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08503 0.04632 0.02878 91 640:  79%|███████▉  | 15/19 [00:06<00:01,  2.75it/s]11/99 7.28G 0.08503 0.04632 0.02878 91 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08498 0.0466 0.02894 79 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.73it/s] 11/99 7.28G 0.08498 0.0466 0.02894 79 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08465 0.04692 0.02894 80 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.93it/s]11/99 7.28G 0.08465 0.04692 0.02894 80 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.28G 0.08478 0.04662 0.02881 75 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.33it/s]11/99 7.28G 0.08478 0.04662 0.02881 75 640: 100%|██████████| 19/19 [00:06<00:00,  3.67it/s]11/99 7.28G 0.08478 0.04662 0.02881 75 640: 100%|██████████| 19/19 [00:06<00:00,  2.77it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.79s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.20s/it]
                   all         55        256    0.00673      0.542     0.0338    0.00921
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08963 0.0535 0.02947 100 640:   0%|          | 0/19 [00:00<?, ?it/s]12/99 7.28G 0.08963 0.0535 0.02947 100 640:   5%|▌         | 1/19 [00:00<00:05,  3.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08651 0.05018 0.02933 74 640:   5%|▌         | 1/19 [00:00<00:05,  3.13it/s]12/99 7.28G 0.08651 0.05018 0.02933 74 640:  11%|█         | 2/19 [00:00<00:05,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.0847 0.05117 0.02974 76 640:  11%|█         | 2/19 [00:01<00:05,  2.95it/s] 12/99 7.28G 0.0847 0.05117 0.02974 76 640:  16%|█▌        | 3/19 [00:01<00:06,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08412 0.04794 0.02907 58 640:  16%|█▌        | 3/19 [00:01<00:06,  2.36it/s]12/99 7.28G 0.08412 0.04794 0.02907 58 640:  21%|██        | 4/19 [00:01<00:07,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08382 0.04785 0.02826 75 640:  21%|██        | 4/19 [00:02<00:07,  2.14it/s]12/99 7.28G 0.08382 0.04785 0.02826 75 640:  26%|██▋       | 5/19 [00:02<00:06,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08326 0.04703 0.02901 60 640:  26%|██▋       | 5/19 [00:02<00:06,  2.06it/s]12/99 7.28G 0.08326 0.04703 0.02901 60 640:  32%|███▏      | 6/19 [00:02<00:05,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08369 0.04861 0.02885 93 640:  32%|███▏      | 6/19 [00:02<00:05,  2.26it/s]12/99 7.28G 0.08369 0.04861 0.02885 93 640:  37%|███▋      | 7/19 [00:02<00:04,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08505 0.05025 0.0289 124 640:  37%|███▋      | 7/19 [00:03<00:04,  2.44it/s]12/99 7.28G 0.08505 0.05025 0.0289 124 640:  42%|████▏     | 8/19 [00:03<00:04,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08381 0.04821 0.02925 41 640:  42%|████▏     | 8/19 [00:03<00:04,  2.57it/s]12/99 7.28G 0.08381 0.04821 0.02925 41 640:  47%|████▋     | 9/19 [00:03<00:03,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08393 0.04927 0.02903 93 640:  47%|████▋     | 9/19 [00:03<00:03,  2.67it/s]12/99 7.28G 0.08393 0.04927 0.02903 93 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08445 0.05222 0.02901 159 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.74it/s]12/99 7.28G 0.08445 0.05222 0.02901 159 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.085 0.05268 0.02902 105 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s]  12/99 7.28G 0.085 0.05268 0.02902 105 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08468 0.05282 0.02905 81 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.81it/s]12/99 7.28G 0.08468 0.05282 0.02905 81 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08438 0.05189 0.02866 61 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.83it/s]12/99 7.28G 0.08438 0.05189 0.02866 61 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08498 0.05305 0.02839 164 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.85it/s]12/99 7.28G 0.08498 0.05305 0.02839 164 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08499 0.05247 0.02844 68 640:  79%|███████▉  | 15/19 [00:06<00:01,  2.86it/s] 12/99 7.28G 0.08499 0.05247 0.02844 68 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08465 0.052 0.02852 66 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.87it/s]  12/99 7.28G 0.08465 0.052 0.02852 66 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08433 0.05127 0.02856 54 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.93it/s]12/99 7.28G 0.08433 0.05127 0.02856 54 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.28G 0.08434 0.05169 0.02848 94 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.32it/s]12/99 7.28G 0.08434 0.05169 0.02848 94 640: 100%|██████████| 19/19 [00:06<00:00,  3.66it/s]12/99 7.28G 0.08434 0.05169 0.02848 94 640: 100%|██████████| 19/19 [00:06<00:00,  2.79it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.85s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]
                   all         55        256    0.00693      0.514     0.0388    0.00954
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08749 0.04497 0.03227 76 640:   0%|          | 0/19 [00:00<?, ?it/s]13/99 7.28G 0.08749 0.04497 0.03227 76 640:   5%|▌         | 1/19 [00:00<00:08,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08804 0.05424 0.02992 113 640:   5%|▌         | 1/19 [00:00<00:08,  2.18it/s]13/99 7.28G 0.08804 0.05424 0.02992 113 640:  11%|█         | 2/19 [00:00<00:08,  2.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08755 0.05632 0.02956 111 640:  11%|█         | 2/19 [00:01<00:08,  2.01it/s]13/99 7.28G 0.08755 0.05632 0.02956 111 640:  16%|█▌        | 3/19 [00:01<00:08,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08603 0.05056 0.02858 51 640:  16%|█▌        | 3/19 [00:01<00:08,  1.97it/s] 13/99 7.28G 0.08603 0.05056 0.02858 51 640:  21%|██        | 4/19 [00:01<00:07,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08569 0.05 0.02879 74 640:  21%|██        | 4/19 [00:02<00:07,  2.10it/s]   13/99 7.28G 0.08569 0.05 0.02879 74 640:  26%|██▋       | 5/19 [00:02<00:06,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08448 0.04821 0.02825 56 640:  26%|██▋       | 5/19 [00:02<00:06,  2.30it/s]13/99 7.28G 0.08448 0.04821 0.02825 56 640:  32%|███▏      | 6/19 [00:02<00:05,  2.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08498 0.04783 0.0286 75 640:  32%|███▏      | 6/19 [00:02<00:05,  2.46it/s] 13/99 7.28G 0.08498 0.04783 0.0286 75 640:  37%|███▋      | 7/19 [00:02<00:04,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08453 0.04608 0.0292 47 640:  37%|███▋      | 7/19 [00:03<00:04,  2.57it/s]13/99 7.28G 0.08453 0.04608 0.0292 47 640:  42%|████▏     | 8/19 [00:03<00:04,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.0843 0.0447 0.02889 53 640:  42%|████▏     | 8/19 [00:03<00:04,  2.63it/s] 13/99 7.28G 0.0843 0.0447 0.02889 53 640:  47%|████▋     | 9/19 [00:03<00:03,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08412 0.0434 0.0286 48 640:  47%|████▋     | 9/19 [00:04<00:03,  2.69it/s]13/99 7.28G 0.08412 0.0434 0.0286 48 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08439 0.0458 0.02846 121 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.73it/s]13/99 7.28G 0.08439 0.0458 0.02846 121 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08478 0.04552 0.02833 81 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.75it/s]13/99 7.28G 0.08478 0.04552 0.02833 81 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08475 0.04464 0.02846 52 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.77it/s]13/99 7.28G 0.08475 0.04464 0.02846 52 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08439 0.0445 0.02838 64 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.19it/s] 13/99 7.28G 0.08439 0.0445 0.02838 64 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08462 0.04532 0.02841 100 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.55it/s]13/99 7.28G 0.08462 0.04532 0.02841 100 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08463 0.04584 0.02836 83 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.86it/s] 13/99 7.28G 0.08463 0.04584 0.02836 83 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08407 0.04558 0.02837 58 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.11it/s]13/99 7.28G 0.08407 0.04558 0.02837 58 640:  89%|████████▉ | 17/19 [00:05<00:00,  4.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.08429 0.04644 0.02842 108 640:  89%|████████▉ | 17/19 [00:06<00:00,  4.31it/s]13/99 7.28G 0.08429 0.04644 0.02842 108 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.28G 0.0842 0.04688 0.0284 83 640:  95%|█████████▍| 18/19 [00:07<00:00,  3.54it/s]   13/99 7.28G 0.0842 0.04688 0.0284 83 640: 100%|██████████| 19/19 [00:07<00:00,  1.61it/s]13/99 7.28G 0.0842 0.04688 0.0284 83 640: 100%|██████████| 19/19 [00:07<00:00,  2.49it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.53s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]
                   all         55        256    0.00777      0.563     0.0362    0.00936
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08131 0.04333 0.02733 68 640:   0%|          | 0/19 [00:00<?, ?it/s]14/99 7.28G 0.08131 0.04333 0.02733 68 640:   5%|▌         | 1/19 [00:00<00:07,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07912 0.04624 0.02605 75 640:   5%|▌         | 1/19 [00:00<00:07,  2.32it/s]14/99 7.28G 0.07912 0.04624 0.02605 75 640:  11%|█         | 2/19 [00:00<00:08,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07802 0.04766 0.02648 71 640:  11%|█         | 2/19 [00:01<00:08,  2.08it/s]14/99 7.28G 0.07802 0.04766 0.02648 71 640:  16%|█▌        | 3/19 [00:01<00:07,  2.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.0792 0.0458 0.02721 64 640:  16%|█▌        | 3/19 [00:01<00:07,  2.01it/s]  14/99 7.28G 0.0792 0.0458 0.02721 64 640:  21%|██        | 4/19 [00:01<00:07,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.07938 0.04452 0.02725 60 640:  21%|██        | 4/19 [00:02<00:07,  2.08it/s]14/99 7.28G 0.07938 0.04452 0.02725 60 640:  26%|██▋       | 5/19 [00:02<00:06,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08 0.04461 0.02699 75 640:  26%|██▋       | 5/19 [00:02<00:06,  2.30it/s]   14/99 7.28G 0.08 0.04461 0.02699 75 640:  32%|███▏      | 6/19 [00:02<00:05,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08119 0.04479 0.02745 72 640:  32%|███▏      | 6/19 [00:02<00:05,  2.47it/s]14/99 7.28G 0.08119 0.04479 0.02745 72 640:  37%|███▋      | 7/19 [00:02<00:04,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08151 0.04543 0.02764 81 640:  37%|███▋      | 7/19 [00:03<00:04,  2.58it/s]14/99 7.28G 0.08151 0.04543 0.02764 81 640:  42%|████▏     | 8/19 [00:03<00:04,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08204 0.04463 0.0273 67 640:  42%|████▏     | 8/19 [00:03<00:04,  2.66it/s] 14/99 7.28G 0.08204 0.04463 0.0273 67 640:  47%|████▋     | 9/19 [00:03<00:03,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08201 0.04453 0.02765 64 640:  47%|████▋     | 9/19 [00:04<00:03,  2.70it/s]14/99 7.28G 0.08201 0.04453 0.02765 64 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08222 0.04549 0.02771 93 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.74it/s]14/99 7.28G 0.08222 0.04549 0.02771 93 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08274 0.04581 0.02777 89 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s]14/99 7.28G 0.08274 0.04581 0.02777 89 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08322 0.04539 0.02776 72 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.88it/s]14/99 7.28G 0.08322 0.04539 0.02776 72 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08364 0.04711 0.02785 117 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.38it/s]14/99 7.28G 0.08364 0.04711 0.02785 117 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08338 0.04655 0.02786 60 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.86it/s] 14/99 7.28G 0.08338 0.04655 0.02786 60 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.0832 0.04667 0.02782 73 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.28it/s] 14/99 7.28G 0.0832 0.04667 0.02782 73 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.0826 0.0459 0.02773 47 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.43it/s] 14/99 7.28G 0.0826 0.0459 0.02773 47 640:  89%|████████▉ | 17/19 [00:05<00:00,  3.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08265 0.04551 0.02767 65 640:  89%|████████▉ | 17/19 [00:06<00:00,  3.51it/s]14/99 7.28G 0.08265 0.04551 0.02767 65 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.28G 0.08243 0.04475 0.02785 43 640:  95%|█████████▍| 18/19 [00:07<00:00,  2.87it/s]14/99 7.28G 0.08243 0.04475 0.02785 43 640: 100%|██████████| 19/19 [00:07<00:00,  1.91it/s]14/99 7.28G 0.08243 0.04475 0.02785 43 640: 100%|██████████| 19/19 [00:07<00:00,  2.61it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.48s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.80s/it]
                   all         55        256    0.00709      0.575      0.043     0.0127
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08208 0.05211 0.02556 82 640:   0%|          | 0/19 [00:00<?, ?it/s]15/99 7.28G 0.08208 0.05211 0.02556 82 640:   5%|▌         | 1/19 [00:00<00:09,  1.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.07916 0.04849 0.02488 67 640:   5%|▌         | 1/19 [00:01<00:09,  1.91it/s]15/99 7.28G 0.07916 0.04849 0.02488 67 640:  11%|█         | 2/19 [00:01<00:08,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08042 0.05192 0.02594 90 640:  11%|█         | 2/19 [00:01<00:08,  1.92it/s]15/99 7.28G 0.08042 0.05192 0.02594 90 640:  16%|█▌        | 3/19 [00:01<00:08,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08166 0.05416 0.02659 105 640:  16%|█▌        | 3/19 [00:01<00:08,  1.93it/s]15/99 7.28G 0.08166 0.05416 0.02659 105 640:  21%|██        | 4/19 [00:01<00:06,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08273 0.0528 0.0262 87 640:  21%|██        | 4/19 [00:02<00:06,  2.21it/s]   15/99 7.28G 0.08273 0.0528 0.0262 87 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08235 0.04969 0.02602 55 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s]15/99 7.28G 0.08235 0.04969 0.02602 55 640:  32%|███▏      | 6/19 [00:02<00:05,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08185 0.04757 0.02643 58 640:  32%|███▏      | 6/19 [00:02<00:05,  2.58it/s]15/99 7.28G 0.08185 0.04757 0.02643 58 640:  37%|███▋      | 7/19 [00:02<00:04,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08137 0.04605 0.02684 51 640:  37%|███▋      | 7/19 [00:03<00:04,  2.67it/s]15/99 7.28G 0.08137 0.04605 0.02684 51 640:  42%|████▏     | 8/19 [00:03<00:04,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08281 0.04665 0.02674 118 640:  42%|████▏     | 8/19 [00:03<00:04,  2.75it/s]15/99 7.28G 0.08281 0.04665 0.02674 118 640:  47%|████▋     | 9/19 [00:03<00:03,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08293 0.04607 0.02696 64 640:  47%|████▋     | 9/19 [00:04<00:03,  2.72it/s] 15/99 7.28G 0.08293 0.04607 0.02696 64 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08333 0.04795 0.02704 113 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.77it/s]15/99 7.28G 0.08333 0.04795 0.02704 113 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08288 0.04795 0.02735 70 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s] 15/99 7.28G 0.08288 0.04795 0.02735 70 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08367 0.04932 0.0272 139 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.07it/s]15/99 7.28G 0.08367 0.04932 0.0272 139 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08385 0.04927 0.02738 83 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.57it/s]15/99 7.28G 0.08385 0.04927 0.02738 83 640:  74%|███████▎  | 14/19 [00:04<00:01,  4.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.0838 0.04945 0.02748 85 640:  74%|███████▎  | 14/19 [00:05<00:01,  4.03it/s] 15/99 7.28G 0.0838 0.04945 0.02748 85 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08326 0.04814 0.02737 39 640:  79%|███████▉  | 15/19 [00:05<00:00,  4.19it/s]15/99 7.28G 0.08326 0.04814 0.02737 39 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08307 0.04825 0.02745 75 640:  84%|████████▍ | 16/19 [00:06<00:00,  4.37it/s]15/99 7.28G 0.08307 0.04825 0.02745 75 640:  89%|████████▉ | 17/19 [00:06<00:01,  1.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.08295 0.04773 0.02749 58 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.58it/s]15/99 7.28G 0.08295 0.04773 0.02749 58 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.28G 0.0828 0.04634 0.02711 31 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.71it/s] 15/99 7.28G 0.0828 0.04634 0.02711 31 640: 100%|██████████| 19/19 [00:08<00:00,  1.41it/s]15/99 7.28G 0.0828 0.04634 0.02711 31 640: 100%|██████████| 19/19 [00:08<00:00,  2.26it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.24s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]
                   all         55        256    0.00792      0.573     0.0424      0.013
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.07615 0.03 0.02898 42 640:   0%|          | 0/19 [00:00<?, ?it/s]16/99 7.28G 0.07615 0.03 0.02898 42 640:   5%|▌         | 1/19 [00:00<00:08,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08162 0.04806 0.02762 125 640:   5%|▌         | 1/19 [00:00<00:08,  2.04it/s]16/99 7.28G 0.08162 0.04806 0.02762 125 640:  11%|█         | 2/19 [00:00<00:07,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08439 0.04773 0.02771 90 640:  11%|█         | 2/19 [00:01<00:07,  2.26it/s] 16/99 7.28G 0.08439 0.04773 0.02771 90 640:  16%|█▌        | 3/19 [00:01<00:06,  2.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08473 0.05029 0.02756 103 640:  16%|█▌        | 3/19 [00:01<00:06,  2.46it/s]16/99 7.28G 0.08473 0.05029 0.02756 103 640:  21%|██        | 4/19 [00:01<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08359 0.05312 0.02695 101 640:  21%|██        | 4/19 [00:01<00:05,  2.59it/s]16/99 7.28G 0.08359 0.05312 0.02695 101 640:  26%|██▋       | 5/19 [00:01<00:04,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08262 0.05127 0.02775 58 640:  26%|██▋       | 5/19 [00:01<00:04,  3.23it/s] 16/99 7.28G 0.08262 0.05127 0.02775 58 640:  32%|███▏      | 6/19 [00:01<00:03,  3.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08343 0.05318 0.02772 117 640:  32%|███▏      | 6/19 [00:02<00:03,  3.80it/s]16/99 7.28G 0.08343 0.05318 0.02772 117 640:  37%|███▋      | 7/19 [00:02<00:02,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08292 0.05036 0.02793 44 640:  37%|███▋      | 7/19 [00:02<00:02,  4.27it/s] 16/99 7.28G 0.08292 0.05036 0.02793 44 640:  42%|████▏     | 8/19 [00:02<00:02,  4.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.0831 0.04933 0.02769 67 640:  42%|████▏     | 8/19 [00:02<00:02,  4.65it/s] 16/99 7.28G 0.0831 0.04933 0.02769 67 640:  47%|████▋     | 9/19 [00:02<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08299 0.04729 0.02753 46 640:  47%|████▋     | 9/19 [00:02<00:02,  4.95it/s]16/99 7.28G 0.08299 0.04729 0.02753 46 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08307 0.05003 0.02739 124 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.62it/s]16/99 7.28G 0.08307 0.05003 0.02739 124 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08309 0.04984 0.02712 85 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.89it/s] 16/99 7.28G 0.08309 0.04984 0.02712 85 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08316 0.04941 0.027 76 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.81it/s]  16/99 7.28G 0.08316 0.04941 0.027 76 640:  68%|██████▊   | 13/19 [00:03<00:01,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08368 0.0482 0.027 59 640:  68%|██████▊   | 13/19 [00:03<00:01,  3.29it/s] 16/99 7.28G 0.08368 0.0482 0.027 59 640:  74%|███████▎  | 14/19 [00:03<00:01,  3.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08341 0.04721 0.02728 52 640:  74%|███████▎  | 14/19 [00:04<00:01,  3.60it/s]16/99 7.28G 0.08341 0.04721 0.02728 52 640:  79%|███████▉  | 15/19 [00:04<00:01,  3.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08327 0.04724 0.02704 85 640:  79%|███████▉  | 15/19 [00:04<00:01,  3.37it/s]16/99 7.28G 0.08327 0.04724 0.02704 85 640:  84%|████████▍ | 16/19 [00:04<00:01,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08329 0.04725 0.02719 78 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.98it/s]16/99 7.28G 0.08329 0.04725 0.02719 78 640:  89%|████████▉ | 17/19 [00:06<00:01,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08345 0.04783 0.02711 104 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.26it/s]16/99 7.28G 0.08345 0.04783 0.02711 104 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.28G 0.08367 0.04801 0.02696 93 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.14it/s] 16/99 7.28G 0.08367 0.04801 0.02696 93 640: 100%|██████████| 19/19 [00:08<00:00,  1.02it/s]16/99 7.28G 0.08367 0.04801 0.02696 93 640: 100%|██████████| 19/19 [00:08<00:00,  2.14it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
                   all         55        256      0.113     0.0932     0.0708     0.0168
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08509 0.03994 0.02932 71 640:   0%|          | 0/19 [00:00<?, ?it/s]17/99 7.28G 0.08509 0.03994 0.02932 71 640:   5%|▌         | 1/19 [00:00<00:08,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08291 0.03674 0.02655 49 640:   5%|▌         | 1/19 [00:00<00:08,  2.21it/s]17/99 7.28G 0.08291 0.03674 0.02655 49 640:  11%|█         | 2/19 [00:00<00:06,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08517 0.04506 0.02574 132 640:  11%|█         | 2/19 [00:00<00:06,  2.64it/s]17/99 7.28G 0.08517 0.04506 0.02574 132 640:  16%|█▌        | 3/19 [00:00<00:04,  3.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08538 0.04934 0.0275 102 640:  16%|█▌        | 3/19 [00:01<00:04,  3.51it/s] 17/99 7.28G 0.08538 0.04934 0.0275 102 640:  21%|██        | 4/19 [00:01<00:03,  4.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08574 0.04894 0.02793 85 640:  21%|██        | 4/19 [00:01<00:03,  4.15it/s]17/99 7.28G 0.08574 0.04894 0.02793 85 640:  26%|██▋       | 5/19 [00:01<00:03,  4.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08607 0.04947 0.02793 93 640:  26%|██▋       | 5/19 [00:01<00:03,  4.59it/s]17/99 7.28G 0.08607 0.04947 0.02793 93 640:  32%|███▏      | 6/19 [00:01<00:02,  4.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08682 0.04851 0.02776 80 640:  32%|███▏      | 6/19 [00:01<00:02,  4.93it/s]17/99 7.28G 0.08682 0.04851 0.02776 80 640:  37%|███▋      | 7/19 [00:01<00:02,  5.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08585 0.04822 0.02764 74 640:  37%|███▋      | 7/19 [00:01<00:02,  5.17it/s]17/99 7.28G 0.08585 0.04822 0.02764 74 640:  42%|████▏     | 8/19 [00:01<00:02,  5.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08522 0.04746 0.02754 62 640:  42%|████▏     | 8/19 [00:02<00:02,  5.32it/s]17/99 7.28G 0.08522 0.04746 0.02754 62 640:  47%|████▋     | 9/19 [00:02<00:01,  5.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08629 0.04831 0.02736 137 640:  47%|████▋     | 9/19 [00:02<00:01,  5.44it/s]17/99 7.28G 0.08629 0.04831 0.02736 137 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08541 0.04654 0.02788 45 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.53it/s] 17/99 7.28G 0.08541 0.04654 0.02788 45 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.0855 0.0466 0.0278 79 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.59it/s]   17/99 7.28G 0.0855 0.0466 0.0278 79 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08565 0.04602 0.02776 70 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.32it/s]17/99 7.28G 0.08565 0.04602 0.02776 70 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08534 0.0457 0.02746 69 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.66it/s] 17/99 7.28G 0.08534 0.0457 0.02746 69 640:  74%|███████▎  | 14/19 [00:03<00:02,  2.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08591 0.04702 0.02744 142 640:  74%|███████▎  | 14/19 [00:04<00:02,  2.22it/s]17/99 7.28G 0.08591 0.04702 0.02744 142 640:  79%|███████▉  | 15/19 [00:04<00:02,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08592 0.047 0.02735 82 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.81it/s]   17/99 7.28G 0.08592 0.047 0.02735 82 640:  84%|████████▍ | 16/19 [00:05<00:02,  1.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08539 0.04646 0.02722 58 640:  84%|████████▍ | 16/19 [00:07<00:02,  1.29it/s]17/99 7.28G 0.08539 0.04646 0.02722 58 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08554 0.04732 0.02723 115 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.09it/s]17/99 7.28G 0.08554 0.04732 0.02723 115 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.28G 0.08522 0.04707 0.0274 65 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.31it/s]  17/99 7.28G 0.08522 0.04707 0.0274 65 640: 100%|██████████| 19/19 [00:08<00:00,  1.13it/s]17/99 7.28G 0.08522 0.04707 0.0274 65 640: 100%|██████████| 19/19 [00:08<00:00,  2.17it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.63s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
                   all         55        256      0.113      0.041     0.0745     0.0188
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08031 0.03245 0.026 50 640:   0%|          | 0/19 [00:00<?, ?it/s]18/99 7.28G 0.08031 0.03245 0.026 50 640:   5%|▌         | 1/19 [00:00<00:06,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08402 0.03064 0.02728 48 640:   5%|▌         | 1/19 [00:00<00:06,  2.85it/s]18/99 7.28G 0.08402 0.03064 0.02728 48 640:  11%|█         | 2/19 [00:00<00:05,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08341 0.03598 0.02744 79 640:  11%|█         | 2/19 [00:01<00:05,  2.84it/s]18/99 7.28G 0.08341 0.03598 0.02744 79 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08244 0.03596 0.02665 53 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]18/99 7.28G 0.08244 0.03596 0.02665 53 640:  21%|██        | 4/19 [00:01<00:05,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08343 0.0385 0.02614 83 640:  21%|██        | 4/19 [00:01<00:05,  2.87it/s] 18/99 7.28G 0.08343 0.0385 0.02614 83 640:  26%|██▋       | 5/19 [00:01<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08244 0.04056 0.02648 74 640:  26%|██▋       | 5/19 [00:02<00:04,  2.86it/s]18/99 7.28G 0.08244 0.04056 0.02648 74 640:  32%|███▏      | 6/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08307 0.0433 0.02657 109 640:  32%|███▏      | 6/19 [00:02<00:04,  2.86it/s]18/99 7.28G 0.08307 0.0433 0.02657 109 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08258 0.04366 0.02686 71 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]18/99 7.28G 0.08258 0.04366 0.02686 71 640:  42%|████▏     | 8/19 [00:02<00:03,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08238 0.04264 0.02704 52 640:  42%|████▏     | 8/19 [00:03<00:03,  2.81it/s]18/99 7.28G 0.08238 0.04264 0.02704 52 640:  47%|████▋     | 9/19 [00:03<00:03,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08221 0.04189 0.02733 51 640:  47%|████▋     | 9/19 [00:03<00:03,  3.11it/s]18/99 7.28G 0.08221 0.04189 0.02733 51 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08282 0.04202 0.02746 79 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.47it/s]18/99 7.28G 0.08282 0.04202 0.02746 79 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08202 0.04144 0.02706 48 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.73it/s]18/99 7.28G 0.08202 0.04144 0.02706 48 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08176 0.04168 0.0269 74 640:  63%|██████▎   | 12/19 [00:04<00:01,  3.74it/s] 18/99 7.28G 0.08176 0.04168 0.0269 74 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08251 0.04267 0.02673 128 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.57it/s]18/99 7.28G 0.08251 0.04267 0.02673 128 640:  74%|███████▎  | 14/19 [00:04<00:01,  3.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.0824 0.04205 0.02663 51 640:  74%|███████▎  | 14/19 [00:04<00:01,  3.52it/s]  18/99 7.28G 0.0824 0.04205 0.02663 51 640:  79%|███████▉  | 15/19 [00:04<00:01,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08282 0.044 0.02632 149 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.23it/s]18/99 7.28G 0.08282 0.044 0.02632 149 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08308 0.04403 0.02645 75 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.39it/s]18/99 7.28G 0.08308 0.04403 0.02645 75 640:  89%|████████▉ | 17/19 [00:06<00:01,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08274 0.04466 0.02622 89 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.31it/s]18/99 7.28G 0.08274 0.04466 0.02622 89 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.28G 0.08266 0.04444 0.02624 63 640:  95%|█████████▍| 18/19 [00:09<00:00,  1.30it/s]18/99 7.28G 0.08266 0.04444 0.02624 63 640: 100%|██████████| 19/19 [00:09<00:00,  1.08it/s]18/99 7.28G 0.08266 0.04444 0.02624 63 640: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.47s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
                   all         55        256      0.123     0.0428     0.0596     0.0159
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.0803 0.03578 0.02955 53 640:   0%|          | 0/19 [00:00<?, ?it/s]19/99 7.28G 0.0803 0.03578 0.02955 53 640:   5%|▌         | 1/19 [00:00<00:06,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08131 0.03653 0.02561 61 640:   5%|▌         | 1/19 [00:00<00:06,  2.87it/s]19/99 7.28G 0.08131 0.03653 0.02561 61 640:  11%|█         | 2/19 [00:00<00:06,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08009 0.03368 0.02577 46 640:  11%|█         | 2/19 [00:01<00:06,  2.78it/s]19/99 7.28G 0.08009 0.03368 0.02577 46 640:  16%|█▌        | 3/19 [00:01<00:05,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08196 0.04467 0.02637 135 640:  16%|█▌        | 3/19 [00:01<00:05,  2.74it/s]19/99 7.28G 0.08196 0.04467 0.02637 135 640:  21%|██        | 4/19 [00:01<00:05,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08179 0.04352 0.02584 62 640:  21%|██        | 4/19 [00:01<00:05,  2.71it/s] 19/99 7.28G 0.08179 0.04352 0.02584 62 640:  26%|██▋       | 5/19 [00:01<00:05,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08055 0.04276 0.02633 56 640:  26%|██▋       | 5/19 [00:02<00:05,  2.73it/s]19/99 7.28G 0.08055 0.04276 0.02633 56 640:  32%|███▏      | 6/19 [00:02<00:04,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08201 0.04417 0.02576 118 640:  32%|███▏      | 6/19 [00:02<00:04,  2.73it/s]19/99 7.28G 0.08201 0.04417 0.02576 118 640:  37%|███▋      | 7/19 [00:02<00:04,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08316 0.04609 0.02551 121 640:  37%|███▋      | 7/19 [00:02<00:04,  2.95it/s]19/99 7.28G 0.08316 0.04609 0.02551 121 640:  42%|████▏     | 8/19 [00:02<00:03,  3.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08336 0.04649 0.02583 83 640:  42%|████▏     | 8/19 [00:02<00:03,  3.42it/s] 19/99 7.28G 0.08336 0.04649 0.02583 83 640:  47%|████▋     | 9/19 [00:02<00:02,  3.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08392 0.04628 0.0262 82 640:  47%|████▋     | 9/19 [00:03<00:02,  3.85it/s] 19/99 7.28G 0.08392 0.04628 0.0262 82 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08389 0.04526 0.02637 55 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.11it/s]19/99 7.28G 0.08389 0.04526 0.02637 55 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08402 0.04526 0.02626 78 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.26it/s]19/99 7.28G 0.08402 0.04526 0.02626 78 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08416 0.0443 0.02622 55 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.42it/s] 19/99 7.28G 0.08416 0.0443 0.02622 55 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08453 0.04441 0.02611 89 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.55it/s]19/99 7.28G 0.08453 0.04441 0.02611 89 640:  74%|███████▎  | 14/19 [00:03<00:01,  4.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08486 0.04437 0.02625 79 640:  74%|███████▎  | 14/19 [00:04<00:01,  4.64it/s]19/99 7.28G 0.08486 0.04437 0.02625 79 640:  79%|███████▉  | 15/19 [00:04<00:00,  4.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08471 0.04393 0.02638 64 640:  79%|███████▉  | 15/19 [00:04<00:00,  4.83it/s]19/99 7.28G 0.08471 0.04393 0.02638 64 640:  84%|████████▍ | 16/19 [00:04<00:00,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08445 0.0436 0.02633 61 640:  84%|████████▍ | 16/19 [00:05<00:00,  4.34it/s] 19/99 7.28G 0.08445 0.0436 0.02633 61 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08454 0.04454 0.0263 101 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.26it/s]19/99 7.28G 0.08454 0.04454 0.0263 101 640:  95%|█████████▍| 18/19 [00:05<00:00,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
19/99 7.28G 0.08403 0.04429 0.02631 57 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.38it/s]19/99 7.28G 0.08403 0.04429 0.02631 57 640: 100%|██████████| 19/19 [00:06<00:00,  2.45it/s]19/99 7.28G 0.08403 0.04429 0.02631 57 640: 100%|██████████| 19/19 [00:06<00:00,  3.14it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.74s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]
                   all         55        256      0.428      0.037      0.056     0.0176
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08402 0.04513 0.02614 75 640:   0%|          | 0/19 [00:00<?, ?it/s]20/99 7.28G 0.08402 0.04513 0.02614 75 640:   5%|▌         | 1/19 [00:00<00:06,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08172 0.04541 0.02888 69 640:   5%|▌         | 1/19 [00:00<00:06,  2.77it/s]20/99 7.28G 0.08172 0.04541 0.02888 69 640:  11%|█         | 2/19 [00:00<00:06,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.07834 0.04274 0.02722 51 640:  11%|█         | 2/19 [00:01<00:06,  2.79it/s]20/99 7.28G 0.07834 0.04274 0.02722 51 640:  16%|█▌        | 3/19 [00:01<00:05,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0816 0.04997 0.02618 141 640:  16%|█▌        | 3/19 [00:01<00:05,  2.69it/s]20/99 7.28G 0.0816 0.04997 0.02618 141 640:  21%|██        | 4/19 [00:01<00:05,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08194 0.05075 0.02567 85 640:  21%|██        | 4/19 [00:01<00:05,  2.67it/s]20/99 7.28G 0.08194 0.05075 0.02567 85 640:  26%|██▋       | 5/19 [00:01<00:05,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08249 0.04876 0.02614 57 640:  26%|██▋       | 5/19 [00:02<00:05,  2.69it/s]20/99 7.28G 0.08249 0.04876 0.02614 57 640:  32%|███▏      | 6/19 [00:02<00:04,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08373 0.05125 0.02624 115 640:  32%|███▏      | 6/19 [00:02<00:04,  2.69it/s]20/99 7.28G 0.08373 0.05125 0.02624 115 640:  37%|███▋      | 7/19 [00:02<00:04,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.0832 0.0505 0.02607 72 640:  37%|███▋      | 7/19 [00:02<00:04,  2.69it/s]   20/99 7.28G 0.0832 0.0505 0.02607 72 640:  42%|████▏     | 8/19 [00:02<00:04,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08412 0.04997 0.02616 107 640:  42%|████▏     | 8/19 [00:03<00:04,  2.68it/s]20/99 7.28G 0.08412 0.04997 0.02616 107 640:  47%|████▋     | 9/19 [00:03<00:03,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08452 0.04929 0.02612 80 640:  47%|████▋     | 9/19 [00:03<00:03,  2.69it/s] 20/99 7.28G 0.08452 0.04929 0.02612 80 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08411 0.04765 0.02572 48 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.69it/s]20/99 7.28G 0.08411 0.04765 0.02572 48 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08372 0.04593 0.02596 43 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.69it/s]20/99 7.28G 0.08372 0.04593 0.02596 43 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08327 0.04533 0.02611 60 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.07it/s]20/99 7.28G 0.08327 0.04533 0.02611 60 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08352 0.04603 0.02615 93 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.45it/s]20/99 7.28G 0.08352 0.04603 0.02615 93 640:  74%|███████▎  | 14/19 [00:04<00:01,  3.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08352 0.04525 0.02604 53 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.58it/s]20/99 7.28G 0.08352 0.04525 0.02604 53 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08367 0.04538 0.0263 78 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.29it/s] 20/99 7.28G 0.08367 0.04538 0.0263 78 640:  84%|████████▍ | 16/19 [00:05<00:00,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08447 0.04648 0.0263 159 640:  84%|████████▍ | 16/19 [00:05<00:00,  3.09it/s]20/99 7.28G 0.08447 0.04648 0.0263 159 640:  89%|████████▉ | 17/19 [00:05<00:00,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08438 0.04721 0.02636 103 640:  89%|████████▉ | 17/19 [00:05<00:00,  3.23it/s]20/99 7.28G 0.08438 0.04721 0.02636 103 640:  95%|█████████▍| 18/19 [00:05<00:00,  3.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
20/99 7.28G 0.08447 0.04612 0.02652 45 640:  95%|█████████▍| 18/19 [00:06<00:00,  3.64it/s] 20/99 7.28G 0.08447 0.04612 0.02652 45 640: 100%|██████████| 19/19 [00:06<00:00,  3.92it/s]20/99 7.28G 0.08447 0.04612 0.02652 45 640: 100%|██████████| 19/19 [00:06<00:00,  3.08it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
                   all         55        256     0.0954     0.0462      0.106     0.0339
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08424 0.07325 0.02548 123 640:   0%|          | 0/19 [00:00<?, ?it/s]21/99 7.28G 0.08424 0.07325 0.02548 123 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08401 0.06438 0.02892 92 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s] 21/99 7.28G 0.08401 0.06438 0.02892 92 640:  11%|█         | 2/19 [00:00<00:06,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08482 0.05584 0.02899 66 640:  11%|█         | 2/19 [00:01<00:06,  2.83it/s]21/99 7.28G 0.08482 0.05584 0.02899 66 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08271 0.0502 0.02846 49 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s] 21/99 7.28G 0.08271 0.0502 0.02846 49 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08406 0.05314 0.02765 120 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]21/99 7.28G 0.08406 0.05314 0.02765 120 640:  26%|██▋       | 5/19 [00:01<00:04,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08396 0.05191 0.02811 70 640:  26%|██▋       | 5/19 [00:02<00:04,  2.85it/s] 21/99 7.28G 0.08396 0.05191 0.02811 70 640:  32%|███▏      | 6/19 [00:02<00:04,  2.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08396 0.05029 0.02761 67 640:  32%|███▏      | 6/19 [00:02<00:04,  2.89it/s]21/99 7.28G 0.08396 0.05029 0.02761 67 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08417 0.04903 0.02688 78 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]21/99 7.28G 0.08417 0.04903 0.02688 78 640:  42%|████▏     | 8/19 [00:02<00:03,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08423 0.04702 0.02648 47 640:  42%|████▏     | 8/19 [00:03<00:03,  2.85it/s]21/99 7.28G 0.08423 0.04702 0.02648 47 640:  47%|████▋     | 9/19 [00:03<00:03,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08418 0.04744 0.02606 95 640:  47%|████▋     | 9/19 [00:03<00:03,  2.86it/s]21/99 7.28G 0.08418 0.04744 0.02606 95 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08434 0.0492 0.02593 113 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.74it/s]21/99 7.28G 0.08434 0.0492 0.02593 113 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08419 0.04928 0.02608 82 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.70it/s]21/99 7.28G 0.08419 0.04928 0.02608 82 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08413 0.04911 0.02624 81 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.71it/s]21/99 7.28G 0.08413 0.04911 0.02624 81 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.0832 0.04744 0.02588 38 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.70it/s] 21/99 7.28G 0.0832 0.04744 0.02588 38 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.0841 0.04953 0.02588 197 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.71it/s]21/99 7.28G 0.0841 0.04953 0.02588 197 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08452 0.0506 0.02602 142 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.68it/s]21/99 7.28G 0.08452 0.0506 0.02602 142 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08467 0.05025 0.02604 86 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.86it/s]21/99 7.28G 0.08467 0.05025 0.02604 86 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08462 0.04956 0.0258 66 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.37it/s] 21/99 7.28G 0.08462 0.04956 0.0258 66 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
21/99 7.28G 0.08433 0.04937 0.02566 75 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.45it/s]21/99 7.28G 0.08433 0.04937 0.02566 75 640: 100%|██████████| 19/19 [00:06<00:00,  2.77it/s]21/99 7.28G 0.08433 0.04937 0.02566 75 640: 100%|██████████| 19/19 [00:06<00:00,  2.74it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
                   all         55        256      0.424     0.0426     0.0964     0.0316
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.07841 0.03115 0.02299 49 640:   0%|          | 0/19 [00:00<?, ?it/s]22/99 7.28G 0.07841 0.03115 0.02299 49 640:   5%|▌         | 1/19 [00:00<00:07,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08181 0.04218 0.02435 92 640:   5%|▌         | 1/19 [00:00<00:07,  2.33it/s]22/99 7.28G 0.08181 0.04218 0.02435 92 640:  11%|█         | 2/19 [00:00<00:06,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08238 0.0387 0.02492 51 640:  11%|█         | 2/19 [00:01<00:06,  2.69it/s] 22/99 7.28G 0.08238 0.0387 0.02492 51 640:  16%|█▌        | 3/19 [00:01<00:05,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08102 0.03716 0.02398 52 640:  16%|█▌        | 3/19 [00:01<00:05,  2.78it/s]22/99 7.28G 0.08102 0.03716 0.02398 52 640:  21%|██        | 4/19 [00:01<00:05,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08146 0.03806 0.02423 63 640:  21%|██        | 4/19 [00:01<00:05,  2.82it/s]22/99 7.28G 0.08146 0.03806 0.02423 63 640:  26%|██▋       | 5/19 [00:01<00:04,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08167 0.03685 0.02444 49 640:  26%|██▋       | 5/19 [00:02<00:04,  2.85it/s]22/99 7.28G 0.08167 0.03685 0.02444 49 640:  32%|███▏      | 6/19 [00:02<00:04,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08261 0.03792 0.02513 74 640:  32%|███▏      | 6/19 [00:02<00:04,  2.85it/s]22/99 7.28G 0.08261 0.03792 0.02513 74 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08344 0.03929 0.02484 93 640:  37%|███▋      | 7/19 [00:02<00:04,  2.86it/s]22/99 7.28G 0.08344 0.03929 0.02484 93 640:  42%|████▏     | 8/19 [00:02<00:03,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08285 0.03849 0.02456 50 640:  42%|████▏     | 8/19 [00:03<00:03,  2.86it/s]22/99 7.28G 0.08285 0.03849 0.02456 50 640:  47%|████▋     | 9/19 [00:03<00:03,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08197 0.0375 0.02431 47 640:  47%|████▋     | 9/19 [00:03<00:03,  2.86it/s] 22/99 7.28G 0.08197 0.0375 0.02431 47 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08267 0.03919 0.02487 106 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.82it/s]22/99 7.28G 0.08267 0.03919 0.02487 106 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08317 0.03919 0.02471 88 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.78it/s] 22/99 7.28G 0.08317 0.03919 0.02471 88 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08285 0.03862 0.025 48 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.76it/s]  22/99 7.28G 0.08285 0.03862 0.025 48 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08321 0.03843 0.02475 80 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.75it/s]22/99 7.28G 0.08321 0.03843 0.02475 80 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0828 0.03888 0.02454 70 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.75it/s] 22/99 7.28G 0.0828 0.03888 0.02454 70 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08265 0.03913 0.02466 66 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.75it/s]22/99 7.28G 0.08265 0.03913 0.02466 66 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.0827 0.0395 0.02495 71 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.74it/s]  22/99 7.28G 0.0827 0.0395 0.02495 71 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08271 0.03987 0.02526 69 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.74it/s]22/99 7.28G 0.08271 0.03987 0.02526 69 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
22/99 7.28G 0.08265 0.03944 0.02539 48 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.67it/s]22/99 7.28G 0.08265 0.03944 0.02539 48 640: 100%|██████████| 19/19 [00:06<00:00,  2.70it/s]22/99 7.28G 0.08265 0.03944 0.02539 48 640: 100%|██████████| 19/19 [00:06<00:00,  2.76it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]
                   all         55        256      0.455     0.0481      0.113     0.0339
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08807 0.06496 0.02675 109 640:   0%|          | 0/19 [00:00<?, ?it/s]23/99 7.28G 0.08807 0.06496 0.02675 109 640:   5%|▌         | 1/19 [00:00<00:09,  1.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08267 0.04844 0.02508 48 640:   5%|▌         | 1/19 [00:01<00:09,  1.94it/s] 23/99 7.28G 0.08267 0.04844 0.02508 48 640:  11%|█         | 2/19 [00:01<00:08,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08195 0.0425 0.02474 49 640:  11%|█         | 2/19 [00:01<00:08,  1.93it/s] 23/99 7.28G 0.08195 0.0425 0.02474 49 640:  16%|█▌        | 3/19 [00:01<00:08,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.0834 0.04512 0.02537 92 640:  16%|█▌        | 3/19 [00:01<00:08,  1.92it/s]23/99 7.28G 0.0834 0.04512 0.02537 92 640:  21%|██        | 4/19 [00:01<00:06,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08185 0.04423 0.02534 60 640:  21%|██        | 4/19 [00:02<00:06,  2.18it/s]23/99 7.28G 0.08185 0.04423 0.02534 60 640:  26%|██▋       | 5/19 [00:02<00:05,  2.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08297 0.04712 0.02478 120 640:  26%|██▋       | 5/19 [00:02<00:05,  2.37it/s]23/99 7.28G 0.08297 0.04712 0.02478 120 640:  32%|███▏      | 6/19 [00:02<00:05,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.083 0.04774 0.02513 82 640:  32%|███▏      | 6/19 [00:02<00:05,  2.51it/s]   23/99 7.28G 0.083 0.04774 0.02513 82 640:  37%|███▋      | 7/19 [00:02<00:04,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08294 0.04871 0.02557 94 640:  37%|███▋      | 7/19 [00:03<00:04,  2.61it/s]23/99 7.28G 0.08294 0.04871 0.02557 94 640:  42%|████▏     | 8/19 [00:03<00:04,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08227 0.04742 0.02627 57 640:  42%|████▏     | 8/19 [00:03<00:04,  2.69it/s]23/99 7.28G 0.08227 0.04742 0.02627 57 640:  47%|████▋     | 9/19 [00:03<00:03,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08278 0.04716 0.02612 86 640:  47%|████▋     | 9/19 [00:04<00:03,  2.74it/s]23/99 7.28G 0.08278 0.04716 0.02612 86 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.0829 0.04646 0.02592 74 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.78it/s] 23/99 7.28G 0.0829 0.04646 0.02592 74 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08353 0.04678 0.02572 98 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.79it/s]23/99 7.28G 0.08353 0.04678 0.02572 98 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08331 0.04686 0.02547 81 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.82it/s]23/99 7.28G 0.08331 0.04686 0.02547 81 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08304 0.0465 0.02546 67 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.73it/s] 23/99 7.28G 0.08304 0.0465 0.02546 67 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08297 0.04536 0.02558 45 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.72it/s]23/99 7.28G 0.08297 0.04536 0.02558 45 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08252 0.04475 0.02594 50 640:  79%|███████▉  | 15/19 [00:06<00:01,  2.72it/s]23/99 7.28G 0.08252 0.04475 0.02594 50 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08275 0.04562 0.02621 105 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.73it/s]23/99 7.28G 0.08275 0.04562 0.02621 105 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08254 0.04464 0.02604 47 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.73it/s] 23/99 7.28G 0.08254 0.04464 0.02604 47 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
23/99 7.28G 0.08296 0.04533 0.02588 118 640:  95%|█████████▍| 18/19 [00:07<00:00,  2.74it/s]23/99 7.28G 0.08296 0.04533 0.02588 118 640: 100%|██████████| 19/19 [00:07<00:00,  2.80it/s]23/99 7.28G 0.08296 0.04533 0.02588 118 640: 100%|██████████| 19/19 [00:07<00:00,  2.60it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.21s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.20s/it]
                   all         55        256      0.452     0.0496      0.103      0.033
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08853 0.05301 0.02156 111 640:   0%|          | 0/19 [00:00<?, ?it/s]24/99 7.28G 0.08853 0.05301 0.02156 111 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08356 0.04801 0.02306 72 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s] 24/99 7.28G 0.08356 0.04801 0.02306 72 640:  11%|█         | 2/19 [00:00<00:07,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08546 0.05104 0.02339 115 640:  11%|█         | 2/19 [00:01<00:07,  2.32it/s]24/99 7.28G 0.08546 0.05104 0.02339 115 640:  16%|█▌        | 3/19 [00:01<00:06,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08535 0.0498 0.02569 72 640:  16%|█▌        | 3/19 [00:01<00:06,  2.51it/s]  24/99 7.28G 0.08535 0.0498 0.02569 72 640:  21%|██        | 4/19 [00:01<00:05,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08435 0.04964 0.02514 76 640:  21%|██        | 4/19 [00:01<00:05,  2.62it/s]24/99 7.28G 0.08435 0.04964 0.02514 76 640:  26%|██▋       | 5/19 [00:01<00:05,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.0833 0.04749 0.02577 53 640:  26%|██▋       | 5/19 [00:02<00:05,  2.68it/s] 24/99 7.28G 0.0833 0.04749 0.02577 53 640:  32%|███▏      | 6/19 [00:02<00:04,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08246 0.04599 0.02577 58 640:  32%|███▏      | 6/19 [00:02<00:04,  2.71it/s]24/99 7.28G 0.08246 0.04599 0.02577 58 640:  37%|███▋      | 7/19 [00:02<00:04,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08278 0.0443 0.0257 56 640:  37%|███▋      | 7/19 [00:02<00:04,  2.72it/s]  24/99 7.28G 0.08278 0.0443 0.0257 56 640:  42%|████▏     | 8/19 [00:02<00:04,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08316 0.04537 0.02556 95 640:  42%|████▏     | 8/19 [00:03<00:04,  2.74it/s]24/99 7.28G 0.08316 0.04537 0.02556 95 640:  47%|████▋     | 9/19 [00:03<00:03,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08362 0.04549 0.02529 84 640:  47%|████▋     | 9/19 [00:03<00:03,  2.75it/s]24/99 7.28G 0.08362 0.04549 0.02529 84 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08385 0.04691 0.02517 104 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.75it/s]24/99 7.28G 0.08385 0.04691 0.02517 104 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08318 0.04607 0.02546 55 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.67it/s] 24/99 7.28G 0.08318 0.04607 0.02546 55 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08305 0.04597 0.02566 71 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.67it/s]24/99 7.28G 0.08305 0.04597 0.02566 71 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08283 0.04587 0.02562 74 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.68it/s]24/99 7.28G 0.08283 0.04587 0.02562 74 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08283 0.04659 0.02552 92 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.69it/s]24/99 7.28G 0.08283 0.04659 0.02552 92 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08283 0.04723 0.02553 97 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.69it/s]24/99 7.28G 0.08283 0.04723 0.02553 97 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08254 0.04722 0.02535 72 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.68it/s]24/99 7.28G 0.08254 0.04722 0.02535 72 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.0825 0.04697 0.02531 73 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.69it/s] 24/99 7.28G 0.0825 0.04697 0.02531 73 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
24/99 7.28G 0.08254 0.04667 0.02516 70 640:  95%|█████████▍| 18/19 [00:07<00:00,  2.69it/s]24/99 7.28G 0.08254 0.04667 0.02516 70 640: 100%|██████████| 19/19 [00:07<00:00,  2.65it/s]24/99 7.28G 0.08254 0.04667 0.02516 70 640: 100%|██████████| 19/19 [00:07<00:00,  2.67it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.67s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]
                   all         55        256       0.43     0.0561      0.069     0.0176
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08212 0.04573 0.02659 76 640:   0%|          | 0/19 [00:00<?, ?it/s]25/99 7.28G 0.08212 0.04573 0.02659 76 640:   5%|▌         | 1/19 [00:00<00:06,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08135 0.03997 0.02771 53 640:   5%|▌         | 1/19 [00:00<00:06,  2.95it/s]25/99 7.28G 0.08135 0.03997 0.02771 53 640:  11%|█         | 2/19 [00:00<00:05,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08211 0.04161 0.02689 83 640:  11%|█         | 2/19 [00:01<00:05,  2.87it/s]25/99 7.28G 0.08211 0.04161 0.02689 83 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08375 0.04808 0.0286 112 640:  16%|█▌        | 3/19 [00:01<00:05,  2.88it/s]25/99 7.28G 0.08375 0.04808 0.0286 112 640:  21%|██        | 4/19 [00:01<00:05,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08441 0.04634 0.0277 74 640:  21%|██        | 4/19 [00:01<00:05,  2.88it/s] 25/99 7.28G 0.08441 0.04634 0.0277 74 640:  26%|██▋       | 5/19 [00:01<00:04,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08429 0.04614 0.02791 74 640:  26%|██▋       | 5/19 [00:02<00:04,  2.93it/s]25/99 7.28G 0.08429 0.04614 0.02791 74 640:  32%|███▏      | 6/19 [00:02<00:04,  2.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08372 0.04578 0.02796 71 640:  32%|███▏      | 6/19 [00:02<00:04,  2.90it/s]25/99 7.28G 0.08372 0.04578 0.02796 71 640:  37%|███▋      | 7/19 [00:02<00:04,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08387 0.04649 0.02759 89 640:  37%|███▋      | 7/19 [00:02<00:04,  2.88it/s]25/99 7.28G 0.08387 0.04649 0.02759 89 640:  42%|████▏     | 8/19 [00:02<00:03,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08351 0.04612 0.02752 74 640:  42%|████▏     | 8/19 [00:03<00:03,  2.88it/s]25/99 7.28G 0.08351 0.04612 0.02752 74 640:  47%|████▋     | 9/19 [00:03<00:03,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08418 0.04624 0.02711 96 640:  47%|████▋     | 9/19 [00:03<00:03,  2.88it/s]25/99 7.28G 0.08418 0.04624 0.02711 96 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08402 0.04511 0.02667 61 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.90it/s]25/99 7.28G 0.08402 0.04511 0.02667 61 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08412 0.04665 0.02673 115 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.82it/s]25/99 7.28G 0.08412 0.04665 0.02673 115 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08406 0.04757 0.0264 108 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.80it/s] 25/99 7.28G 0.08406 0.04757 0.0264 108 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08362 0.04661 0.02616 50 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.76it/s]25/99 7.28G 0.08362 0.04661 0.02616 50 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08339 0.04717 0.02599 89 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.75it/s]25/99 7.28G 0.08339 0.04717 0.02599 89 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08355 0.04648 0.0261 65 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.75it/s] 25/99 7.28G 0.08355 0.04648 0.0261 65 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08318 0.04599 0.02565 61 640:  84%|████████▍ | 16/19 [00:06<00:01,  2.74it/s]25/99 7.28G 0.08318 0.04599 0.02565 61 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08345 0.04715 0.02593 118 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.73it/s]25/99 7.28G 0.08345 0.04715 0.02593 118 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
25/99 7.28G 0.08347 0.0466 0.02604 63 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.73it/s]  25/99 7.28G 0.08347 0.0466 0.02604 63 640: 100%|██████████| 19/19 [00:06<00:00,  2.74it/s]25/99 7.28G 0.08347 0.0466 0.02604 63 640: 100%|██████████| 19/19 [00:06<00:00,  2.81it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.81s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]
                   all         55        256      0.426     0.0648     0.0891     0.0236
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08824 0.05935 0.02722 128 640:   0%|          | 0/19 [00:00<?, ?it/s]26/99 7.28G 0.08824 0.05935 0.02722 128 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08403 0.04909 0.02819 59 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s] 26/99 7.28G 0.08403 0.04909 0.02819 59 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08337 0.0493 0.02734 83 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s] 26/99 7.28G 0.08337 0.0493 0.02734 83 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08179 0.04687 0.02753 61 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]26/99 7.28G 0.08179 0.04687 0.02753 61 640:  21%|██        | 4/19 [00:00<00:03,  4.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08295 0.04543 0.02689 75 640:  21%|██        | 4/19 [00:01<00:03,  4.97it/s]26/99 7.28G 0.08295 0.04543 0.02689 75 640:  26%|██▋       | 5/19 [00:01<00:04,  3.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08309 0.04672 0.02646 87 640:  26%|██▋       | 5/19 [00:01<00:04,  3.20it/s]26/99 7.28G 0.08309 0.04672 0.02646 87 640:  32%|███▏      | 6/19 [00:01<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08333 0.04745 0.02697 82 640:  32%|███▏      | 6/19 [00:02<00:05,  2.59it/s]26/99 7.28G 0.08333 0.04745 0.02697 82 640:  37%|███▋      | 7/19 [00:02<00:04,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08448 0.05317 0.02685 176 640:  37%|███▋      | 7/19 [00:02<00:04,  2.43it/s]26/99 7.28G 0.08448 0.05317 0.02685 176 640:  42%|████▏     | 8/19 [00:02<00:04,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08419 0.05227 0.02693 74 640:  42%|████▏     | 8/19 [00:02<00:04,  2.52it/s] 26/99 7.28G 0.08419 0.05227 0.02693 74 640:  47%|████▋     | 9/19 [00:02<00:03,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08399 0.05222 0.0266 87 640:  47%|████▋     | 9/19 [00:03<00:03,  2.60it/s] 26/99 7.28G 0.08399 0.05222 0.0266 87 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08357 0.05063 0.02683 52 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.66it/s]26/99 7.28G 0.08357 0.05063 0.02683 52 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08335 0.05072 0.0267 86 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.70it/s] 26/99 7.28G 0.08335 0.05072 0.0267 86 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08287 0.04949 0.02646 55 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.73it/s]26/99 7.28G 0.08287 0.04949 0.02646 55 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08301 0.04875 0.02654 68 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.76it/s]26/99 7.28G 0.08301 0.04875 0.02654 68 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08327 0.04904 0.02659 103 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.77it/s]26/99 7.28G 0.08327 0.04904 0.02659 103 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08336 0.04914 0.02646 88 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.73it/s] 26/99 7.28G 0.08336 0.04914 0.02646 88 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08369 0.0492 0.02612 97 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.70it/s] 26/99 7.28G 0.08369 0.0492 0.02612 97 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08389 0.04944 0.02591 101 640:  89%|████████▉ | 17/19 [00:06<00:00,  2.70it/s]26/99 7.28G 0.08389 0.04944 0.02591 101 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
26/99 7.28G 0.08378 0.0498 0.02567 94 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.70it/s]  26/99 7.28G 0.08378 0.0498 0.02567 94 640: 100%|██████████| 19/19 [00:06<00:00,  2.70it/s]26/99 7.28G 0.08378 0.0498 0.02567 94 640: 100%|██████████| 19/19 [00:06<00:00,  2.86it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.64s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
                   all         55        256      0.411     0.0721      0.111     0.0373
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07967 0.03421 0.02871 57 640:   0%|          | 0/19 [00:00<?, ?it/s]27/99 7.28G 0.07967 0.03421 0.02871 57 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.07919 0.03674 0.02783 64 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s]27/99 7.28G 0.07919 0.03674 0.02783 64 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08144 0.04408 0.02713 117 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]27/99 7.28G 0.08144 0.04408 0.02713 117 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08175 0.04241 0.02733 63 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s] 27/99 7.28G 0.08175 0.04241 0.02733 63 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.0818 0.0408 0.02703 56 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]  27/99 7.28G 0.0818 0.0408 0.02703 56 640:  26%|██▋       | 5/19 [00:00<00:02,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08137 0.04075 0.02691 64 640:  26%|██▋       | 5/19 [00:01<00:02,  5.66it/s]27/99 7.28G 0.08137 0.04075 0.02691 64 640:  32%|███▏      | 6/19 [00:01<00:02,  5.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08132 0.04208 0.02694 81 640:  32%|███▏      | 6/19 [00:01<00:02,  5.21it/s]27/99 7.28G 0.08132 0.04208 0.02694 81 640:  37%|███▋      | 7/19 [00:01<00:03,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08128 0.04158 0.02722 61 640:  37%|███▋      | 7/19 [00:02<00:03,  3.46it/s]27/99 7.28G 0.08128 0.04158 0.02722 61 640:  42%|████▏     | 8/19 [00:02<00:03,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08194 0.04419 0.0274 122 640:  42%|████▏     | 8/19 [00:02<00:03,  2.79it/s]27/99 7.28G 0.08194 0.04419 0.0274 122 640:  47%|████▋     | 9/19 [00:02<00:03,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.0816 0.04306 0.0272 48 640:  47%|████▋     | 9/19 [00:02<00:03,  2.54it/s]  27/99 7.28G 0.0816 0.04306 0.0272 48 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08215 0.04384 0.02693 97 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.64it/s]27/99 7.28G 0.08215 0.04384 0.02693 97 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08259 0.04432 0.02725 82 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.70it/s]27/99 7.28G 0.08259 0.04432 0.02725 82 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08273 0.04499 0.02678 95 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.75it/s]27/99 7.28G 0.08273 0.04499 0.02678 95 640:  68%|██████▊   | 13/19 [00:03<00:02,  2.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08248 0.04514 0.02688 72 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.79it/s]27/99 7.28G 0.08248 0.04514 0.02688 72 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08238 0.04408 0.02706 47 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.80it/s]27/99 7.28G 0.08238 0.04408 0.02706 47 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08213 0.04384 0.02743 61 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.82it/s]27/99 7.28G 0.08213 0.04384 0.02743 61 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.0822 0.04413 0.02721 86 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.84it/s] 27/99 7.28G 0.0822 0.04413 0.02721 86 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.0822 0.04416 0.02707 73 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.85it/s]27/99 7.28G 0.0822 0.04416 0.02707 73 640:  95%|█████████▍| 18/19 [00:05<00:00,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
27/99 7.28G 0.08263 0.04488 0.02697 131 640:  95%|█████████▍| 18/19 [00:06<00:00,  2.85it/s]27/99 7.28G 0.08263 0.04488 0.02697 131 640: 100%|██████████| 19/19 [00:06<00:00,  2.84it/s]27/99 7.28G 0.08263 0.04488 0.02697 131 640: 100%|██████████| 19/19 [00:06<00:00,  3.13it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.77s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
                   all         55        256      0.414     0.0604      0.156     0.0506
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07493 0.03435 0.02448 52 640:   0%|          | 0/19 [00:00<?, ?it/s]28/99 7.28G 0.07493 0.03435 0.02448 52 640:   5%|▌         | 1/19 [00:00<00:03,  5.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08332 0.05083 0.02619 120 640:   5%|▌         | 1/19 [00:00<00:03,  5.56it/s]28/99 7.28G 0.08332 0.05083 0.02619 120 640:  11%|█         | 2/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08245 0.049 0.02583 68 640:  11%|█         | 2/19 [00:00<00:03,  5.64it/s]   28/99 7.28G 0.08245 0.049 0.02583 68 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08312 0.04865 0.02559 82 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]28/99 7.28G 0.08312 0.04865 0.02559 82 640:  21%|██        | 4/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08205 0.04617 0.02465 58 640:  21%|██        | 4/19 [00:00<00:02,  5.67it/s]28/99 7.28G 0.08205 0.04617 0.02465 58 640:  26%|██▋       | 5/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08097 0.04583 0.02404 72 640:  26%|██▋       | 5/19 [00:01<00:02,  5.70it/s]28/99 7.28G 0.08097 0.04583 0.02404 72 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08097 0.04552 0.02413 76 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s]28/99 7.28G 0.08097 0.04552 0.02413 76 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07955 0.04367 0.02461 44 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]28/99 7.28G 0.07955 0.04367 0.02461 44 640:  42%|████▏     | 8/19 [00:01<00:02,  5.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.0792 0.04349 0.02472 65 640:  42%|████▏     | 8/19 [00:01<00:02,  5.13it/s] 28/99 7.28G 0.0792 0.04349 0.02472 65 640:  47%|████▋     | 9/19 [00:01<00:02,  3.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.07934 0.04518 0.02479 100 640:  47%|████▋     | 9/19 [00:02<00:02,  3.36it/s]28/99 7.28G 0.07934 0.04518 0.02479 100 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08066 0.04761 0.02482 154 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.72it/s]28/99 7.28G 0.08066 0.04761 0.02482 154 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08091 0.04686 0.02472 67 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.45it/s] 28/99 7.28G 0.08091 0.04686 0.02472 67 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08087 0.04628 0.02478 66 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.56it/s]28/99 7.28G 0.08087 0.04628 0.02478 66 640:  68%|██████▊   | 13/19 [00:03<00:02,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08103 0.04554 0.02469 63 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.64it/s]28/99 7.28G 0.08103 0.04554 0.02469 63 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.081 0.04498 0.02472 64 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.71it/s]  28/99 7.28G 0.081 0.04498 0.02472 64 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08073 0.04423 0.02459 52 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.75it/s]28/99 7.28G 0.08073 0.04423 0.02459 52 640:  84%|████████▍ | 16/19 [00:04<00:01,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08035 0.04379 0.02424 60 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.78it/s]28/99 7.28G 0.08035 0.04379 0.02424 60 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08086 0.04406 0.02429 106 640:  89%|████████▉ | 17/19 [00:05<00:00,  2.81it/s]28/99 7.28G 0.08086 0.04406 0.02429 106 640:  95%|█████████▍| 18/19 [00:05<00:00,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
28/99 7.28G 0.08119 0.04409 0.02428 89 640:  95%|█████████▍| 18/19 [00:05<00:00,  2.83it/s] 28/99 7.28G 0.08119 0.04409 0.02428 89 640: 100%|██████████| 19/19 [00:05<00:00,  2.84it/s]28/99 7.28G 0.08119 0.04409 0.02428 89 640: 100%|██████████| 19/19 [00:05<00:00,  3.27it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.15s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.77s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
                   all         55        256      0.403     0.0879      0.183     0.0582
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.07958 0.04707 0.02634 77 640:   0%|          | 0/19 [00:00<?, ?it/s]29/99 7.28G 0.07958 0.04707 0.02634 77 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08092 0.04328 0.02759 57 640:   5%|▌         | 1/19 [00:00<00:03,  5.62it/s]29/99 7.28G 0.08092 0.04328 0.02759 57 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08517 0.04766 0.02724 118 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]29/99 7.28G 0.08517 0.04766 0.02724 118 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08467 0.04553 0.02701 61 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s] 29/99 7.28G 0.08467 0.04553 0.02701 61 640:  21%|██        | 4/19 [00:00<00:02,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08391 0.04571 0.02674 82 640:  21%|██        | 4/19 [00:00<00:02,  5.66it/s]29/99 7.28G 0.08391 0.04571 0.02674 82 640:  26%|██▋       | 5/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08404 0.04713 0.02654 93 640:  26%|██▋       | 5/19 [00:01<00:02,  5.69it/s]29/99 7.28G 0.08404 0.04713 0.02654 93 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08363 0.04802 0.02605 87 640:  32%|███▏      | 6/19 [00:01<00:02,  5.71it/s]29/99 7.28G 0.08363 0.04802 0.02605 87 640:  37%|███▋      | 7/19 [00:01<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08353 0.05016 0.02596 116 640:  37%|███▋      | 7/19 [00:01<00:02,  5.71it/s]29/99 7.28G 0.08353 0.05016 0.02596 116 640:  42%|████▏     | 8/19 [00:01<00:01,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08367 0.04844 0.02605 65 640:  42%|████▏     | 8/19 [00:01<00:01,  5.71it/s] 29/99 7.28G 0.08367 0.04844 0.02605 65 640:  47%|████▋     | 9/19 [00:01<00:01,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08277 0.04658 0.02616 46 640:  47%|████▋     | 9/19 [00:01<00:01,  5.67it/s]29/99 7.28G 0.08277 0.04658 0.02616 46 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08206 0.0456 0.02604 54 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.61it/s] 29/99 7.28G 0.08206 0.0456 0.02604 54 640:  58%|█████▊    | 11/19 [00:01<00:01,  5.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08252 0.04713 0.02602 118 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.44it/s]29/99 7.28G 0.08252 0.04713 0.02602 118 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08251 0.04728 0.02587 84 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.98it/s] 29/99 7.28G 0.08251 0.04728 0.02587 84 640:  68%|██████▊   | 13/19 [00:02<00:01,  3.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08244 0.04743 0.02599 81 640:  68%|██████▊   | 13/19 [00:03<00:01,  3.35it/s]29/99 7.28G 0.08244 0.04743 0.02599 81 640:  74%|███████▎  | 14/19 [00:03<00:02,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08261 0.04788 0.02597 94 640:  74%|███████▎  | 14/19 [00:03<00:02,  2.23it/s]29/99 7.28G 0.08261 0.04788 0.02597 94 640:  79%|███████▉  | 15/19 [00:03<00:01,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08271 0.04735 0.0258 72 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.44it/s] 29/99 7.28G 0.08271 0.04735 0.0258 72 640:  84%|████████▍ | 16/19 [00:04<00:01,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08262 0.04655 0.02588 58 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.58it/s]29/99 7.28G 0.08262 0.04655 0.02588 58 640:  89%|████████▉ | 17/19 [00:05<00:01,  1.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08248 0.0465 0.0258 73 640:  89%|████████▉ | 17/19 [00:05<00:01,  1.84it/s]  29/99 7.28G 0.08248 0.0465 0.0258 73 640:  95%|█████████▍| 18/19 [00:05<00:00,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
29/99 7.28G 0.08236 0.04628 0.02565 77 640:  95%|█████████▍| 18/19 [00:06<00:00,  1.75it/s]29/99 7.28G 0.08236 0.04628 0.02565 77 640: 100%|██████████| 19/19 [00:06<00:00,  1.98it/s]29/99 7.28G 0.08236 0.04628 0.02565 77 640: 100%|██████████| 19/19 [00:06<00:00,  3.13it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.64s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.97s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]
                   all         55        256      0.405     0.0684      0.166     0.0446
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08382 0.04035 0.02256 69 640:   0%|          | 0/19 [00:00<?, ?it/s]30/99 7.28G 0.08382 0.04035 0.02256 69 640:   5%|▌         | 1/19 [00:00<00:05,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.07973 0.04206 0.02304 71 640:   5%|▌         | 1/19 [00:00<00:05,  3.15it/s]30/99 7.28G 0.07973 0.04206 0.02304 71 640:  11%|█         | 2/19 [00:00<00:06,  2.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08199 0.05134 0.02341 121 640:  11%|█         | 2/19 [00:01<00:06,  2.45it/s]30/99 7.28G 0.08199 0.05134 0.02341 121 640:  16%|█▌        | 3/19 [00:01<00:06,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08373 0.05366 0.02352 119 640:  16%|█▌        | 3/19 [00:01<00:06,  2.32it/s]30/99 7.28G 0.08373 0.05366 0.02352 119 640:  21%|██        | 4/19 [00:01<00:06,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08455 0.05145 0.0237 80 640:  21%|██        | 4/19 [00:01<00:06,  2.44it/s]  30/99 7.28G 0.08455 0.05145 0.0237 80 640:  26%|██▋       | 5/19 [00:01<00:04,  3.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.0823 0.04903 0.02442 54 640:  26%|██▋       | 5/19 [00:01<00:04,  3.07it/s]30/99 7.28G 0.0823 0.04903 0.02442 54 640:  32%|███▏      | 6/19 [00:01<00:03,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08167 0.0464 0.02426 48 640:  32%|███▏      | 6/19 [00:02<00:03,  3.65it/s]30/99 7.28G 0.08167 0.0464 0.02426 48 640:  37%|███▋      | 7/19 [00:02<00:02,  4.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08064 0.04517 0.02428 53 640:  37%|███▋      | 7/19 [00:02<00:02,  4.15it/s]30/99 7.28G 0.08064 0.04517 0.02428 53 640:  42%|████▏     | 8/19 [00:02<00:02,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08098 0.04703 0.02431 113 640:  42%|████▏     | 8/19 [00:02<00:02,  4.55it/s]30/99 7.28G 0.08098 0.04703 0.02431 113 640:  47%|████▋     | 9/19 [00:02<00:02,  4.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08113 0.04679 0.02432 77 640:  47%|████▋     | 9/19 [00:02<00:02,  4.87it/s] 30/99 7.28G 0.08113 0.04679 0.02432 77 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08239 0.05071 0.02428 214 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.11it/s]30/99 7.28G 0.08239 0.05071 0.02428 214 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08171 0.04938 0.02412 55 640:  58%|█████▊    | 11/19 [00:03<00:01,  5.07it/s] 30/99 7.28G 0.08171 0.04938 0.02412 55 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08188 0.04894 0.02427 83 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.06it/s]30/99 7.28G 0.08188 0.04894 0.02427 83 640:  68%|██████▊   | 13/19 [00:03<00:01,  3.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08182 0.04924 0.02465 80 640:  68%|██████▊   | 13/19 [00:04<00:01,  3.17it/s]30/99 7.28G 0.08182 0.04924 0.02465 80 640:  74%|███████▎  | 14/19 [00:04<00:02,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08132 0.04816 0.02484 47 640:  74%|███████▎  | 14/19 [00:05<00:02,  2.10it/s]30/99 7.28G 0.08132 0.04816 0.02484 47 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08077 0.04649 0.02456 33 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.63it/s]30/99 7.28G 0.08077 0.04649 0.02456 33 640:  84%|████████▍ | 16/19 [00:05<00:01,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08016 0.0456 0.02446 46 640:  84%|████████▍ | 16/19 [00:06<00:01,  1.78it/s] 30/99 7.28G 0.08016 0.0456 0.02446 46 640:  89%|████████▉ | 17/19 [00:06<00:01,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08057 0.04619 0.02456 113 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.46it/s]30/99 7.28G 0.08057 0.04619 0.02456 113 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
30/99 7.28G 0.08076 0.04592 0.02456 87 640:  95%|█████████▍| 18/19 [00:10<00:00,  1.64it/s] 30/99 7.28G 0.08076 0.04592 0.02456 87 640: 100%|██████████| 19/19 [00:10<00:00,  1.40s/it]30/99 7.28G 0.08076 0.04592 0.02456 87 640: 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.60s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.96s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]
                   all         55        256      0.222     0.0832      0.149     0.0477
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08664 0.04972 0.02656 92 640:   0%|          | 0/19 [00:00<?, ?it/s]31/99 7.28G 0.08664 0.04972 0.02656 92 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08076 0.04055 0.02563 47 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]31/99 7.28G 0.08076 0.04055 0.02563 47 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0828 0.0491 0.02542 121 640:  11%|█         | 2/19 [00:00<00:02,  5.68it/s] 31/99 7.28G 0.0828 0.0491 0.02542 121 640:  16%|█▌        | 3/19 [00:00<00:04,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08453 0.04994 0.02563 123 640:  16%|█▌        | 3/19 [00:00<00:04,  3.88it/s]31/99 7.28G 0.08453 0.04994 0.02563 123 640:  21%|██        | 4/19 [00:00<00:03,  4.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08438 0.05167 0.02588 105 640:  21%|██        | 4/19 [00:01<00:03,  4.45it/s]31/99 7.28G 0.08438 0.05167 0.02588 105 640:  26%|██▋       | 5/19 [00:01<00:02,  4.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0837 0.05071 0.02557 76 640:  26%|██▋       | 5/19 [00:01<00:02,  4.84it/s]  31/99 7.28G 0.0837 0.05071 0.02557 76 640:  32%|███▏      | 6/19 [00:01<00:02,  5.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08468 0.05004 0.02518 95 640:  32%|███▏      | 6/19 [00:01<00:02,  5.12it/s]31/99 7.28G 0.08468 0.05004 0.02518 95 640:  37%|███▋      | 7/19 [00:01<00:02,  4.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08439 0.04897 0.02577 72 640:  37%|███▋      | 7/19 [00:01<00:02,  4.66it/s]31/99 7.28G 0.08439 0.04897 0.02577 72 640:  42%|████▏     | 8/19 [00:01<00:03,  3.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08451 0.0483 0.02553 71 640:  42%|████▏     | 8/19 [00:02<00:03,  3.64it/s] 31/99 7.28G 0.08451 0.0483 0.02553 71 640:  47%|████▋     | 9/19 [00:02<00:02,  3.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08333 0.0463 0.02476 47 640:  47%|████▋     | 9/19 [00:02<00:02,  3.76it/s]31/99 7.28G 0.08333 0.0463 0.02476 47 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0831 0.04737 0.02476 96 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.20it/s]31/99 7.28G 0.0831 0.04737 0.02476 96 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08274 0.04655 0.02519 59 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.40it/s]31/99 7.28G 0.08274 0.04655 0.02519 59 640:  63%|██████▎   | 12/19 [00:02<00:01,  3.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08265 0.04584 0.02488 63 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.88it/s]31/99 7.28G 0.08265 0.04584 0.02488 63 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08322 0.0462 0.02506 103 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.31it/s]31/99 7.28G 0.08322 0.0462 0.02506 103 640:  74%|███████▎  | 14/19 [00:03<00:01,  4.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08307 0.04699 0.0254 97 640:  74%|███████▎  | 14/19 [00:03<00:01,  4.67it/s] 31/99 7.28G 0.08307 0.04699 0.0254 97 640:  79%|███████▉  | 15/19 [00:03<00:00,  4.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08299 0.04698 0.02534 76 640:  79%|███████▉  | 15/19 [00:06<00:00,  4.94it/s]31/99 7.28G 0.08299 0.04698 0.02534 76 640:  84%|████████▍ | 16/19 [00:06<00:03,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.0829 0.04648 0.02537 64 640:  84%|████████▍ | 16/19 [00:06<00:03,  1.05s/it] 31/99 7.28G 0.0829 0.04648 0.02537 64 640:  89%|████████▉ | 17/19 [00:06<00:01,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08302 0.04725 0.02559 100 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.19it/s]31/99 7.28G 0.08302 0.04725 0.02559 100 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
31/99 7.28G 0.08313 0.04798 0.02551 107 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.42it/s]31/99 7.28G 0.08313 0.04798 0.02551 107 640: 100%|██████████| 19/19 [00:08<00:00,  1.14it/s]31/99 7.28G 0.08313 0.04798 0.02551 107 640: 100%|██████████| 19/19 [00:08<00:00,  2.26it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]
                   all         55        256      0.168     0.0955      0.141     0.0448
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08268 0.03184 0.02555 50 640:   0%|          | 0/19 [00:00<?, ?it/s]32/99 7.28G 0.08268 0.03184 0.02555 50 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08481 0.04882 0.0241 141 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]32/99 7.28G 0.08481 0.04882 0.0241 141 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08106 0.04129 0.02438 41 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]32/99 7.28G 0.08106 0.04129 0.02438 41 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07891 0.03894 0.0229 46 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s] 32/99 7.28G 0.07891 0.03894 0.0229 46 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08028 0.04042 0.02341 96 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]32/99 7.28G 0.08028 0.04042 0.02341 96 640:  26%|██▋       | 5/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07989 0.03982 0.02327 63 640:  26%|██▋       | 5/19 [00:01<00:02,  5.75it/s]32/99 7.28G 0.07989 0.03982 0.02327 63 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07975 0.03977 0.02362 66 640:  32%|███▏      | 6/19 [00:01<00:02,  5.75it/s]32/99 7.28G 0.07975 0.03977 0.02362 66 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.07939 0.03827 0.02375 42 640:  37%|███▋      | 7/19 [00:01<00:02,  5.75it/s]32/99 7.28G 0.07939 0.03827 0.02375 42 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08026 0.04073 0.02373 121 640:  42%|████▏     | 8/19 [00:01<00:01,  5.75it/s]32/99 7.28G 0.08026 0.04073 0.02373 121 640:  47%|████▋     | 9/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08008 0.04034 0.02378 57 640:  47%|████▋     | 9/19 [00:01<00:01,  5.75it/s] 32/99 7.28G 0.08008 0.04034 0.02378 57 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08044 0.04193 0.0235 106 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.75it/s]32/99 7.28G 0.08044 0.04193 0.0235 106 640:  58%|█████▊    | 11/19 [00:01<00:01,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08 0.04154 0.02333 58 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.67it/s]   32/99 7.28G 0.08 0.04154 0.02333 58 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08058 0.04078 0.02327 68 640:  63%|██████▎   | 12/19 [00:04<00:01,  5.67it/s]32/99 7.28G 0.08058 0.04078 0.02327 68 640:  68%|██████▊   | 13/19 [00:04<00:05,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08125 0.04008 0.02313 68 640:  68%|██████▊   | 13/19 [00:05<00:05,  1.08it/s]32/99 7.28G 0.08125 0.04008 0.02313 68 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08185 0.04305 0.02327 157 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.33it/s]32/99 7.28G 0.08185 0.04305 0.02327 157 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08214 0.04383 0.02318 115 640:  79%|███████▉  | 15/19 [00:07<00:02,  1.59it/s]32/99 7.28G 0.08214 0.04383 0.02318 115 640:  84%|████████▍ | 16/19 [00:07<00:02,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.0821 0.04415 0.02319 86 640:  84%|████████▍ | 16/19 [00:07<00:02,  1.02it/s]  32/99 7.28G 0.0821 0.04415 0.02319 86 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08213 0.04446 0.02341 84 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.16it/s]32/99 7.28G 0.08213 0.04446 0.02341 84 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
32/99 7.28G 0.08241 0.04554 0.02339 127 640:  95%|█████████▍| 18/19 [00:09<00:00,  1.34it/s]32/99 7.28G 0.08241 0.04554 0.02339 127 640: 100%|██████████| 19/19 [00:09<00:00,  1.15it/s]32/99 7.28G 0.08241 0.04554 0.02339 127 640: 100%|██████████| 19/19 [00:09<00:00,  2.01it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.50s/it]
                   all         55        256      0.182     0.0927       0.13     0.0417
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07297 0.02524 0.01944 37 640:   0%|          | 0/19 [00:00<?, ?it/s]33/99 7.28G 0.07297 0.02524 0.01944 37 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07687 0.03309 0.02035 72 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]33/99 7.28G 0.07687 0.03309 0.02035 72 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07561 0.0351 0.01932 63 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s] 33/99 7.28G 0.07561 0.0351 0.01932 63 640:  16%|█▌        | 3/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.0768 0.03571 0.02094 59 640:  16%|█▌        | 3/19 [00:00<00:02,  5.75it/s]33/99 7.28G 0.0768 0.03571 0.02094 59 640:  21%|██        | 4/19 [00:00<00:02,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07808 0.03827 0.02168 85 640:  21%|██        | 4/19 [00:00<00:02,  5.76it/s]33/99 7.28G 0.07808 0.03827 0.02168 85 640:  26%|██▋       | 5/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07835 0.03907 0.02279 72 640:  26%|██▋       | 5/19 [00:01<00:02,  5.75it/s]33/99 7.28G 0.07835 0.03907 0.02279 72 640:  32%|███▏      | 6/19 [00:01<00:02,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07865 0.03931 0.02247 68 640:  32%|███▏      | 6/19 [00:01<00:02,  5.57it/s]33/99 7.28G 0.07865 0.03931 0.02247 68 640:  37%|███▋      | 7/19 [00:01<00:02,  4.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07842 0.04009 0.02285 72 640:  37%|███▋      | 7/19 [00:01<00:02,  4.20it/s]33/99 7.28G 0.07842 0.04009 0.02285 72 640:  42%|████▏     | 8/19 [00:01<00:03,  3.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.0784 0.04079 0.02289 78 640:  42%|████▏     | 8/19 [00:02<00:03,  3.62it/s] 33/99 7.28G 0.0784 0.04079 0.02289 78 640:  47%|████▋     | 9/19 [00:02<00:02,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07878 0.0399 0.02343 50 640:  47%|████▋     | 9/19 [00:02<00:02,  3.46it/s]33/99 7.28G 0.07878 0.0399 0.02343 50 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07946 0.04183 0.02364 111 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.91it/s]33/99 7.28G 0.07946 0.04183 0.02364 111 640:  58%|█████▊    | 11/19 [00:02<00:02,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07965 0.04178 0.02379 72 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.88it/s] 33/99 7.28G 0.07965 0.04178 0.02379 72 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.07959 0.04159 0.02403 67 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.42it/s]33/99 7.28G 0.07959 0.04159 0.02403 67 640:  68%|██████▊   | 13/19 [00:05<00:05,  1.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08015 0.0412 0.02399 69 640:  68%|██████▊   | 13/19 [00:06<00:05,  1.10it/s] 33/99 7.28G 0.08015 0.0412 0.02399 69 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08029 0.04152 0.02406 77 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.05it/s]33/99 7.28G 0.08029 0.04152 0.02406 77 640:  79%|███████▉  | 15/19 [00:06<00:03,  1.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08076 0.0413 0.02414 71 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.27it/s] 33/99 7.28G 0.08076 0.0413 0.02414 71 640:  84%|████████▍ | 16/19 [00:08<00:02,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08061 0.04081 0.02434 53 640:  84%|████████▍ | 16/19 [00:08<00:02,  1.07it/s]33/99 7.28G 0.08061 0.04081 0.02434 53 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08102 0.04254 0.02439 141 640:  89%|████████▉ | 17/19 [00:10<00:01,  1.19it/s]33/99 7.28G 0.08102 0.04254 0.02439 141 640:  95%|█████████▍| 18/19 [00:10<00:01,  1.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
33/99 7.28G 0.08084 0.04234 0.02395 65 640:  95%|█████████▍| 18/19 [00:10<00:01,  1.02s/it] 33/99 7.28G 0.08084 0.04234 0.02395 65 640: 100%|██████████| 19/19 [00:10<00:00,  1.20it/s]33/99 7.28G 0.08084 0.04234 0.02395 65 640: 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.06s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
                   all         55        256      0.241      0.109       0.14     0.0382
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.0771 0.02629 0.02227 41 640:   0%|          | 0/19 [00:00<?, ?it/s]34/99 7.28G 0.0771 0.02629 0.02227 41 640:   5%|▌         | 1/19 [00:00<00:04,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08186 0.03544 0.02409 78 640:   5%|▌         | 1/19 [00:00<00:04,  4.40it/s]34/99 7.28G 0.08186 0.03544 0.02409 78 640:  11%|█         | 2/19 [00:00<00:04,  4.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08287 0.04035 0.02394 90 640:  11%|█         | 2/19 [00:00<00:04,  4.01it/s]34/99 7.28G 0.08287 0.04035 0.02394 90 640:  16%|█▌        | 3/19 [00:00<00:04,  3.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08218 0.03715 0.02512 42 640:  16%|█▌        | 3/19 [00:01<00:04,  3.76it/s]34/99 7.28G 0.08218 0.03715 0.02512 42 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08349 0.04159 0.02545 112 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]34/99 7.28G 0.08349 0.04159 0.02545 112 640:  26%|██▋       | 5/19 [00:01<00:04,  2.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.0829 0.04375 0.02498 95 640:  26%|██▋       | 5/19 [00:02<00:04,  2.87it/s]  34/99 7.28G 0.0829 0.04375 0.02498 95 640:  32%|███▏      | 6/19 [00:02<00:05,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08271 0.04313 0.02532 68 640:  32%|███▏      | 6/19 [00:02<00:05,  2.42it/s]34/99 7.28G 0.08271 0.04313 0.02532 68 640:  37%|███▋      | 7/19 [00:02<00:05,  2.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08274 0.04124 0.02581 40 640:  37%|███▋      | 7/19 [00:02<00:05,  2.19it/s]34/99 7.28G 0.08274 0.04124 0.02581 40 640:  42%|████▏     | 8/19 [00:02<00:04,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08225 0.04198 0.02596 77 640:  42%|████▏     | 8/19 [00:03<00:04,  2.36it/s]34/99 7.28G 0.08225 0.04198 0.02596 77 640:  47%|████▋     | 9/19 [00:03<00:04,  2.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08196 0.04163 0.02549 65 640:  47%|████▋     | 9/19 [00:03<00:04,  2.48it/s]34/99 7.28G 0.08196 0.04163 0.02549 65 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08163 0.04126 0.02496 63 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.54it/s]34/99 7.28G 0.08163 0.04126 0.02496 63 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08132 0.04012 0.02532 41 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.59it/s]34/99 7.28G 0.08132 0.04012 0.02532 41 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08197 0.04042 0.02515 92 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.65it/s]34/99 7.28G 0.08197 0.04042 0.02515 92 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08254 0.04059 0.02518 80 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.69it/s]34/99 7.28G 0.08254 0.04059 0.02518 80 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08275 0.04064 0.02497 77 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.73it/s]34/99 7.28G 0.08275 0.04064 0.02497 77 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08313 0.04037 0.02478 78 640:  79%|███████▉  | 15/19 [00:05<00:01,  2.67it/s]34/99 7.28G 0.08313 0.04037 0.02478 78 640:  84%|████████▍ | 16/19 [00:05<00:01,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08306 0.04113 0.02458 92 640:  84%|████████▍ | 16/19 [00:07<00:01,  2.60it/s]34/99 7.28G 0.08306 0.04113 0.02458 92 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08351 0.04195 0.02464 115 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.44it/s]34/99 7.28G 0.08351 0.04195 0.02464 115 640:  95%|█████████▍| 18/19 [00:07<00:00,  1.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
34/99 7.28G 0.08364 0.04266 0.02463 99 640:  95%|█████████▍| 18/19 [00:09<00:00,  1.52it/s] 34/99 7.28G 0.08364 0.04266 0.02463 99 640: 100%|██████████| 19/19 [00:09<00:00,  1.12it/s]34/99 7.28G 0.08364 0.04266 0.02463 99 640: 100%|██████████| 19/19 [00:09<00:00,  2.02it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]
                   all         55        256      0.254     0.0711      0.138     0.0413
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08028 0.04607 0.02765 77 640:   0%|          | 0/19 [00:00<?, ?it/s]35/99 7.28G 0.08028 0.04607 0.02765 77 640:   5%|▌         | 1/19 [00:00<00:05,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.081 0.05715 0.02664 114 640:   5%|▌         | 1/19 [00:00<00:05,  3.39it/s] 35/99 7.28G 0.081 0.05715 0.02664 114 640:  11%|█         | 2/19 [00:00<00:03,  4.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08244 0.05416 0.02693 83 640:  11%|█         | 2/19 [00:00<00:03,  4.47it/s]35/99 7.28G 0.08244 0.05416 0.02693 83 640:  16%|█▌        | 3/19 [00:00<00:03,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08351 0.05254 0.0259 85 640:  16%|█▌        | 3/19 [00:00<00:03,  4.98it/s] 35/99 7.28G 0.08351 0.05254 0.0259 85 640:  21%|██        | 4/19 [00:00<00:02,  5.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08568 0.05595 0.02501 184 640:  21%|██        | 4/19 [00:00<00:02,  5.24it/s]35/99 7.28G 0.08568 0.05595 0.02501 184 640:  26%|██▋       | 5/19 [00:00<00:02,  5.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08459 0.05256 0.02604 58 640:  26%|██▋       | 5/19 [00:01<00:02,  5.41it/s] 35/99 7.28G 0.08459 0.05256 0.02604 58 640:  32%|███▏      | 6/19 [00:01<00:02,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08427 0.05033 0.02548 72 640:  32%|███▏      | 6/19 [00:01<00:02,  5.53it/s]35/99 7.28G 0.08427 0.05033 0.02548 72 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08385 0.04967 0.02556 73 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]35/99 7.28G 0.08385 0.04967 0.02556 73 640:  42%|████▏     | 8/19 [00:01<00:01,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08371 0.04957 0.02601 78 640:  42%|████▏     | 8/19 [00:01<00:01,  5.65it/s]35/99 7.28G 0.08371 0.04957 0.02601 78 640:  47%|████▋     | 9/19 [00:01<00:01,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08373 0.04943 0.02563 97 640:  47%|████▋     | 9/19 [00:01<00:01,  5.68it/s]35/99 7.28G 0.08373 0.04943 0.02563 97 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08399 0.04904 0.02554 85 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.60it/s]35/99 7.28G 0.08399 0.04904 0.02554 85 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08389 0.04941 0.02524 98 640:  58%|█████▊    | 11/19 [00:03<00:01,  5.38it/s]35/99 7.28G 0.08389 0.04941 0.02524 98 640:  63%|██████▎   | 12/19 [00:03<00:03,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08401 0.04988 0.02547 92 640:  63%|██████▎   | 12/19 [00:03<00:03,  2.28it/s]35/99 7.28G 0.08401 0.04988 0.02547 92 640:  68%|██████▊   | 13/19 [00:03<00:02,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08381 0.04873 0.02495 62 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.33it/s]35/99 7.28G 0.08381 0.04873 0.02495 62 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08404 0.05037 0.02507 138 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.28it/s]35/99 7.28G 0.08404 0.05037 0.02507 138 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08403 0.04925 0.02524 53 640:  79%|███████▉  | 15/19 [00:06<00:02,  1.46it/s] 35/99 7.28G 0.08403 0.04925 0.02524 53 640:  84%|████████▍ | 16/19 [00:06<00:02,  1.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08413 0.0492 0.02555 89 640:  84%|████████▍ | 16/19 [00:07<00:02,  1.12it/s] 35/99 7.28G 0.08413 0.0492 0.02555 89 640:  89%|████████▉ | 17/19 [00:07<00:01,  1.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.08392 0.04923 0.02537 82 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.10it/s]35/99 7.28G 0.08392 0.04923 0.02537 82 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
35/99 7.28G 0.0836 0.04887 0.02519 70 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.25it/s] 35/99 7.28G 0.0836 0.04887 0.02519 70 640: 100%|██████████| 19/19 [00:08<00:00,  1.57it/s]35/99 7.28G 0.0836 0.04887 0.02519 70 640: 100%|██████████| 19/19 [00:08<00:00,  2.19it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.46s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
                   all         55        256      0.395     0.0633      0.125      0.033
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0828 0.04943 0.02285 85 640:   0%|          | 0/19 [00:00<?, ?it/s]36/99 7.28G 0.0828 0.04943 0.02285 85 640:   5%|▌         | 1/19 [00:00<00:03,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0846 0.05722 0.02301 118 640:   5%|▌         | 1/19 [00:00<00:03,  5.70it/s]36/99 7.28G 0.0846 0.05722 0.02301 118 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0849 0.05265 0.02142 95 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s] 36/99 7.28G 0.0849 0.05265 0.02142 95 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08349 0.04737 0.02167 59 640:  16%|█▌        | 3/19 [00:00<00:02,  5.73it/s]36/99 7.28G 0.08349 0.04737 0.02167 59 640:  21%|██        | 4/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.084 0.04811 0.02204 101 640:  21%|██        | 4/19 [00:00<00:02,  5.75it/s] 36/99 7.28G 0.084 0.04811 0.02204 101 640:  26%|██▋       | 5/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08394 0.04608 0.02289 63 640:  26%|██▋       | 5/19 [00:01<00:02,  5.75it/s]36/99 7.28G 0.08394 0.04608 0.02289 63 640:  32%|███▏      | 6/19 [00:01<00:02,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08288 0.04379 0.02294 47 640:  32%|███▏      | 6/19 [00:01<00:02,  5.76it/s]36/99 7.28G 0.08288 0.04379 0.02294 47 640:  37%|███▋      | 7/19 [00:01<00:02,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08279 0.04333 0.02305 73 640:  37%|███▋      | 7/19 [00:01<00:02,  5.76it/s]36/99 7.28G 0.08279 0.04333 0.02305 73 640:  42%|████▏     | 8/19 [00:01<00:01,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0831 0.04339 0.02315 77 640:  42%|████▏     | 8/19 [00:01<00:01,  5.76it/s] 36/99 7.28G 0.0831 0.04339 0.02315 77 640:  47%|████▋     | 9/19 [00:01<00:01,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0819 0.04191 0.02294 42 640:  47%|████▋     | 9/19 [00:01<00:01,  5.62it/s]36/99 7.28G 0.0819 0.04191 0.02294 42 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08218 0.04257 0.02312 95 640:  53%|█████▎    | 10/19 [00:03<00:01,  5.66it/s]36/99 7.28G 0.08218 0.04257 0.02312 95 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.0826 0.04455 0.02301 129 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.68it/s]36/99 7.28G 0.0826 0.04455 0.02301 129 640:  63%|██████▎   | 12/19 [00:03<00:03,  1.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08226 0.04349 0.02282 50 640:  63%|██████▎   | 12/19 [00:05<00:03,  1.90it/s]36/99 7.28G 0.08226 0.04349 0.02282 50 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08196 0.0426 0.02309 47 640:  68%|██████▊   | 13/19 [00:06<00:04,  1.25it/s] 36/99 7.28G 0.08196 0.0426 0.02309 47 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08212 0.04394 0.02306 107 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.16it/s]36/99 7.28G 0.08212 0.04394 0.02306 107 640:  79%|███████▉  | 15/19 [00:06<00:03,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08208 0.04445 0.02291 86 640:  79%|███████▉  | 15/19 [00:07<00:03,  1.26it/s] 36/99 7.28G 0.08208 0.04445 0.02291 86 640:  84%|████████▍ | 16/19 [00:07<00:01,  1.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08197 0.04401 0.02288 59 640:  84%|████████▍ | 16/19 [00:08<00:01,  1.51it/s]36/99 7.28G 0.08197 0.04401 0.02288 59 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08169 0.04388 0.023 63 640:  89%|████████▉ | 17/19 [00:08<00:01,  1.32it/s]  36/99 7.28G 0.08169 0.04388 0.023 63 640:  95%|█████████▍| 18/19 [00:08<00:00,  1.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
36/99 7.28G 0.08207 0.04624 0.02299 166 640:  95%|█████████▍| 18/19 [00:10<00:00,  1.67it/s]36/99 7.28G 0.08207 0.04624 0.02299 166 640: 100%|██████████| 19/19 [00:10<00:00,  1.05s/it]36/99 7.28G 0.08207 0.04624 0.02299 166 640: 100%|██████████| 19/19 [00:10<00:00,  1.82it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]
                   all         55        256      0.399      0.067      0.117     0.0403
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07625 0.03445 0.02028 56 640:   0%|          | 0/19 [00:00<?, ?it/s]37/99 7.28G 0.07625 0.03445 0.02028 56 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08025 0.04024 0.0229 89 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s] 37/99 7.28G 0.08025 0.04024 0.0229 89 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.07834 0.03669 0.02232 49 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]37/99 7.28G 0.07834 0.03669 0.02232 49 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08091 0.04483 0.02198 142 640:  16%|█▌        | 3/19 [00:00<00:02,  5.69it/s]37/99 7.28G 0.08091 0.04483 0.02198 142 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08196 0.0436 0.02365 68 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]  37/99 7.28G 0.08196 0.0436 0.02365 68 640:  26%|██▋       | 5/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08366 0.04476 0.02392 110 640:  26%|██▋       | 5/19 [00:01<00:02,  5.73it/s]37/99 7.28G 0.08366 0.04476 0.02392 110 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08283 0.04558 0.02448 80 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s] 37/99 7.28G 0.08283 0.04558 0.02448 80 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08329 0.04744 0.02382 114 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]37/99 7.28G 0.08329 0.04744 0.02382 114 640:  42%|████▏     | 8/19 [00:01<00:01,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08286 0.04711 0.02389 73 640:  42%|████▏     | 8/19 [00:01<00:01,  5.74it/s] 37/99 7.28G 0.08286 0.04711 0.02389 73 640:  47%|████▋     | 9/19 [00:01<00:01,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08306 0.04697 0.02384 86 640:  47%|████▋     | 9/19 [00:01<00:01,  5.70it/s]37/99 7.28G 0.08306 0.04697 0.02384 86 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.083 0.04634 0.02384 68 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.71it/s]  37/99 7.28G 0.083 0.04634 0.02384 68 640:  58%|█████▊    | 11/19 [00:01<00:01,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08329 0.04714 0.02385 114 640:  58%|█████▊    | 11/19 [00:03<00:01,  5.73it/s]37/99 7.28G 0.08329 0.04714 0.02385 114 640:  63%|██████▎   | 12/19 [00:03<00:04,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08331 0.04655 0.02376 71 640:  63%|██████▎   | 12/19 [00:04<00:04,  1.40it/s] 37/99 7.28G 0.08331 0.04655 0.02376 71 640:  68%|██████▊   | 13/19 [00:04<00:03,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08336 0.04663 0.02382 86 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.64it/s]37/99 7.28G 0.08336 0.04663 0.02382 86 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08365 0.04746 0.02396 104 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.49it/s]37/99 7.28G 0.08365 0.04746 0.02396 104 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08336 0.04695 0.02408 61 640:  79%|███████▉  | 15/19 [00:08<00:02,  1.54it/s] 37/99 7.28G 0.08336 0.04695 0.02408 61 640:  84%|████████▍ | 16/19 [00:08<00:03,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08341 0.04668 0.02425 84 640:  84%|████████▍ | 16/19 [00:09<00:03,  1.16s/it]37/99 7.28G 0.08341 0.04668 0.02425 84 640:  89%|████████▉ | 17/19 [00:09<00:02,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08356 0.04737 0.02394 117 640:  89%|████████▉ | 17/19 [00:10<00:02,  1.25s/it]37/99 7.28G 0.08356 0.04737 0.02394 117 640:  95%|█████████▍| 18/19 [00:10<00:01,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
37/99 7.28G 0.08366 0.047 0.0239 86 640:  95%|█████████▍| 18/19 [00:11<00:01,  1.21s/it]    37/99 7.28G 0.08366 0.047 0.0239 86 640: 100%|██████████| 19/19 [00:11<00:00,  1.11s/it]37/99 7.28G 0.08366 0.047 0.0239 86 640: 100%|██████████| 19/19 [00:11<00:00,  1.66it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.74s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]
                   all         55        256      0.396     0.0677      0.134     0.0404
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08199 0.03808 0.02599 63 640:   0%|          | 0/19 [00:00<?, ?it/s]38/99 7.28G 0.08199 0.03808 0.02599 63 640:   5%|▌         | 1/19 [00:00<00:03,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08329 0.04381 0.02676 94 640:   5%|▌         | 1/19 [00:00<00:03,  5.51it/s]38/99 7.28G 0.08329 0.04381 0.02676 94 640:  11%|█         | 2/19 [00:00<00:03,  5.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07999 0.04123 0.02553 57 640:  11%|█         | 2/19 [00:00<00:03,  5.41it/s]38/99 7.28G 0.07999 0.04123 0.02553 57 640:  16%|█▌        | 3/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.07933 0.03914 0.02466 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.45it/s]38/99 7.28G 0.07933 0.03914 0.02466 52 640:  21%|██        | 4/19 [00:00<00:02,  5.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08109 0.03873 0.02418 71 640:  21%|██        | 4/19 [00:00<00:02,  5.49it/s]38/99 7.28G 0.08109 0.03873 0.02418 71 640:  26%|██▋       | 5/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08076 0.03884 0.02424 66 640:  26%|██▋       | 5/19 [00:01<00:02,  5.45it/s]38/99 7.28G 0.08076 0.03884 0.02424 66 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08084 0.03966 0.02427 71 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s]38/99 7.28G 0.08084 0.03966 0.02427 71 640:  37%|███▋      | 7/19 [00:01<00:03,  3.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08058 0.03938 0.02408 59 640:  37%|███▋      | 7/19 [00:02<00:03,  3.42it/s]38/99 7.28G 0.08058 0.03938 0.02408 59 640:  42%|████▏     | 8/19 [00:02<00:03,  2.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08105 0.03993 0.02386 83 640:  42%|████▏     | 8/19 [00:02<00:03,  2.91it/s]38/99 7.28G 0.08105 0.03993 0.02386 83 640:  47%|████▋     | 9/19 [00:02<00:03,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08174 0.04171 0.02418 108 640:  47%|████▋     | 9/19 [00:02<00:03,  2.86it/s]38/99 7.28G 0.08174 0.04171 0.02418 108 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08096 0.03984 0.02416 33 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.84it/s] 38/99 7.28G 0.08096 0.03984 0.02416 33 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08024 0.03894 0.02399 46 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.83it/s]38/99 7.28G 0.08024 0.03894 0.02399 46 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08042 0.03932 0.02422 79 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.75it/s]38/99 7.28G 0.08042 0.03932 0.02422 79 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08071 0.0393 0.02397 77 640:  68%|██████▊   | 13/19 [00:06<00:04,  1.36it/s] 38/99 7.28G 0.08071 0.0393 0.02397 77 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.0811 0.0395 0.02391 88 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.17it/s] 38/99 7.28G 0.0811 0.0395 0.02391 88 640:  79%|███████▉  | 15/19 [00:09<00:06,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08072 0.03884 0.02363 45 640:  79%|███████▉  | 15/19 [00:10<00:06,  1.67s/it]38/99 7.28G 0.08072 0.03884 0.02363 45 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.081 0.03864 0.02382 64 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.22s/it]  38/99 7.28G 0.081 0.03864 0.02382 64 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08052 0.03825 0.02396 47 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.48s/it]38/99 7.28G 0.08052 0.03825 0.02396 47 640:  95%|█████████▍| 18/19 [00:12<00:01,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
38/99 7.28G 0.08024 0.03764 0.02387 40 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.21s/it]38/99 7.28G 0.08024 0.03764 0.02387 40 640: 100%|██████████| 19/19 [00:13<00:00,  1.02s/it]38/99 7.28G 0.08024 0.03764 0.02387 40 640: 100%|██████████| 19/19 [00:13<00:00,  1.43it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.47s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
                   all         55        256       0.41     0.0786      0.168     0.0484
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08276 0.03473 0.02334 62 640:   0%|          | 0/19 [00:00<?, ?it/s]39/99 7.28G 0.08276 0.03473 0.02334 62 640:   5%|▌         | 1/19 [00:00<00:03,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.07978 0.03562 0.02178 60 640:   5%|▌         | 1/19 [00:00<00:03,  4.56it/s]39/99 7.28G 0.07978 0.03562 0.02178 60 640:  11%|█         | 2/19 [00:00<00:05,  3.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08245 0.04241 0.02456 102 640:  11%|█         | 2/19 [00:00<00:05,  3.20it/s]39/99 7.28G 0.08245 0.04241 0.02456 102 640:  16%|█▌        | 3/19 [00:00<00:05,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08091 0.03907 0.02379 49 640:  16%|█▌        | 3/19 [00:01<00:05,  2.95it/s] 39/99 7.28G 0.08091 0.03907 0.02379 49 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08066 0.03999 0.02279 73 640:  21%|██        | 4/19 [00:01<00:05,  2.85it/s]39/99 7.28G 0.08066 0.03999 0.02279 73 640:  26%|██▋       | 5/19 [00:01<00:04,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08144 0.04327 0.02275 112 640:  26%|██▋       | 5/19 [00:02<00:04,  2.80it/s]39/99 7.28G 0.08144 0.04327 0.02275 112 640:  32%|███▏      | 6/19 [00:02<00:04,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08147 0.04274 0.02307 67 640:  32%|███▏      | 6/19 [00:02<00:04,  2.78it/s] 39/99 7.28G 0.08147 0.04274 0.02307 67 640:  37%|███▋      | 7/19 [00:02<00:04,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08142 0.042 0.02324 64 640:  37%|███▋      | 7/19 [00:02<00:04,  2.64it/s]  39/99 7.28G 0.08142 0.042 0.02324 64 640:  42%|████▏     | 8/19 [00:02<00:04,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.0815 0.04229 0.02336 82 640:  42%|████▏     | 8/19 [00:03<00:04,  2.51it/s]39/99 7.28G 0.0815 0.04229 0.02336 82 640:  47%|████▋     | 9/19 [00:03<00:04,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08198 0.04209 0.02337 79 640:  47%|████▋     | 9/19 [00:03<00:04,  2.42it/s]39/99 7.28G 0.08198 0.04209 0.02337 79 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08238 0.04303 0.02352 99 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.48it/s]39/99 7.28G 0.08238 0.04303 0.02352 99 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08181 0.04308 0.02436 68 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.42it/s]39/99 7.28G 0.08181 0.04308 0.02436 68 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08097 0.04252 0.024 52 640:  63%|██████▎   | 12/19 [00:06<00:05,  1.27it/s]  39/99 7.28G 0.08097 0.04252 0.024 52 640:  68%|██████▊   | 13/19 [00:06<00:03,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08182 0.04456 0.02394 161 640:  68%|██████▊   | 13/19 [00:07<00:03,  1.66it/s]39/99 7.28G 0.08182 0.04456 0.02394 161 640:  74%|███████▎  | 14/19 [00:07<00:04,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08161 0.04549 0.02386 97 640:  74%|███████▎  | 14/19 [00:08<00:04,  1.02it/s] 39/99 7.28G 0.08161 0.04549 0.02386 97 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08113 0.04441 0.02379 41 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.22it/s]39/99 7.28G 0.08113 0.04441 0.02379 41 640:  84%|████████▍ | 16/19 [00:08<00:01,  1.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08101 0.04499 0.02358 91 640:  84%|████████▍ | 16/19 [00:11<00:01,  1.54it/s]39/99 7.28G 0.08101 0.04499 0.02358 91 640:  89%|████████▉ | 17/19 [00:11<00:02,  1.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08098 0.04485 0.02346 80 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.24s/it]39/99 7.28G 0.08098 0.04485 0.02346 80 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.90s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
39/99 7.28G 0.08073 0.04466 0.02344 72 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.90s/it]39/99 7.28G 0.08073 0.04466 0.02344 72 640: 100%|██████████| 19/19 [00:16<00:00,  1.80s/it]39/99 7.28G 0.08073 0.04466 0.02344 72 640: 100%|██████████| 19/19 [00:16<00:00,  1.17it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
                   all         55        256      0.406     0.0804      0.149     0.0547
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08118 0.05187 0.02661 93 640:   0%|          | 0/19 [00:00<?, ?it/s]40/99 7.28G 0.08118 0.05187 0.02661 93 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.07915 0.04609 0.02774 64 640:   5%|▌         | 1/19 [00:00<00:03,  5.60it/s]40/99 7.28G 0.07915 0.04609 0.02774 64 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08212 0.04905 0.02714 106 640:  11%|█         | 2/19 [00:00<00:02,  5.67it/s]40/99 7.28G 0.08212 0.04905 0.02714 106 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08145 0.04521 0.02688 56 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s] 40/99 7.28G 0.08145 0.04521 0.02688 56 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08142 0.04533 0.02778 73 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]40/99 7.28G 0.08142 0.04533 0.02778 73 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08227 0.04793 0.02868 103 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s]40/99 7.28G 0.08227 0.04793 0.02868 103 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08293 0.04627 0.0278 72 640:  32%|███▏      | 6/19 [00:01<00:02,  5.74it/s]  40/99 7.28G 0.08293 0.04627 0.0278 72 640:  37%|███▋      | 7/19 [00:01<00:02,  5.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08229 0.04475 0.02756 52 640:  37%|███▋      | 7/19 [00:01<00:02,  5.63it/s]40/99 7.28G 0.08229 0.04475 0.02756 52 640:  42%|████▏     | 8/19 [00:01<00:02,  4.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08199 0.0454 0.02697 89 640:  42%|████▏     | 8/19 [00:02<00:02,  4.23it/s] 40/99 7.28G 0.08199 0.0454 0.02697 89 640:  47%|████▋     | 9/19 [00:02<00:04,  2.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08169 0.04423 0.02675 54 640:  47%|████▋     | 9/19 [00:02<00:04,  2.40it/s]40/99 7.28G 0.08169 0.04423 0.02675 54 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08069 0.04343 0.0272 55 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.36it/s] 40/99 7.28G 0.08069 0.04343 0.0272 55 640:  58%|█████▊    | 11/19 [00:04<00:06,  1.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08035 0.04297 0.02701 61 640:  58%|█████▊    | 11/19 [00:05<00:06,  1.24it/s]40/99 7.28G 0.08035 0.04297 0.02701 61 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08046 0.04341 0.02664 85 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.38it/s]40/99 7.28G 0.08046 0.04341 0.02664 85 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08088 0.04489 0.02671 108 640:  68%|██████▊   | 13/19 [00:07<00:03,  1.53it/s]40/99 7.28G 0.08088 0.04489 0.02671 108 640:  74%|███████▎  | 14/19 [00:07<00:04,  1.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08085 0.0435 0.02635 37 640:  74%|███████▎  | 14/19 [00:08<00:04,  1.04it/s]  40/99 7.28G 0.08085 0.0435 0.02635 37 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08135 0.04495 0.02596 143 640:  79%|███████▉  | 15/19 [00:10<00:04,  1.13s/it]40/99 7.28G 0.08135 0.04495 0.02596 143 640:  84%|████████▍ | 16/19 [00:10<00:04,  1.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08215 0.04667 0.0257 169 640:  84%|████████▍ | 16/19 [00:13<00:04,  1.43s/it] 40/99 7.28G 0.08215 0.04667 0.0257 169 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08226 0.04667 0.02573 90 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.93s/it]40/99 7.28G 0.08226 0.04667 0.02573 90 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
40/99 7.28G 0.08245 0.04682 0.0256 106 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.45s/it]40/99 7.28G 0.08245 0.04682 0.0256 106 640: 100%|██████████| 19/19 [00:16<00:00,  1.82s/it]40/99 7.28G 0.08245 0.04682 0.0256 106 640: 100%|██████████| 19/19 [00:16<00:00,  1.12it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.74s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.43s/it]
                   all         55        256      0.401     0.0754      0.143     0.0488
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.07886 0.03402 0.02168 58 640:   0%|          | 0/19 [00:00<?, ?it/s]41/99 7.3G 0.07886 0.03402 0.02168 58 640:   5%|▌         | 1/19 [00:00<00:08,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08405 0.04763 0.0217 123 640:   5%|▌         | 1/19 [00:00<00:08,  2.23it/s]41/99 7.3G 0.08405 0.04763 0.0217 123 640:  11%|█         | 2/19 [00:00<00:07,  2.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08547 0.04933 0.02368 103 640:  11%|█         | 2/19 [00:01<00:07,  2.40it/s]41/99 7.3G 0.08547 0.04933 0.02368 103 640:  16%|█▌        | 3/19 [00:01<00:07,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08304 0.04537 0.0243 53 640:  16%|█▌        | 3/19 [00:01<00:07,  2.26it/s]  41/99 7.3G 0.08304 0.04537 0.0243 53 640:  21%|██        | 4/19 [00:01<00:05,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08255 0.04597 0.02479 87 640:  21%|██        | 4/19 [00:01<00:05,  2.76it/s]41/99 7.3G 0.08255 0.04597 0.02479 87 640:  26%|██▋       | 5/19 [00:01<00:04,  3.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08282 0.04694 0.02449 100 640:  26%|██▋       | 5/19 [00:02<00:04,  3.27it/s]41/99 7.3G 0.08282 0.04694 0.02449 100 640:  32%|███▏      | 6/19 [00:02<00:03,  3.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08155 0.04531 0.02425 53 640:  32%|███▏      | 6/19 [00:02<00:03,  3.49it/s] 41/99 7.3G 0.08155 0.04531 0.02425 53 640:  37%|███▋      | 7/19 [00:02<00:03,  3.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08232 0.04595 0.02435 107 640:  37%|███▋      | 7/19 [00:02<00:03,  3.26it/s]41/99 7.3G 0.08232 0.04595 0.02435 107 640:  42%|████▏     | 8/19 [00:02<00:03,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.0824 0.04677 0.02413 93 640:  42%|████▏     | 8/19 [00:02<00:03,  3.65it/s]  41/99 7.3G 0.0824 0.04677 0.02413 93 640:  47%|████▋     | 9/19 [00:02<00:02,  3.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08179 0.04555 0.02439 56 640:  47%|████▋     | 9/19 [00:02<00:02,  3.95it/s]41/99 7.3G 0.08179 0.04555 0.02439 56 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08145 0.04489 0.02512 57 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.30it/s]41/99 7.3G 0.08145 0.04489 0.02512 57 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08209 0.04646 0.02489 138 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.66it/s]41/99 7.3G 0.08209 0.04646 0.02489 138 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08203 0.04701 0.02447 104 640:  63%|██████▎   | 12/19 [00:05<00:01,  3.65it/s]41/99 7.3G 0.08203 0.04701 0.02447 104 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08225 0.04673 0.02438 82 640:  68%|██████▊   | 13/19 [00:08<00:04,  1.38it/s] 41/99 7.3G 0.08225 0.04673 0.02438 82 640:  74%|███████▎  | 14/19 [00:08<00:06,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08237 0.04703 0.02444 92 640:  74%|███████▎  | 14/19 [00:08<00:06,  1.35s/it]41/99 7.3G 0.08237 0.04703 0.02444 92 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08216 0.04664 0.02459 63 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.09s/it]41/99 7.3G 0.08216 0.04664 0.02459 63 640:  84%|████████▍ | 16/19 [00:09<00:03,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08182 0.04659 0.0247 71 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.13s/it] 41/99 7.3G 0.08182 0.04659 0.0247 71 640:  89%|████████▉ | 17/19 [00:10<00:01,  1.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08177 0.04695 0.02474 84 640:  89%|████████▉ | 17/19 [00:10<00:01,  1.06it/s]41/99 7.3G 0.08177 0.04695 0.02474 84 640:  95%|█████████▍| 18/19 [00:10<00:00,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
41/99 7.3G 0.08174 0.04691 0.02456 79 640:  95%|█████████▍| 18/19 [00:15<00:00,  1.31it/s]41/99 7.3G 0.08174 0.04691 0.02456 79 640: 100%|██████████| 19/19 [00:15<00:00,  1.91s/it]41/99 7.3G 0.08174 0.04691 0.02456 79 640: 100%|██████████| 19/19 [00:15<00:00,  1.25it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.86s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
                   all         55        256      0.268      0.169       0.14     0.0432
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07702 0.03439 0.02614 55 640:   0%|          | 0/19 [00:00<?, ?it/s]42/99 7.3G 0.07702 0.03439 0.02614 55 640:   5%|▌         | 1/19 [00:00<00:04,  3.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07876 0.03893 0.02474 76 640:   5%|▌         | 1/19 [00:00<00:04,  3.90it/s]42/99 7.3G 0.07876 0.03893 0.02474 76 640:  11%|█         | 2/19 [00:00<00:03,  4.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07984 0.04152 0.02403 85 640:  11%|█         | 2/19 [00:00<00:03,  4.41it/s]42/99 7.3G 0.07984 0.04152 0.02403 85 640:  16%|█▌        | 3/19 [00:00<00:03,  4.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08 0.04043 0.02387 58 640:  16%|█▌        | 3/19 [00:00<00:03,  4.60it/s]   42/99 7.3G 0.08 0.04043 0.02387 58 640:  21%|██        | 4/19 [00:00<00:03,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07976 0.03855 0.02418 53 640:  21%|██        | 4/19 [00:01<00:03,  4.71it/s]42/99 7.3G 0.07976 0.03855 0.02418 53 640:  26%|██▋       | 5/19 [00:01<00:02,  4.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.07949 0.03712 0.02412 47 640:  26%|██▋       | 5/19 [00:01<00:02,  4.78it/s]42/99 7.3G 0.07949 0.03712 0.02412 47 640:  32%|███▏      | 6/19 [00:01<00:03,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08091 0.04136 0.02391 126 640:  32%|███▏      | 6/19 [00:01<00:03,  3.86it/s]42/99 7.3G 0.08091 0.04136 0.02391 126 640:  37%|███▋      | 7/19 [00:01<00:03,  3.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08096 0.04248 0.02423 89 640:  37%|███▋      | 7/19 [00:02<00:03,  3.30it/s] 42/99 7.3G 0.08096 0.04248 0.02423 89 640:  42%|████▏     | 8/19 [00:02<00:03,  3.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08082 0.04291 0.02358 89 640:  42%|████▏     | 8/19 [00:02<00:03,  3.03it/s]42/99 7.3G 0.08082 0.04291 0.02358 89 640:  47%|████▋     | 9/19 [00:02<00:02,  3.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08023 0.04089 0.02352 36 640:  47%|████▋     | 9/19 [00:02<00:02,  3.56it/s]42/99 7.3G 0.08023 0.04089 0.02352 36 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08151 0.04336 0.0233 178 640:  53%|█████▎    | 10/19 [00:04<00:02,  4.04it/s]42/99 7.3G 0.08151 0.04336 0.0233 178 640:  58%|█████▊    | 11/19 [00:04<00:06,  1.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08106 0.04285 0.02325 62 640:  58%|█████▊    | 11/19 [00:05<00:06,  1.20it/s]42/99 7.3G 0.08106 0.04285 0.02325 62 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08099 0.04208 0.02311 55 640:  63%|██████▎   | 12/19 [00:06<00:04,  1.45it/s]42/99 7.3G 0.08099 0.04208 0.02311 55 640:  68%|██████▊   | 13/19 [00:06<00:05,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.0816 0.0418 0.02318 87 640:  68%|██████▊   | 13/19 [00:07<00:05,  1.19it/s]  42/99 7.3G 0.0816 0.0418 0.02318 87 640:  74%|███████▎  | 14/19 [00:07<00:04,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08175 0.04114 0.02304 58 640:  74%|███████▎  | 14/19 [00:07<00:04,  1.03it/s]42/99 7.3G 0.08175 0.04114 0.02304 58 640:  79%|███████▉  | 15/19 [00:07<00:03,  1.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08215 0.04148 0.02319 94 640:  79%|███████▉  | 15/19 [00:11<00:03,  1.31it/s]42/99 7.3G 0.08215 0.04148 0.02319 94 640:  84%|████████▍ | 16/19 [00:11<00:04,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08188 0.04073 0.02313 45 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.52s/it]42/99 7.3G 0.08188 0.04073 0.02313 45 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.0817 0.04144 0.02299 97 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.63s/it] 42/99 7.3G 0.0817 0.04144 0.02299 97 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.88s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
42/99 7.3G 0.08179 0.04225 0.0232 93 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.88s/it]42/99 7.3G 0.08179 0.04225 0.0232 93 640: 100%|██████████| 19/19 [00:19<00:00,  2.67s/it]42/99 7.3G 0.08179 0.04225 0.0232 93 640: 100%|██████████| 19/19 [00:19<00:00,  1.05s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]
                   all         55        256      0.153       0.15       0.13     0.0395
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08312 0.03558 0.02201 66 640:   0%|          | 0/19 [00:00<?, ?it/s]43/99 7.3G 0.08312 0.03558 0.02201 66 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08183 0.04268 0.02299 88 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]43/99 7.3G 0.08183 0.04268 0.02299 88 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.07987 0.0383 0.022 47 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]   43/99 7.3G 0.07987 0.0383 0.022 47 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08145 0.04855 0.02308 135 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]43/99 7.3G 0.08145 0.04855 0.02308 135 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08347 0.05925 0.02375 212 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]43/99 7.3G 0.08347 0.05925 0.02375 212 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08206 0.05461 0.02457 50 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s] 43/99 7.3G 0.08206 0.05461 0.02457 50 640:  32%|███▏      | 6/19 [00:01<00:02,  5.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08245 0.0552 0.0248 117 640:  32%|███▏      | 6/19 [00:01<00:02,  5.47it/s] 43/99 7.3G 0.08245 0.0552 0.0248 117 640:  37%|███▋      | 7/19 [00:01<00:02,  5.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08273 0.05257 0.02504 69 640:  37%|███▋      | 7/19 [00:01<00:02,  5.34it/s]43/99 7.3G 0.08273 0.05257 0.02504 69 640:  42%|████▏     | 8/19 [00:01<00:02,  5.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08218 0.05247 0.02502 86 640:  42%|████▏     | 8/19 [00:01<00:02,  5.40it/s]43/99 7.3G 0.08218 0.05247 0.02502 86 640:  47%|████▋     | 9/19 [00:01<00:02,  4.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08241 0.05183 0.02526 86 640:  47%|████▋     | 9/19 [00:01<00:02,  4.24it/s]43/99 7.3G 0.08241 0.05183 0.02526 86 640:  53%|█████▎    | 10/19 [00:01<00:01,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08203 0.0506 0.0251 56 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.56it/s]  43/99 7.3G 0.08203 0.0506 0.0251 56 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08177 0.04898 0.02492 50 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.33it/s]43/99 7.3G 0.08177 0.04898 0.02492 50 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08202 0.04868 0.02482 86 640:  63%|██████▎   | 12/19 [00:04<00:01,  4.34it/s]43/99 7.3G 0.08202 0.04868 0.02482 86 640:  68%|██████▊   | 13/19 [00:04<00:03,  1.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08178 0.04805 0.02463 67 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.57it/s]43/99 7.3G 0.08178 0.04805 0.02463 67 640:  74%|███████▎  | 14/19 [00:05<00:04,  1.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08215 0.04869 0.02473 110 640:  74%|███████▎  | 14/19 [00:06<00:04,  1.17it/s]43/99 7.3G 0.08215 0.04869 0.02473 110 640:  79%|███████▉  | 15/19 [00:06<00:04,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08247 0.04774 0.02466 67 640:  79%|███████▉  | 15/19 [00:10<00:04,  1.05s/it] 43/99 7.3G 0.08247 0.04774 0.02466 67 640:  84%|████████▍ | 16/19 [00:10<00:05,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08221 0.04728 0.02453 67 640:  84%|████████▍ | 16/19 [00:12<00:05,  1.75s/it]43/99 7.3G 0.08221 0.04728 0.02453 67 640:  89%|████████▉ | 17/19 [00:12<00:03,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08175 0.04653 0.02434 55 640:  89%|████████▉ | 17/19 [00:12<00:03,  1.76s/it]43/99 7.3G 0.08175 0.04653 0.02434 55 640:  95%|█████████▍| 18/19 [00:12<00:01,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
43/99 7.3G 0.08139 0.04596 0.02424 56 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.50s/it]43/99 7.3G 0.08139 0.04596 0.02424 56 640: 100%|██████████| 19/19 [00:14<00:00,  1.42s/it]43/99 7.3G 0.08139 0.04596 0.02424 56 640: 100%|██████████| 19/19 [00:14<00:00,  1.34it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.99s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.76s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.55s/it]
                   all         55        256      0.197      0.149       0.12     0.0383
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08003 0.03905 0.02697 66 640:   0%|          | 0/19 [00:00<?, ?it/s]44/99 7.3G 0.08003 0.03905 0.02697 66 640:   5%|▌         | 1/19 [00:00<00:03,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0801 0.03578 0.02408 60 640:   5%|▌         | 1/19 [00:00<00:03,  5.31it/s] 44/99 7.3G 0.0801 0.03578 0.02408 60 640:  11%|█         | 2/19 [00:00<00:03,  5.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08064 0.03669 0.02577 68 640:  11%|█         | 2/19 [00:00<00:03,  5.05it/s]44/99 7.3G 0.08064 0.03669 0.02577 68 640:  16%|█▌        | 3/19 [00:00<00:03,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08115 0.0429 0.02609 103 640:  16%|█▌        | 3/19 [00:01<00:03,  4.11it/s]44/99 7.3G 0.08115 0.0429 0.02609 103 640:  21%|██        | 4/19 [00:01<00:04,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08148 0.04361 0.02585 75 640:  21%|██        | 4/19 [00:01<00:04,  3.66it/s]44/99 7.3G 0.08148 0.04361 0.02585 75 640:  26%|██▋       | 5/19 [00:01<00:04,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08259 0.04547 0.02509 118 640:  26%|██▋       | 5/19 [00:01<00:04,  3.16it/s]44/99 7.3G 0.08259 0.04547 0.02509 118 640:  32%|███▏      | 6/19 [00:01<00:04,  3.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08191 0.04447 0.02432 68 640:  32%|███▏      | 6/19 [00:01<00:04,  3.25it/s] 44/99 7.3G 0.08191 0.04447 0.02432 68 640:  37%|███▋      | 7/19 [00:01<00:03,  3.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08227 0.04479 0.02407 90 640:  37%|███▋      | 7/19 [00:02<00:03,  3.61it/s]44/99 7.3G 0.08227 0.04479 0.02407 90 640:  42%|████▏     | 8/19 [00:02<00:02,  3.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08167 0.04335 0.02348 52 640:  42%|████▏     | 8/19 [00:02<00:02,  3.71it/s]44/99 7.3G 0.08167 0.04335 0.02348 52 640:  47%|████▋     | 9/19 [00:02<00:02,  3.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0818 0.04218 0.02369 54 640:  47%|████▋     | 9/19 [00:02<00:02,  3.68it/s] 44/99 7.3G 0.0818 0.04218 0.02369 54 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.0817 0.04291 0.02362 85 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.66it/s]44/99 7.3G 0.0817 0.04291 0.02362 85 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08115 0.04341 0.02311 80 640:  58%|█████▊    | 11/19 [00:04<00:02,  3.17it/s]44/99 7.3G 0.08115 0.04341 0.02311 80 640:  63%|██████▎   | 12/19 [00:04<00:05,  1.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08134 0.04501 0.02371 116 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.32it/s]44/99 7.3G 0.08134 0.04501 0.02371 116 640:  68%|██████▊   | 13/19 [00:07<00:08,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08192 0.04647 0.02369 127 640:  68%|██████▊   | 13/19 [00:08<00:08,  1.38s/it]44/99 7.3G 0.08192 0.04647 0.02369 127 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08185 0.04687 0.02384 90 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.12s/it] 44/99 7.3G 0.08185 0.04687 0.02384 90 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08144 0.04562 0.02381 41 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.07it/s]44/99 7.3G 0.08144 0.04562 0.02381 41 640:  84%|████████▍ | 16/19 [00:08<00:02,  1.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08073 0.04519 0.02377 58 640:  84%|████████▍ | 16/19 [00:10<00:02,  1.36it/s]44/99 7.3G 0.08073 0.04519 0.02377 58 640:  89%|████████▉ | 17/19 [00:10<00:02,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08033 0.04469 0.02404 55 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.11s/it]44/99 7.3G 0.08033 0.04469 0.02404 55 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
44/99 7.3G 0.08025 0.0443 0.02397 63 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.62s/it] 44/99 7.3G 0.08025 0.0443 0.02397 63 640: 100%|██████████| 19/19 [00:16<00:00,  2.08s/it]44/99 7.3G 0.08025 0.0443 0.02397 63 640: 100%|██████████| 19/19 [00:16<00:00,  1.12it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.49s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.56s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
                   all         55        256      0.398     0.0786       0.11     0.0429
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.0799 0.06132 0.02537 98 640:   0%|          | 0/19 [00:00<?, ?it/s]45/99 7.3G 0.0799 0.06132 0.02537 98 640:   5%|▌         | 1/19 [00:00<00:07,  2.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08052 0.06554 0.02602 108 640:   5%|▌         | 1/19 [00:01<00:07,  2.40it/s]45/99 7.3G 0.08052 0.06554 0.02602 108 640:  11%|█         | 2/19 [00:01<00:09,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08023 0.06014 0.02591 84 640:  11%|█         | 2/19 [00:01<00:09,  1.73it/s] 45/99 7.3G 0.08023 0.06014 0.02591 84 640:  16%|█▌        | 3/19 [00:01<00:09,  1.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08112 0.05595 0.02527 81 640:  16%|█▌        | 3/19 [00:02<00:09,  1.69it/s]45/99 7.3G 0.08112 0.05595 0.02527 81 640:  21%|██        | 4/19 [00:02<00:07,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08227 0.05422 0.02435 104 640:  21%|██        | 4/19 [00:02<00:07,  1.97it/s]45/99 7.3G 0.08227 0.05422 0.02435 104 640:  26%|██▋       | 5/19 [00:02<00:06,  2.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08073 0.05123 0.02394 57 640:  26%|██▋       | 5/19 [00:03<00:06,  2.13it/s] 45/99 7.3G 0.08073 0.05123 0.02394 57 640:  32%|███▏      | 6/19 [00:03<00:06,  2.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08067 0.05053 0.02376 82 640:  32%|███▏      | 6/19 [00:03<00:06,  2.01it/s]45/99 7.3G 0.08067 0.05053 0.02376 82 640:  37%|███▋      | 7/19 [00:03<00:06,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08025 0.0486 0.02394 55 640:  37%|███▋      | 7/19 [00:04<00:06,  1.89it/s] 45/99 7.3G 0.08025 0.0486 0.02394 55 640:  42%|████▏     | 8/19 [00:04<00:05,  1.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07984 0.04743 0.0237 61 640:  42%|████▏     | 8/19 [00:04<00:05,  1.96it/s]45/99 7.3G 0.07984 0.04743 0.0237 61 640:  47%|████▋     | 9/19 [00:04<00:04,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07949 0.04617 0.0238 58 640:  47%|████▋     | 9/19 [00:04<00:04,  2.18it/s]45/99 7.3G 0.07949 0.04617 0.0238 58 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07989 0.04703 0.02345 105 640:  53%|█████▎    | 10/19 [00:05<00:03,  2.35it/s]45/99 7.3G 0.07989 0.04703 0.02345 105 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08031 0.04642 0.02285 87 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.61it/s] 45/99 7.3G 0.08031 0.04642 0.02285 87 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08039 0.04655 0.0226 89 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.04it/s] 45/99 7.3G 0.08039 0.04655 0.0226 89 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07946 0.04502 0.02298 36 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.42it/s]45/99 7.3G 0.07946 0.04502 0.02298 36 640:  74%|███████▎  | 14/19 [00:05<00:01,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.0797 0.04457 0.0229 73 640:  74%|███████▎  | 14/19 [00:09<00:01,  3.86it/s]  45/99 7.3G 0.0797 0.04457 0.0229 73 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07994 0.04436 0.02335 65 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.21s/it]45/99 7.3G 0.07994 0.04436 0.02335 65 640:  84%|████████▍ | 16/19 [00:09<00:03,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.07974 0.0439 0.02334 62 640:  84%|████████▍ | 16/19 [00:14<00:03,  1.09s/it] 45/99 7.3G 0.07974 0.0439 0.02334 62 640:  89%|████████▉ | 17/19 [00:14<00:04,  2.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08006 0.04554 0.02335 125 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.24s/it]45/99 7.3G 0.08006 0.04554 0.02335 125 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
45/99 7.3G 0.08023 0.04542 0.02323 76 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.66s/it] 45/99 7.3G 0.08023 0.04542 0.02323 76 640: 100%|██████████| 19/19 [00:18<00:00,  1.91s/it]45/99 7.3G 0.08023 0.04542 0.02323 76 640: 100%|██████████| 19/19 [00:18<00:00,  1.02it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.09s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
                   all         55        256      0.226      0.127        0.1     0.0349
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.08145 0.0385 0.02639 73 640:   0%|          | 0/19 [00:00<?, ?it/s]46/99 7.3G 0.08145 0.0385 0.02639 73 640:   5%|▌         | 1/19 [00:00<00:07,  2.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.08032 0.03829 0.02534 62 640:   5%|▌         | 1/19 [00:00<00:07,  2.31it/s]46/99 7.3G 0.08032 0.03829 0.02534 62 640:  11%|█         | 2/19 [00:00<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07963 0.03686 0.02546 51 640:  11%|█         | 2/19 [00:01<00:06,  2.50it/s]46/99 7.3G 0.07963 0.03686 0.02546 51 640:  16%|█▌        | 3/19 [00:01<00:07,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07844 0.03755 0.02459 63 640:  16%|█▌        | 3/19 [00:01<00:07,  2.25it/s]46/99 7.3G 0.07844 0.03755 0.02459 63 640:  21%|██        | 4/19 [00:01<00:06,  2.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07811 0.03745 0.02435 60 640:  21%|██        | 4/19 [00:02<00:06,  2.45it/s]46/99 7.3G 0.07811 0.03745 0.02435 60 640:  26%|██▋       | 5/19 [00:02<00:05,  2.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07812 0.03761 0.02339 69 640:  26%|██▋       | 5/19 [00:02<00:05,  2.52it/s]46/99 7.3G 0.07812 0.03761 0.02339 69 640:  32%|███▏      | 6/19 [00:02<00:05,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07843 0.03907 0.02323 80 640:  32%|███▏      | 6/19 [00:03<00:05,  2.23it/s]46/99 7.3G 0.07843 0.03907 0.02323 80 640:  37%|███▋      | 7/19 [00:03<00:05,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07856 0.03817 0.0238 50 640:  37%|███▋      | 7/19 [00:03<00:05,  2.06it/s] 46/99 7.3G 0.07856 0.03817 0.0238 50 640:  42%|████▏     | 8/19 [00:03<00:06,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07881 0.03923 0.02399 77 640:  42%|████▏     | 8/19 [00:04<00:06,  1.82it/s]46/99 7.3G 0.07881 0.03923 0.02399 77 640:  47%|████▋     | 9/19 [00:04<00:05,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07945 0.04085 0.0247 99 640:  47%|████▋     | 9/19 [00:04<00:05,  1.86it/s] 46/99 7.3G 0.07945 0.04085 0.0247 99 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.0799 0.04122 0.02455 84 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.12it/s]46/99 7.3G 0.0799 0.04122 0.02455 84 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07977 0.04067 0.02446 61 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.57it/s]46/99 7.3G 0.07977 0.04067 0.02446 61 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07956 0.03991 0.02408 52 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.71it/s]46/99 7.3G 0.07956 0.03991 0.02408 52 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07942 0.0406 0.02365 85 640:  68%|██████▊   | 13/19 [00:06<00:01,  3.05it/s] 46/99 7.3G 0.07942 0.0406 0.02365 85 640:  74%|███████▎  | 14/19 [00:06<00:02,  1.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07958 0.04068 0.02379 70 640:  74%|███████▎  | 14/19 [00:10<00:02,  1.75it/s]46/99 7.3G 0.07958 0.04068 0.02379 70 640:  79%|███████▉  | 15/19 [00:10<00:06,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07974 0.04069 0.0237 76 640:  79%|███████▉  | 15/19 [00:10<00:06,  1.68s/it] 46/99 7.3G 0.07974 0.04069 0.0237 76 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07973 0.04073 0.0239 66 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.23s/it]46/99 7.3G 0.07973 0.04073 0.0239 66 640:  89%|████████▉ | 17/19 [00:11<00:02,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07975 0.04038 0.02368 62 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.03s/it]46/99 7.3G 0.07975 0.04038 0.02368 62 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
46/99 7.3G 0.07976 0.03991 0.02376 61 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.24s/it]46/99 7.3G 0.07976 0.03991 0.02376 61 640: 100%|██████████| 19/19 [00:13<00:00,  1.08s/it]46/99 7.3G 0.07976 0.03991 0.02376 61 640: 100%|██████████| 19/19 [00:13<00:00,  1.36it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.40s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]
                   all         55        256       0.26       0.15      0.119     0.0394
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07903 0.03963 0.02559 62 640:   0%|          | 0/19 [00:00<?, ?it/s]47/99 7.3G 0.07903 0.03963 0.02559 62 640:   5%|▌         | 1/19 [00:00<00:11,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08131 0.0387 0.02427 67 640:   5%|▌         | 1/19 [00:01<00:11,  1.62it/s] 47/99 7.3G 0.08131 0.0387 0.02427 67 640:  11%|█         | 2/19 [00:01<00:09,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.07987 0.0381 0.0233 59 640:  11%|█         | 2/19 [00:01<00:09,  1.83it/s] 47/99 7.3G 0.07987 0.0381 0.0233 59 640:  16%|█▌        | 3/19 [00:01<00:09,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08076 0.04369 0.0233 112 640:  16%|█▌        | 3/19 [00:02<00:09,  1.62it/s]47/99 7.3G 0.08076 0.04369 0.0233 112 640:  21%|██        | 4/19 [00:02<00:08,  1.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08219 0.04745 0.02297 128 640:  21%|██        | 4/19 [00:03<00:08,  1.68it/s]47/99 7.3G 0.08219 0.04745 0.02297 128 640:  26%|██▋       | 5/19 [00:03<00:08,  1.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08252 0.04692 0.0234 69 640:  26%|██▋       | 5/19 [00:03<00:08,  1.58it/s]  47/99 7.3G 0.08252 0.04692 0.0234 69 640:  32%|███▏      | 6/19 [00:03<00:07,  1.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08253 0.04602 0.0234 65 640:  32%|███▏      | 6/19 [00:04<00:07,  1.67it/s]47/99 7.3G 0.08253 0.04602 0.0234 65 640:  37%|███▋      | 7/19 [00:04<00:07,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08213 0.04458 0.02312 61 640:  37%|███▋      | 7/19 [00:04<00:07,  1.66it/s]47/99 7.3G 0.08213 0.04458 0.02312 61 640:  42%|████▏     | 8/19 [00:04<00:06,  1.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08162 0.04303 0.0231 50 640:  42%|████▏     | 8/19 [00:05<00:06,  1.74it/s] 47/99 7.3G 0.08162 0.04303 0.0231 50 640:  47%|████▋     | 9/19 [00:05<00:05,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08189 0.04452 0.02323 101 640:  47%|████▋     | 9/19 [00:05<00:05,  1.83it/s]47/99 7.3G 0.08189 0.04452 0.02323 101 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08222 0.04548 0.02329 96 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.07it/s] 47/99 7.3G 0.08222 0.04548 0.02329 96 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.0815 0.04413 0.02326 46 640:  58%|█████▊    | 11/19 [00:07<00:03,  2.26it/s] 47/99 7.3G 0.0815 0.04413 0.02326 46 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08174 0.04375 0.0233 75 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.38it/s]47/99 7.3G 0.08174 0.04375 0.0233 75 640:  68%|██████▊   | 13/19 [00:07<00:03,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08161 0.04328 0.02331 62 640:  68%|██████▊   | 13/19 [00:07<00:03,  1.80it/s]47/99 7.3G 0.08161 0.04328 0.02331 62 640:  74%|███████▎  | 14/19 [00:07<00:02,  2.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08155 0.04324 0.02335 76 640:  74%|███████▎  | 14/19 [00:08<00:02,  2.27it/s]47/99 7.3G 0.08155 0.04324 0.02335 76 640:  79%|███████▉  | 15/19 [00:08<00:01,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08089 0.04213 0.02318 41 640:  79%|███████▉  | 15/19 [00:09<00:01,  2.17it/s]47/99 7.3G 0.08089 0.04213 0.02318 41 640:  84%|████████▍ | 16/19 [00:09<00:02,  1.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08116 0.04288 0.02316 106 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.32it/s]47/99 7.3G 0.08116 0.04288 0.02316 106 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.74s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08106 0.04341 0.02294 94 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.74s/it] 47/99 7.3G 0.08106 0.04341 0.02294 94 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
47/99 7.3G 0.08084 0.04341 0.02288 73 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.27s/it]47/99 7.3G 0.08084 0.04341 0.02288 73 640: 100%|██████████| 19/19 [00:19<00:00,  2.56s/it]47/99 7.3G 0.08084 0.04341 0.02288 73 640: 100%|██████████| 19/19 [00:19<00:00,  1.02s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  4.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.14s/it]
                   all         55        256      0.241      0.125      0.113     0.0346
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08904 0.0511 0.02255 101 640:   0%|          | 0/19 [00:00<?, ?it/s]48/99 7.3G 0.08904 0.0511 0.02255 101 640:   5%|▌         | 1/19 [00:00<00:06,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08209 0.04161 0.02172 53 640:   5%|▌         | 1/19 [00:00<00:06,  2.75it/s]48/99 7.3G 0.08209 0.04161 0.02172 53 640:  11%|█         | 2/19 [00:00<00:04,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08331 0.04928 0.02206 118 640:  11%|█         | 2/19 [00:00<00:04,  3.97it/s]48/99 7.3G 0.08331 0.04928 0.02206 118 640:  16%|█▌        | 3/19 [00:00<00:03,  4.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08137 0.044 0.02214 46 640:  16%|█▌        | 3/19 [00:00<00:03,  4.62it/s]   48/99 7.3G 0.08137 0.044 0.02214 46 640:  21%|██        | 4/19 [00:00<00:02,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08148 0.04555 0.02288 86 640:  21%|██        | 4/19 [00:01<00:02,  5.02it/s]48/99 7.3G 0.08148 0.04555 0.02288 86 640:  26%|██▋       | 5/19 [00:01<00:02,  4.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08129 0.04825 0.02286 104 640:  26%|██▋       | 5/19 [00:01<00:02,  4.70it/s]48/99 7.3G 0.08129 0.04825 0.02286 104 640:  32%|███▏      | 6/19 [00:01<00:03,  3.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08091 0.04758 0.02201 81 640:  32%|███▏      | 6/19 [00:01<00:03,  3.69it/s] 48/99 7.3G 0.08091 0.04758 0.02201 81 640:  37%|███▋      | 7/19 [00:01<00:03,  3.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08045 0.04578 0.02162 59 640:  37%|███▋      | 7/19 [00:02<00:03,  3.36it/s]48/99 7.3G 0.08045 0.04578 0.02162 59 640:  42%|████▏     | 8/19 [00:02<00:02,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08021 0.04428 0.02192 55 640:  42%|████▏     | 8/19 [00:02<00:02,  3.87it/s]48/99 7.3G 0.08021 0.04428 0.02192 55 640:  47%|████▋     | 9/19 [00:02<00:02,  4.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08117 0.04782 0.02255 166 640:  47%|████▋     | 9/19 [00:02<00:02,  4.31it/s]48/99 7.3G 0.08117 0.04782 0.02255 166 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08077 0.04746 0.02268 76 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.50it/s] 48/99 7.3G 0.08077 0.04746 0.02268 76 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08081 0.04727 0.02295 77 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.88it/s]48/99 7.3G 0.08081 0.04727 0.02295 77 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08104 0.04769 0.02289 109 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.43it/s]48/99 7.3G 0.08104 0.04769 0.02289 109 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08077 0.04658 0.02316 47 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.46it/s] 48/99 7.3G 0.08077 0.04658 0.02316 47 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08079 0.0453 0.0231 45 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.65it/s]  48/99 7.3G 0.08079 0.0453 0.0231 45 640:  79%|███████▉  | 15/19 [00:05<00:02,  1.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08013 0.04438 0.02277 46 640:  79%|███████▉  | 15/19 [00:09<00:02,  1.85it/s]48/99 7.3G 0.08013 0.04438 0.02277 46 640:  84%|████████▍ | 16/19 [00:09<00:05,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.07976 0.04392 0.02266 64 640:  84%|████████▍ | 16/19 [00:17<00:05,  1.72s/it]48/99 7.3G 0.07976 0.04392 0.02266 64 640:  89%|████████▉ | 17/19 [00:17<00:07,  3.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08042 0.04519 0.02281 146 640:  89%|████████▉ | 17/19 [00:18<00:07,  3.58s/it]48/99 7.3G 0.08042 0.04519 0.02281 146 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
48/99 7.3G 0.08012 0.0451 0.02269 72 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.56s/it]  48/99 7.3G 0.08012 0.0451 0.02269 72 640: 100%|██████████| 19/19 [00:18<00:00,  1.84s/it]48/99 7.3G 0.08012 0.0451 0.02269 72 640: 100%|██████████| 19/19 [00:18<00:00,  1.04it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.30s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.48s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
                   all         55        256      0.274      0.143      0.122     0.0407
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08404 0.04184 0.02005 78 640:   0%|          | 0/19 [00:00<?, ?it/s]49/99 7.3G 0.08404 0.04184 0.02005 78 640:   5%|▌         | 1/19 [00:00<00:06,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08107 0.04279 0.02031 77 640:   5%|▌         | 1/19 [00:00<00:06,  2.67it/s]49/99 7.3G 0.08107 0.04279 0.02031 77 640:  11%|█         | 2/19 [00:00<00:05,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08096 0.03992 0.02173 59 640:  11%|█         | 2/19 [00:00<00:05,  3.12it/s]49/99 7.3G 0.08096 0.03992 0.02173 59 640:  16%|█▌        | 3/19 [00:00<00:05,  3.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08063 0.03812 0.02291 54 640:  16%|█▌        | 3/19 [00:01<00:05,  3.18it/s]49/99 7.3G 0.08063 0.03812 0.02291 54 640:  21%|██        | 4/19 [00:01<00:04,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08204 0.04204 0.02292 106 640:  21%|██        | 4/19 [00:01<00:04,  3.22it/s]49/99 7.3G 0.08204 0.04204 0.02292 106 640:  26%|██▋       | 5/19 [00:01<00:03,  3.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08055 0.04154 0.02286 65 640:  26%|██▋       | 5/19 [00:01<00:03,  3.78it/s] 49/99 7.3G 0.08055 0.04154 0.02286 65 640:  32%|███▏      | 6/19 [00:01<00:03,  4.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08121 0.0473 0.02255 160 640:  32%|███▏      | 6/19 [00:01<00:03,  4.17it/s]49/99 7.3G 0.08121 0.0473 0.02255 160 640:  37%|███▋      | 7/19 [00:01<00:02,  4.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.0826 0.04696 0.02263 101 640:  37%|███▋      | 7/19 [00:02<00:02,  4.38it/s]49/99 7.3G 0.0826 0.04696 0.02263 101 640:  42%|████▏     | 8/19 [00:02<00:02,  4.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08254 0.04688 0.02253 87 640:  42%|████▏     | 8/19 [00:02<00:02,  4.16it/s]49/99 7.3G 0.08254 0.04688 0.02253 87 640:  47%|████▋     | 9/19 [00:02<00:02,  4.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08203 0.04507 0.02232 48 640:  47%|████▋     | 9/19 [00:02<00:02,  4.54it/s]49/99 7.3G 0.08203 0.04507 0.02232 48 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08098 0.04404 0.02212 51 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.85it/s]49/99 7.3G 0.08098 0.04404 0.02212 51 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08096 0.04499 0.02225 100 640:  58%|█████▊    | 11/19 [00:03<00:01,  5.09it/s]49/99 7.3G 0.08096 0.04499 0.02225 100 640:  63%|██████▎   | 12/19 [00:03<00:02,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08041 0.04395 0.02207 52 640:  63%|██████▎   | 12/19 [00:06<00:02,  3.19it/s] 49/99 7.3G 0.08041 0.04395 0.02207 52 640:  68%|██████▊   | 13/19 [00:06<00:06,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07979 0.0428 0.02223 41 640:  68%|██████▊   | 13/19 [00:12<00:06,  1.11s/it] 49/99 7.3G 0.07979 0.0428 0.02223 41 640:  74%|███████▎  | 14/19 [00:12<00:12,  2.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07979 0.04342 0.02255 81 640:  74%|███████▎  | 14/19 [00:12<00:12,  2.54s/it]49/99 7.3G 0.07979 0.04342 0.02255 81 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.83s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07974 0.04384 0.02267 86 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.83s/it]49/99 7.3G 0.07974 0.04384 0.02267 86 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.07988 0.04464 0.02266 97 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.33s/it]49/99 7.3G 0.07988 0.04464 0.02266 97 640:  89%|████████▉ | 17/19 [00:12<00:01,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.0798 0.0445 0.0229 72 640:  89%|████████▉ | 17/19 [00:14<00:01,  1.02it/s]   49/99 7.3G 0.0798 0.0445 0.0229 72 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
49/99 7.3G 0.08019 0.04428 0.02288 83 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.15s/it]49/99 7.3G 0.08019 0.04428 0.02288 83 640: 100%|██████████| 19/19 [00:16<00:00,  1.65s/it]49/99 7.3G 0.08019 0.04428 0.02288 83 640: 100%|██████████| 19/19 [00:16<00:00,  1.12it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.44s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.53s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
                   all         55        256      0.211      0.154      0.123     0.0414
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07916 0.04452 0.02144 75 640:   0%|          | 0/19 [00:00<?, ?it/s]50/99 7.3G 0.07916 0.04452 0.02144 75 640:   5%|▌         | 1/19 [00:00<00:06,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.07973 0.0459 0.02436 77 640:   5%|▌         | 1/19 [00:00<00:06,  2.57it/s] 50/99 7.3G 0.07973 0.0459 0.02436 77 640:  11%|█         | 2/19 [00:00<00:07,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.0817 0.05425 0.02469 119 640:  11%|█         | 2/19 [00:01<00:07,  2.26it/s]50/99 7.3G 0.0817 0.05425 0.02469 119 640:  16%|█▌        | 3/19 [00:01<00:08,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08168 0.05102 0.02469 77 640:  16%|█▌        | 3/19 [00:02<00:08,  1.99it/s]50/99 7.3G 0.08168 0.05102 0.02469 77 640:  21%|██        | 4/19 [00:02<00:08,  1.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08103 0.0498 0.02417 69 640:  21%|██        | 4/19 [00:02<00:08,  1.86it/s] 50/99 7.3G 0.08103 0.0498 0.02417 69 640:  26%|██▋       | 5/19 [00:02<00:06,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08133 0.04738 0.02328 64 640:  26%|██▋       | 5/19 [00:02<00:06,  2.33it/s]50/99 7.3G 0.08133 0.04738 0.02328 64 640:  32%|███▏      | 6/19 [00:02<00:04,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08207 0.04645 0.02278 86 640:  32%|███▏      | 6/19 [00:02<00:04,  2.92it/s]50/99 7.3G 0.08207 0.04645 0.02278 86 640:  37%|███▋      | 7/19 [00:02<00:03,  3.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08231 0.04599 0.02244 84 640:  37%|███▋      | 7/19 [00:02<00:03,  3.48it/s]50/99 7.3G 0.08231 0.04599 0.02244 84 640:  42%|████▏     | 8/19 [00:02<00:02,  3.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08133 0.04418 0.02261 50 640:  42%|████▏     | 8/19 [00:02<00:02,  3.98it/s]50/99 7.3G 0.08133 0.04418 0.02261 50 640:  47%|████▋     | 9/19 [00:02<00:02,  4.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08131 0.04514 0.02301 87 640:  47%|████▋     | 9/19 [00:03<00:02,  4.41it/s]50/99 7.3G 0.08131 0.04514 0.02301 87 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08105 0.045 0.02282 78 640:  53%|█████▎    | 10/19 [00:08<00:01,  4.59it/s]  50/99 7.3G 0.08105 0.045 0.02282 78 640:  58%|█████▊    | 11/19 [00:08<00:14,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08075 0.04459 0.02289 73 640:  58%|█████▊    | 11/19 [00:08<00:14,  1.79s/it]50/99 7.3G 0.08075 0.04459 0.02289 73 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08028 0.04358 0.02289 51 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.30s/it]50/99 7.3G 0.08028 0.04358 0.02289 51 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08053 0.04267 0.02282 58 640:  68%|██████▊   | 13/19 [00:09<00:05,  1.04it/s]50/99 7.3G 0.08053 0.04267 0.02282 58 640:  74%|███████▎  | 14/19 [00:09<00:03,  1.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08045 0.04205 0.02268 56 640:  74%|███████▎  | 14/19 [00:09<00:03,  1.38it/s]50/99 7.3G 0.08045 0.04205 0.02268 56 640:  79%|███████▉  | 15/19 [00:09<00:02,  1.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08055 0.04231 0.02286 81 640:  79%|███████▉  | 15/19 [00:11<00:02,  1.79it/s]50/99 7.3G 0.08055 0.04231 0.02286 81 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08037 0.04229 0.02315 65 640:  84%|████████▍ | 16/19 [00:12<00:02,  1.07it/s]50/99 7.3G 0.08037 0.04229 0.02315 65 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.08061 0.04288 0.02305 106 640:  89%|████████▉ | 17/19 [00:16<00:02,  1.20s/it]50/99 7.3G 0.08061 0.04288 0.02305 106 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
50/99 7.3G 0.0803 0.04261 0.02269 66 640:  95%|█████████▍| 18/19 [00:24<00:01,  1.79s/it]  50/99 7.3G 0.0803 0.04261 0.02269 66 640: 100%|██████████| 19/19 [00:24<00:00,  3.87s/it]50/99 7.3G 0.0803 0.04261 0.02269 66 640: 100%|██████████| 19/19 [00:24<00:00,  1.30s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]
                   all         55        256       0.23      0.154      0.114     0.0402
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07842 0.03138 0.02579 49 640:   0%|          | 0/19 [00:00<?, ?it/s]51/99 7.3G 0.07842 0.03138 0.02579 49 640:   5%|▌         | 1/19 [00:00<00:07,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07735 0.03789 0.02329 72 640:   5%|▌         | 1/19 [00:00<00:07,  2.42it/s]51/99 7.3G 0.07735 0.03789 0.02329 72 640:  11%|█         | 2/19 [00:00<00:06,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07868 0.04339 0.02216 94 640:  11%|█         | 2/19 [00:01<00:06,  2.67it/s]51/99 7.3G 0.07868 0.04339 0.02216 94 640:  16%|█▌        | 3/19 [00:01<00:05,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08024 0.04413 0.02244 96 640:  16%|█▌        | 3/19 [00:01<00:05,  2.76it/s]51/99 7.3G 0.08024 0.04413 0.02244 96 640:  21%|██        | 4/19 [00:01<00:05,  2.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07943 0.04488 0.02368 74 640:  21%|██        | 4/19 [00:01<00:05,  2.84it/s]51/99 7.3G 0.07943 0.04488 0.02368 74 640:  26%|██▋       | 5/19 [00:01<00:04,  3.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08006 0.0446 0.0228 89 640:  26%|██▋       | 5/19 [00:02<00:04,  3.07it/s]  51/99 7.3G 0.08006 0.0446 0.0228 89 640:  32%|███▏      | 6/19 [00:02<00:04,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08062 0.04444 0.023 78 640:  32%|███▏      | 6/19 [00:02<00:04,  3.19it/s]51/99 7.3G 0.08062 0.04444 0.023 78 640:  37%|███▋      | 7/19 [00:02<00:03,  3.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07961 0.04302 0.02265 52 640:  37%|███▋      | 7/19 [00:02<00:03,  3.58it/s]51/99 7.3G 0.07961 0.04302 0.02265 52 640:  42%|████▏     | 8/19 [00:02<00:02,  4.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.07992 0.04307 0.0233 70 640:  42%|████▏     | 8/19 [00:02<00:02,  4.07it/s] 51/99 7.3G 0.07992 0.04307 0.0233 70 640:  47%|████▋     | 9/19 [00:02<00:02,  4.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08041 0.04217 0.02282 68 640:  47%|████▋     | 9/19 [00:02<00:02,  4.48it/s]51/99 7.3G 0.08041 0.04217 0.02282 68 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08049 0.0424 0.02298 80 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.64it/s] 51/99 7.3G 0.08049 0.0424 0.02298 80 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08042 0.04152 0.02272 56 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.26it/s]51/99 7.3G 0.08042 0.04152 0.02272 56 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08105 0.04334 0.02265 136 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.94it/s]51/99 7.3G 0.08105 0.04334 0.02265 136 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08101 0.04385 0.02282 83 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.17it/s] 51/99 7.3G 0.08101 0.04385 0.02282 83 640:  74%|███████▎  | 14/19 [00:03<00:01,  4.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08055 0.04307 0.0226 57 640:  74%|███████▎  | 14/19 [00:04<00:01,  4.35it/s] 51/99 7.3G 0.08055 0.04307 0.0226 57 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08073 0.04433 0.02282 118 640:  79%|███████▉  | 15/19 [00:15<00:01,  2.65it/s]51/99 7.3G 0.08073 0.04433 0.02282 118 640:  84%|████████▍ | 16/19 [00:15<00:10,  3.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08101 0.04464 0.02272 96 640:  84%|████████▍ | 16/19 [00:17<00:10,  3.44s/it] 51/99 7.3G 0.08101 0.04464 0.02272 96 640:  89%|████████▉ | 17/19 [00:17<00:06,  3.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08121 0.04461 0.02261 85 640:  89%|████████▉ | 17/19 [00:18<00:06,  3.26s/it]51/99 7.3G 0.08121 0.04461 0.02261 85 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
51/99 7.3G 0.08134 0.0436 0.023 40 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.33s/it]   51/99 7.3G 0.08134 0.0436 0.023 40 640: 100%|██████████| 19/19 [00:18<00:00,  1.86s/it]51/99 7.3G 0.08134 0.0436 0.023 40 640: 100%|██████████| 19/19 [00:18<00:00,  1.01it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.92s/it]
                   all         55        256      0.226      0.197      0.132      0.044
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.08423 0.05182 0.02687 91 640:   0%|          | 0/19 [00:00<?, ?it/s]52/99 7.3G 0.08423 0.05182 0.02687 91 640:   5%|▌         | 1/19 [00:00<00:03,  4.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07951 0.0431 0.02401 54 640:   5%|▌         | 1/19 [00:00<00:03,  4.73it/s] 52/99 7.3G 0.07951 0.0431 0.02401 54 640:  11%|█         | 2/19 [00:00<00:03,  4.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07607 0.03825 0.02335 45 640:  11%|█         | 2/19 [00:00<00:03,  4.78it/s]52/99 7.3G 0.07607 0.03825 0.02335 45 640:  16%|█▌        | 3/19 [00:00<00:03,  4.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07531 0.03829 0.02182 62 640:  16%|█▌        | 3/19 [00:00<00:03,  4.82it/s]52/99 7.3G 0.07531 0.03829 0.02182 62 640:  21%|██        | 4/19 [00:00<00:02,  5.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.077 0.04048 0.02118 103 640:  21%|██        | 4/19 [00:00<00:02,  5.10it/s] 52/99 7.3G 0.077 0.04048 0.02118 103 640:  26%|██▋       | 5/19 [00:00<00:02,  5.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07734 0.04201 0.02118 84 640:  26%|██▋       | 5/19 [00:01<00:02,  5.32it/s]52/99 7.3G 0.07734 0.04201 0.02118 84 640:  32%|███▏      | 6/19 [00:01<00:02,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.0769 0.04114 0.02124 56 640:  32%|███▏      | 6/19 [00:01<00:02,  4.55it/s] 52/99 7.3G 0.0769 0.04114 0.02124 56 640:  37%|███▋      | 7/19 [00:01<00:02,  4.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07735 0.03964 0.02132 51 640:  37%|███▋      | 7/19 [00:01<00:02,  4.07it/s]52/99 7.3G 0.07735 0.03964 0.02132 51 640:  42%|████▏     | 8/19 [00:01<00:02,  4.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07721 0.03921 0.02192 51 640:  42%|████▏     | 8/19 [00:02<00:02,  4.03it/s]52/99 7.3G 0.07721 0.03921 0.02192 51 640:  47%|████▋     | 9/19 [00:02<00:02,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.0772 0.03954 0.02198 64 640:  47%|████▋     | 9/19 [00:02<00:02,  3.93it/s] 52/99 7.3G 0.0772 0.03954 0.02198 64 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07731 0.03966 0.02168 76 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.36it/s]52/99 7.3G 0.07731 0.03966 0.02168 76 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07786 0.04218 0.02188 122 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.71it/s]52/99 7.3G 0.07786 0.04218 0.02188 122 640:  63%|██████▎   | 12/19 [00:03<00:03,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07756 0.04144 0.02159 53 640:  63%|██████▎   | 12/19 [00:10<00:03,  2.12it/s] 52/99 7.3G 0.07756 0.04144 0.02159 53 640:  68%|██████▊   | 13/19 [00:10<00:14,  2.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07747 0.04102 0.02153 59 640:  68%|██████▊   | 13/19 [00:11<00:14,  2.42s/it]52/99 7.3G 0.07747 0.04102 0.02153 59 640:  74%|███████▎  | 14/19 [00:11<00:10,  2.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07746 0.04094 0.02163 68 640:  74%|███████▎  | 14/19 [00:12<00:10,  2.14s/it]52/99 7.3G 0.07746 0.04094 0.02163 68 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.0778 0.04167 0.02197 82 640:  79%|███████▉  | 15/19 [00:13<00:06,  1.55s/it] 52/99 7.3G 0.0778 0.04167 0.02197 82 640:  84%|████████▍ | 16/19 [00:13<00:04,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07827 0.04178 0.02215 83 640:  84%|████████▍ | 16/19 [00:14<00:04,  1.39s/it]52/99 7.3G 0.07827 0.04178 0.02215 83 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07864 0.04199 0.02201 99 640:  89%|████████▉ | 17/19 [00:15<00:02,  1.27s/it]52/99 7.3G 0.07864 0.04199 0.02201 99 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
52/99 7.3G 0.07876 0.04266 0.02191 102 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.18s/it]52/99 7.3G 0.07876 0.04266 0.02191 102 640: 100%|██████████| 19/19 [00:15<00:00,  1.14it/s]52/99 7.3G 0.07876 0.04266 0.02191 102 640: 100%|██████████| 19/19 [00:15<00:00,  1.25it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.54s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
                   all         55        256      0.234      0.082      0.113     0.0408
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07884 0.02742 0.02149 51 640:   0%|          | 0/19 [00:00<?, ?it/s]53/99 7.3G 0.07884 0.02742 0.02149 51 640:   5%|▌         | 1/19 [00:00<00:04,  3.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07929 0.02985 0.02388 52 640:   5%|▌         | 1/19 [00:00<00:04,  3.78it/s]53/99 7.3G 0.07929 0.02985 0.02388 52 640:  11%|█         | 2/19 [00:00<00:04,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.07928 0.02868 0.02225 46 640:  11%|█         | 2/19 [00:00<00:04,  3.46it/s]53/99 7.3G 0.07928 0.02868 0.02225 46 640:  16%|█▌        | 3/19 [00:00<00:05,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08159 0.03088 0.0222 77 640:  16%|█▌        | 3/19 [00:01<00:05,  3.16it/s] 53/99 7.3G 0.08159 0.03088 0.0222 77 640:  21%|██        | 4/19 [00:01<00:04,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08194 0.03178 0.0241 56 640:  21%|██        | 4/19 [00:01<00:04,  3.04it/s]53/99 7.3G 0.08194 0.03178 0.0241 56 640:  26%|██▋       | 5/19 [00:01<00:04,  3.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08375 0.03959 0.02349 200 640:  26%|██▋       | 5/19 [00:01<00:04,  3.27it/s]53/99 7.3G 0.08375 0.03959 0.02349 200 640:  32%|███▏      | 6/19 [00:01<00:03,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08253 0.03775 0.02313 39 640:  32%|███▏      | 6/19 [00:01<00:03,  3.83it/s] 53/99 7.3G 0.08253 0.03775 0.02313 39 640:  37%|███▋      | 7/19 [00:01<00:02,  4.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08285 0.03918 0.02315 91 640:  37%|███▋      | 7/19 [00:02<00:02,  4.30it/s]53/99 7.3G 0.08285 0.03918 0.02315 91 640:  42%|████▏     | 8/19 [00:02<00:02,  4.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08202 0.03923 0.02323 61 640:  42%|████▏     | 8/19 [00:02<00:02,  4.67it/s]53/99 7.3G 0.08202 0.03923 0.02323 61 640:  47%|████▋     | 9/19 [00:02<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08191 0.04004 0.02371 77 640:  47%|████▋     | 9/19 [00:07<00:02,  4.95it/s]53/99 7.3G 0.08191 0.04004 0.02371 77 640:  53%|█████▎    | 10/19 [00:07<00:15,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08157 0.04 0.02318 67 640:  53%|█████▎    | 10/19 [00:07<00:15,  1.75s/it]   53/99 7.3G 0.08157 0.04 0.02318 67 640:  58%|█████▊    | 11/19 [00:07<00:10,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.0815 0.03891 0.0227 47 640:  58%|█████▊    | 11/19 [00:08<00:10,  1.37s/it]53/99 7.3G 0.0815 0.03891 0.0227 47 640:  63%|██████▎   | 12/19 [00:08<00:07,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08161 0.03866 0.02291 62 640:  63%|██████▎   | 12/19 [00:08<00:07,  1.01s/it]53/99 7.3G 0.08161 0.03866 0.02291 62 640:  68%|██████▊   | 13/19 [00:08<00:04,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08089 0.03809 0.02259 52 640:  68%|██████▊   | 13/19 [00:09<00:04,  1.33it/s]53/99 7.3G 0.08089 0.03809 0.02259 52 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.0808 0.03811 0.02282 63 640:  74%|███████▎  | 14/19 [00:11<00:04,  1.22it/s] 53/99 7.3G 0.0808 0.03811 0.02282 63 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08071 0.03816 0.02265 69 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.13s/it]53/99 7.3G 0.08071 0.03816 0.02265 69 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08036 0.03748 0.02255 43 640:  84%|████████▍ | 16/19 [00:15<00:02,  1.01it/s]53/99 7.3G 0.08036 0.03748 0.02255 43 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.72s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08019 0.03753 0.02257 65 640:  89%|████████▉ | 17/19 [00:23<00:03,  1.72s/it]53/99 7.3G 0.08019 0.03753 0.02257 65 640:  95%|█████████▍| 18/19 [00:23<00:03,  3.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
53/99 7.3G 0.08008 0.03709 0.02252 55 640:  95%|█████████▍| 18/19 [00:23<00:03,  3.69s/it]53/99 7.3G 0.08008 0.03709 0.02252 55 640: 100%|██████████| 19/19 [00:23<00:00,  2.68s/it]53/99 7.3G 0.08008 0.03709 0.02252 55 640: 100%|██████████| 19/19 [00:23<00:00,  1.25s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.22s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]
                   all         55        256       0.38      0.168      0.149     0.0394
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.082 0.06409 0.02422 112 640:   0%|          | 0/19 [00:00<?, ?it/s]54/99 7.3G 0.082 0.06409 0.02422 112 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08397 0.06386 0.02341 120 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]54/99 7.3G 0.08397 0.06386 0.02341 120 640:  11%|█         | 2/19 [00:00<00:03,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08442 0.05557 0.02314 80 640:  11%|█         | 2/19 [00:00<00:03,  4.56it/s] 54/99 7.3G 0.08442 0.05557 0.02314 80 640:  16%|█▌        | 3/19 [00:00<00:04,  3.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08292 0.05471 0.02244 91 640:  16%|█▌        | 3/19 [00:00<00:04,  3.99it/s]54/99 7.3G 0.08292 0.05471 0.02244 91 640:  21%|██        | 4/19 [00:00<00:03,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.0806 0.04943 0.02361 40 640:  21%|██        | 4/19 [00:01<00:03,  3.87it/s] 54/99 7.3G 0.0806 0.04943 0.02361 40 640:  26%|██▋       | 5/19 [00:01<00:04,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07952 0.04717 0.02346 53 640:  26%|██▋       | 5/19 [00:01<00:04,  3.41it/s]54/99 7.3G 0.07952 0.04717 0.02346 53 640:  32%|███▏      | 6/19 [00:01<00:03,  3.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07893 0.04579 0.02271 64 640:  32%|███▏      | 6/19 [00:01<00:03,  3.95it/s]54/99 7.3G 0.07893 0.04579 0.02271 64 640:  37%|███▋      | 7/19 [00:01<00:02,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.0791 0.04524 0.02253 72 640:  37%|███▋      | 7/19 [00:01<00:02,  4.40it/s] 54/99 7.3G 0.0791 0.04524 0.02253 72 640:  42%|████▏     | 8/19 [00:01<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07928 0.04583 0.02268 81 640:  42%|████▏     | 8/19 [00:02<00:02,  4.74it/s]54/99 7.3G 0.07928 0.04583 0.02268 81 640:  47%|████▋     | 9/19 [00:02<00:02,  4.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07925 0.04658 0.02272 97 640:  47%|████▋     | 9/19 [00:02<00:02,  4.85it/s]54/99 7.3G 0.07925 0.04658 0.02272 97 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07959 0.04744 0.02251 104 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.19it/s]54/99 7.3G 0.07959 0.04744 0.02251 104 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08038 0.04644 0.02236 81 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.37it/s] 54/99 7.3G 0.08038 0.04644 0.02236 81 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08072 0.04698 0.02227 107 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.55it/s]54/99 7.3G 0.08072 0.04698 0.02227 107 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08037 0.04656 0.02214 73 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.86it/s] 54/99 7.3G 0.08037 0.04656 0.02214 73 640:  74%|███████▎  | 14/19 [00:03<00:00,  5.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08071 0.04642 0.02192 88 640:  74%|███████▎  | 14/19 [00:13<00:00,  5.11it/s]54/99 7.3G 0.08071 0.04642 0.02192 88 640:  79%|███████▉  | 15/19 [00:13<00:12,  3.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08051 0.04575 0.02215 60 640:  79%|███████▉  | 15/19 [00:14<00:12,  3.16s/it]54/99 7.3G 0.08051 0.04575 0.02215 60 640:  84%|████████▍ | 16/19 [00:14<00:07,  2.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.08012 0.04525 0.02236 54 640:  84%|████████▍ | 16/19 [00:16<00:07,  2.53s/it]54/99 7.3G 0.08012 0.04525 0.02236 54 640:  89%|████████▉ | 17/19 [00:16<00:04,  2.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07988 0.04552 0.02272 78 640:  89%|████████▉ | 17/19 [00:16<00:04,  2.38s/it]54/99 7.3G 0.07988 0.04552 0.02272 78 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.82s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
54/99 7.3G 0.07986 0.0447 0.02283 47 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.82s/it] 54/99 7.3G 0.07986 0.0447 0.02283 47 640: 100%|██████████| 19/19 [00:17<00:00,  1.40s/it]54/99 7.3G 0.07986 0.0447 0.02283 47 640: 100%|██████████| 19/19 [00:17<00:00,  1.11it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.61s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]
                   all         55        256        0.3       0.16      0.136     0.0328
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.08261 0.04721 0.01893 83 640:   0%|          | 0/19 [00:00<?, ?it/s]55/99 7.3G 0.08261 0.04721 0.01893 83 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0828 0.0462 0.02198 77 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]  55/99 7.3G 0.0828 0.0462 0.02198 77 640:  11%|█         | 2/19 [00:00<00:03,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.08026 0.04192 0.02279 52 640:  11%|█         | 2/19 [00:00<00:03,  5.50it/s]55/99 7.3G 0.08026 0.04192 0.02279 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0793 0.04094 0.02271 62 640:  16%|█▌        | 3/19 [00:00<00:02,  5.62it/s] 55/99 7.3G 0.0793 0.04094 0.02271 62 640:  21%|██        | 4/19 [00:00<00:03,  4.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07882 0.03991 0.02263 58 640:  21%|██        | 4/19 [00:01<00:03,  4.76it/s]55/99 7.3G 0.07882 0.03991 0.02263 58 640:  26%|██▋       | 5/19 [00:01<00:03,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0792 0.04046 0.02366 64 640:  26%|██▋       | 5/19 [00:01<00:03,  4.11it/s] 55/99 7.3G 0.0792 0.04046 0.02366 64 640:  32%|███▏      | 6/19 [00:01<00:03,  3.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07854 0.04047 0.02448 62 640:  32%|███▏      | 6/19 [00:01<00:03,  3.77it/s]55/99 7.3G 0.07854 0.04047 0.02448 62 640:  37%|███▋      | 7/19 [00:01<00:03,  3.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0779 0.03949 0.02458 48 640:  37%|███▋      | 7/19 [00:02<00:03,  3.59it/s] 55/99 7.3G 0.0779 0.03949 0.02458 48 640:  42%|████▏     | 8/19 [00:02<00:03,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07823 0.04009 0.02442 79 640:  42%|████▏     | 8/19 [00:02<00:03,  3.16it/s]55/99 7.3G 0.07823 0.04009 0.02442 79 640:  47%|████▋     | 9/19 [00:02<00:03,  3.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07863 0.04272 0.0243 116 640:  47%|████▋     | 9/19 [00:02<00:03,  3.01it/s]55/99 7.3G 0.07863 0.04272 0.0243 116 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07963 0.04681 0.02426 172 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.40it/s]55/99 7.3G 0.07963 0.04681 0.02426 172 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07955 0.04586 0.02415 61 640:  58%|█████▊    | 11/19 [00:07<00:02,  3.74it/s] 55/99 7.3G 0.07955 0.04586 0.02415 61 640:  63%|██████▎   | 12/19 [00:07<00:11,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.0794 0.04461 0.02403 45 640:  63%|██████▎   | 12/19 [00:08<00:11,  1.59s/it] 55/99 7.3G 0.0794 0.04461 0.02403 45 640:  68%|██████▊   | 13/19 [00:08<00:08,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07944 0.04407 0.02379 70 640:  68%|██████▊   | 13/19 [00:10<00:08,  1.44s/it]55/99 7.3G 0.07944 0.04407 0.02379 70 640:  74%|███████▎  | 14/19 [00:10<00:08,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07951 0.04315 0.02408 50 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.71s/it]55/99 7.3G 0.07951 0.04315 0.02408 50 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07953 0.04388 0.02407 97 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.25s/it]55/99 7.3G 0.07953 0.04388 0.02407 97 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07957 0.04337 0.02396 60 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.00it/s]55/99 7.3G 0.07957 0.04337 0.02396 60 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07958 0.04292 0.02385 55 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.43s/it]55/99 7.3G 0.07958 0.04292 0.02385 55 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
55/99 7.3G 0.07929 0.04274 0.02353 66 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.05s/it]55/99 7.3G 0.07929 0.04274 0.02353 66 640: 100%|██████████| 19/19 [00:14<00:00,  1.11it/s]55/99 7.3G 0.07929 0.04274 0.02353 66 640: 100%|██████████| 19/19 [00:14<00:00,  1.30it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.85s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.88s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]
                   all         55        256      0.236      0.172      0.125     0.0356
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.08003 0.05219 0.02355 89 640:   0%|          | 0/19 [00:00<?, ?it/s]56/99 7.3G 0.08003 0.05219 0.02355 89 640:   5%|▌         | 1/19 [00:00<00:09,  1.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.08043 0.04284 0.0206 65 640:   5%|▌         | 1/19 [00:01<00:09,  1.91it/s] 56/99 7.3G 0.08043 0.04284 0.0206 65 640:  11%|█         | 2/19 [00:01<00:09,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07869 0.0369 0.02292 38 640:  11%|█         | 2/19 [00:01<00:09,  1.78it/s]56/99 7.3G 0.07869 0.0369 0.02292 38 640:  16%|█▌        | 3/19 [00:01<00:06,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07984 0.0426 0.02285 111 640:  16%|█▌        | 3/19 [00:01<00:06,  2.42it/s]56/99 7.3G 0.07984 0.0426 0.02285 111 640:  21%|██        | 4/19 [00:01<00:04,  3.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07966 0.0404 0.02272 54 640:  21%|██        | 4/19 [00:01<00:04,  3.13it/s] 56/99 7.3G 0.07966 0.0404 0.02272 54 640:  26%|██▋       | 5/19 [00:01<00:03,  3.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07884 0.03987 0.02259 65 640:  26%|██▋       | 5/19 [00:01<00:03,  3.75it/s]56/99 7.3G 0.07884 0.03987 0.02259 65 640:  32%|███▏      | 6/19 [00:01<00:03,  4.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07952 0.04228 0.02255 97 640:  32%|███▏      | 6/19 [00:02<00:03,  4.24it/s]56/99 7.3G 0.07952 0.04228 0.02255 97 640:  37%|███▋      | 7/19 [00:02<00:02,  4.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07953 0.04501 0.02305 108 640:  37%|███▋      | 7/19 [00:02<00:02,  4.48it/s]56/99 7.3G 0.07953 0.04501 0.02305 108 640:  42%|████▏     | 8/19 [00:02<00:02,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07926 0.04512 0.02308 74 640:  42%|████▏     | 8/19 [00:06<00:02,  4.55it/s] 56/99 7.3G 0.07926 0.04512 0.02308 74 640:  47%|████▋     | 9/19 [00:06<00:13,  1.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07894 0.04477 0.02354 62 640:  47%|████▋     | 9/19 [00:07<00:13,  1.32s/it]56/99 7.3G 0.07894 0.04477 0.02354 62 640:  53%|█████▎    | 10/19 [00:07<00:11,  1.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07912 0.04493 0.02347 77 640:  53%|█████▎    | 10/19 [00:09<00:11,  1.32s/it]56/99 7.3G 0.07912 0.04493 0.02347 77 640:  58%|█████▊    | 11/19 [00:09<00:13,  1.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07889 0.04443 0.02346 69 640:  58%|█████▊    | 11/19 [00:10<00:13,  1.66s/it]56/99 7.3G 0.07889 0.04443 0.02346 69 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07882 0.04316 0.02368 45 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.27s/it]56/99 7.3G 0.07882 0.04316 0.02368 45 640:  68%|██████▊   | 13/19 [00:10<00:05,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07922 0.04231 0.02378 59 640:  68%|██████▊   | 13/19 [00:12<00:05,  1.02it/s]56/99 7.3G 0.07922 0.04231 0.02378 59 640:  74%|███████▎  | 14/19 [00:12<00:05,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07914 0.04268 0.02375 84 640:  74%|███████▎  | 14/19 [00:13<00:05,  1.15s/it]56/99 7.3G 0.07914 0.04268 0.02375 84 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07888 0.04229 0.02361 57 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.29s/it]56/99 7.3G 0.07888 0.04229 0.02361 57 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.0794 0.04325 0.02345 127 640:  84%|████████▍ | 16/19 [00:20<00:03,  1.00s/it]56/99 7.3G 0.0794 0.04325 0.02345 127 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.77s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.0795 0.04442 0.02362 115 640:  89%|████████▉ | 17/19 [00:22<00:05,  2.77s/it]56/99 7.3G 0.0795 0.04442 0.02362 115 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
56/99 7.3G 0.07954 0.04389 0.02363 59 640:  95%|█████████▍| 18/19 [00:24<00:02,  2.45s/it]56/99 7.3G 0.07954 0.04389 0.02363 59 640: 100%|██████████| 19/19 [00:24<00:00,  2.21s/it]56/99 7.3G 0.07954 0.04389 0.02363 59 640: 100%|██████████| 19/19 [00:24<00:00,  1.27s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.32s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.66s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]
                   all         55        256      0.196      0.183      0.143     0.0411
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07645 0.04438 0.02178 75 640:   0%|          | 0/19 [00:00<?, ?it/s]57/99 7.3G 0.07645 0.04438 0.02178 75 640:   5%|▌         | 1/19 [00:00<00:04,  4.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08427 0.06175 0.02127 195 640:   5%|▌         | 1/19 [00:00<00:04,  4.35it/s]57/99 7.3G 0.08427 0.06175 0.02127 195 640:  11%|█         | 2/19 [00:00<00:04,  4.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08516 0.05443 0.02006 93 640:  11%|█         | 2/19 [00:00<00:04,  4.03it/s] 57/99 7.3G 0.08516 0.05443 0.02006 93 640:  16%|█▌        | 3/19 [00:00<00:03,  4.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08234 0.04891 0.02104 50 640:  16%|█▌        | 3/19 [00:00<00:03,  4.23it/s]57/99 7.3G 0.08234 0.04891 0.02104 50 640:  21%|██        | 4/19 [00:00<00:03,  4.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08117 0.04556 0.02123 51 640:  21%|██        | 4/19 [00:01<00:03,  4.45it/s]57/99 7.3G 0.08117 0.04556 0.02123 51 640:  26%|██▋       | 5/19 [00:01<00:03,  4.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08179 0.04693 0.02228 94 640:  26%|██▋       | 5/19 [00:01<00:03,  4.58it/s]57/99 7.3G 0.08179 0.04693 0.02228 94 640:  32%|███▏      | 6/19 [00:01<00:02,  4.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08147 0.04716 0.02213 83 640:  32%|███▏      | 6/19 [00:01<00:02,  4.68it/s]57/99 7.3G 0.08147 0.04716 0.02213 83 640:  37%|███▋      | 7/19 [00:01<00:02,  4.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08166 0.04578 0.02195 63 640:  37%|███▋      | 7/19 [00:01<00:02,  4.93it/s]57/99 7.3G 0.08166 0.04578 0.02195 63 640:  42%|████▏     | 8/19 [00:01<00:02,  5.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08144 0.04657 0.02197 88 640:  42%|████▏     | 8/19 [00:01<00:02,  5.17it/s]57/99 7.3G 0.08144 0.04657 0.02197 88 640:  47%|████▋     | 9/19 [00:01<00:01,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08125 0.04594 0.02236 67 640:  47%|████▋     | 9/19 [00:02<00:01,  5.35it/s]57/99 7.3G 0.08125 0.04594 0.02236 67 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08083 0.0452 0.0228 70 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.26it/s]  57/99 7.3G 0.08083 0.0452 0.0228 70 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08072 0.04486 0.02294 69 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.10it/s]57/99 7.3G 0.08072 0.04486 0.02294 69 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08056 0.04449 0.02297 75 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.02it/s]57/99 7.3G 0.08056 0.04449 0.02297 75 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08024 0.04381 0.02271 61 640:  68%|██████▊   | 13/19 [00:09<00:01,  4.94it/s]57/99 7.3G 0.08024 0.04381 0.02271 61 640:  74%|███████▎  | 14/19 [00:09<00:10,  2.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07956 0.04327 0.02261 54 640:  74%|███████▎  | 14/19 [00:10<00:10,  2.08s/it]57/99 7.3G 0.07956 0.04327 0.02261 54 640:  79%|███████▉  | 15/19 [00:10<00:07,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07971 0.04438 0.02254 104 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.79s/it]57/99 7.3G 0.07971 0.04438 0.02254 104 640:  84%|████████▍ | 16/19 [00:12<00:05,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.07991 0.04468 0.0228 82 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.85s/it]  57/99 7.3G 0.07991 0.04468 0.0228 82 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08036 0.04473 0.02263 126 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.67s/it]57/99 7.3G 0.08036 0.04473 0.02263 126 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
57/99 7.3G 0.08045 0.04496 0.02266 83 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.23s/it] 57/99 7.3G 0.08045 0.04496 0.02266 83 640: 100%|██████████| 19/19 [00:14<00:00,  1.02it/s]57/99 7.3G 0.08045 0.04496 0.02266 83 640: 100%|██████████| 19/19 [00:14<00:00,  1.35it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.28s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
                   all         55        256      0.187      0.181       0.13     0.0356
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07426 0.02955 0.01891 50 640:   0%|          | 0/19 [00:00<?, ?it/s]58/99 7.3G 0.07426 0.02955 0.01891 50 640:   5%|▌         | 1/19 [00:00<00:03,  5.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07749 0.03963 0.02057 89 640:   5%|▌         | 1/19 [00:00<00:03,  5.34it/s]58/99 7.3G 0.07749 0.03963 0.02057 89 640:  11%|█         | 2/19 [00:00<00:03,  5.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07914 0.04518 0.02171 100 640:  11%|█         | 2/19 [00:00<00:03,  5.48it/s]58/99 7.3G 0.07914 0.04518 0.02171 100 640:  16%|█▌        | 3/19 [00:00<00:02,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07857 0.04447 0.02236 73 640:  16%|█▌        | 3/19 [00:00<00:02,  5.53it/s] 58/99 7.3G 0.07857 0.04447 0.02236 73 640:  21%|██        | 4/19 [00:00<00:02,  5.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07982 0.04123 0.02188 55 640:  21%|██        | 4/19 [00:00<00:02,  5.39it/s]58/99 7.3G 0.07982 0.04123 0.02188 55 640:  26%|██▋       | 5/19 [00:00<00:02,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08004 0.03979 0.02184 56 640:  26%|██▋       | 5/19 [00:01<00:02,  5.45it/s]58/99 7.3G 0.08004 0.03979 0.02184 56 640:  32%|███▏      | 6/19 [00:01<00:02,  5.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07934 0.03874 0.0213 57 640:  32%|███▏      | 6/19 [00:01<00:02,  5.52it/s] 58/99 7.3G 0.07934 0.03874 0.0213 57 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.07998 0.04102 0.02181 115 640:  37%|███▋      | 7/19 [00:01<00:02,  5.60it/s]58/99 7.3G 0.07998 0.04102 0.02181 115 640:  42%|████▏     | 8/19 [00:01<00:01,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08071 0.0447 0.02192 148 640:  42%|████▏     | 8/19 [00:01<00:01,  5.65it/s] 58/99 7.3G 0.08071 0.0447 0.02192 148 640:  47%|████▋     | 9/19 [00:01<00:01,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08048 0.04368 0.02264 54 640:  47%|████▋     | 9/19 [00:01<00:01,  5.68it/s]58/99 7.3G 0.08048 0.04368 0.02264 54 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08093 0.04324 0.02309 68 640:  53%|█████▎    | 10/19 [00:07<00:01,  5.02it/s]58/99 7.3G 0.08093 0.04324 0.02309 68 640:  58%|█████▊    | 11/19 [00:07<00:14,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08021 0.04225 0.023 49 640:  58%|█████▊    | 11/19 [00:07<00:14,  1.79s/it]  58/99 7.3G 0.08021 0.04225 0.023 49 640:  63%|██████▎   | 12/19 [00:07<00:09,  1.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08073 0.04321 0.02329 106 640:  63%|██████▎   | 12/19 [00:11<00:09,  1.40s/it]58/99 7.3G 0.08073 0.04321 0.02329 106 640:  68%|██████▊   | 13/19 [00:11<00:12,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08076 0.0429 0.02314 63 640:  68%|██████▊   | 13/19 [00:12<00:12,  2.00s/it]  58/99 7.3G 0.08076 0.0429 0.02314 63 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08069 0.04279 0.0228 79 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.73s/it]58/99 7.3G 0.08069 0.04279 0.0228 79 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08113 0.04342 0.02286 123 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.27s/it]58/99 7.3G 0.08113 0.04342 0.02286 123 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08129 0.0449 0.02272 138 640:  84%|████████▍ | 16/19 [00:14<00:03,  1.15s/it] 58/99 7.3G 0.08129 0.0449 0.02272 138 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.0808 0.04444 0.02299 58 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.25s/it] 58/99 7.3G 0.0808 0.04444 0.02299 58 640:  95%|█████████▍| 18/19 [00:14<00:00,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
58/99 7.3G 0.08111 0.04608 0.02297 140 640:  95%|█████████▍| 18/19 [00:22<00:00,  1.08it/s]58/99 7.3G 0.08111 0.04608 0.02297 140 640: 100%|██████████| 19/19 [00:22<00:00,  3.02s/it]58/99 7.3G 0.08111 0.04608 0.02297 140 640: 100%|██████████| 19/19 [00:22<00:00,  1.20s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]
                   all         55        256      0.184      0.171      0.148     0.0345
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08603 0.0495 0.02302 90 640:   0%|          | 0/19 [00:00<?, ?it/s]59/99 7.3G 0.08603 0.0495 0.02302 90 640:   5%|▌         | 1/19 [00:00<00:03,  4.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08356 0.05306 0.02431 115 640:   5%|▌         | 1/19 [00:00<00:03,  4.93it/s]59/99 7.3G 0.08356 0.05306 0.02431 115 640:  11%|█         | 2/19 [00:00<00:03,  4.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08041 0.0475 0.02426 58 640:  11%|█         | 2/19 [00:00<00:03,  4.86it/s]  59/99 7.3G 0.08041 0.0475 0.02426 58 640:  16%|█▌        | 3/19 [00:00<00:03,  4.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08126 0.04736 0.02327 97 640:  16%|█▌        | 3/19 [00:00<00:03,  4.60it/s]59/99 7.3G 0.08126 0.04736 0.02327 97 640:  21%|██        | 4/19 [00:00<00:03,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08164 0.0512 0.02367 127 640:  21%|██        | 4/19 [00:01<00:03,  4.98it/s]59/99 7.3G 0.08164 0.0512 0.02367 127 640:  26%|██▋       | 5/19 [00:01<00:03,  3.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07997 0.0485 0.02299 56 640:  26%|██▋       | 5/19 [00:01<00:03,  3.94it/s] 59/99 7.3G 0.07997 0.0485 0.02299 56 640:  32%|███▏      | 6/19 [00:01<00:02,  4.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.08054 0.05062 0.02293 135 640:  32%|███▏      | 6/19 [00:01<00:02,  4.41it/s]59/99 7.3G 0.08054 0.05062 0.02293 135 640:  37%|███▋      | 7/19 [00:01<00:02,  4.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07996 0.0489 0.02231 66 640:  37%|███▋      | 7/19 [00:01<00:02,  4.77it/s]  59/99 7.3G 0.07996 0.0489 0.02231 66 640:  42%|████▏     | 8/19 [00:01<00:02,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07908 0.04807 0.02293 58 640:  42%|████▏     | 8/19 [00:01<00:02,  4.88it/s]59/99 7.3G 0.07908 0.04807 0.02293 58 640:  47%|████▋     | 9/19 [00:01<00:02,  4.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07822 0.04806 0.02237 79 640:  47%|████▋     | 9/19 [00:02<00:02,  4.81it/s]59/99 7.3G 0.07822 0.04806 0.02237 79 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07855 0.05063 0.0221 140 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.01it/s]59/99 7.3G 0.07855 0.05063 0.0221 140 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07898 0.04868 0.02196 53 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.20it/s]59/99 7.3G 0.07898 0.04868 0.02196 53 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.0789 0.04825 0.0217 74 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.35it/s]  59/99 7.3G 0.0789 0.04825 0.0217 74 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07885 0.0472 0.02181 57 640:  68%|██████▊   | 13/19 [00:04<00:01,  4.12it/s]59/99 7.3G 0.07885 0.0472 0.02181 57 640:  74%|███████▎  | 14/19 [00:04<00:03,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07873 0.04801 0.02207 94 640:  74%|███████▎  | 14/19 [00:04<00:03,  1.37it/s]59/99 7.3G 0.07873 0.04801 0.02207 94 640:  79%|███████▉  | 15/19 [00:04<00:02,  1.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.0784 0.047 0.02232 47 640:  79%|███████▉  | 15/19 [00:12<00:02,  1.74it/s]   59/99 7.3G 0.0784 0.047 0.02232 47 640:  84%|████████▍ | 16/19 [00:12<00:08,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07833 0.0469 0.0223 75 640:  84%|████████▍ | 16/19 [00:13<00:08,  2.69s/it]59/99 7.3G 0.07833 0.0469 0.0223 75 640:  89%|████████▉ | 17/19 [00:13<00:04,  2.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07924 0.04923 0.02239 217 640:  89%|████████▉ | 17/19 [00:15<00:04,  2.26s/it]59/99 7.3G 0.07924 0.04923 0.02239 217 640:  95%|█████████▍| 18/19 [00:15<00:02,  2.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
59/99 7.3G 0.07907 0.04894 0.02219 77 640:  95%|█████████▍| 18/19 [00:15<00:02,  2.08s/it] 59/99 7.3G 0.07907 0.04894 0.02219 77 640: 100%|██████████| 19/19 [00:15<00:00,  1.51s/it]59/99 7.3G 0.07907 0.04894 0.02219 77 640: 100%|██████████| 19/19 [00:15<00:00,  1.22it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]
                   all         55        256      0.202      0.183      0.142     0.0367
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08882 0.05879 0.02137 127 640:   0%|          | 0/19 [00:00<?, ?it/s]60/99 7.3G 0.08882 0.05879 0.02137 127 640:   5%|▌         | 1/19 [00:00<00:03,  4.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0846 0.04604 0.02344 54 640:   5%|▌         | 1/19 [00:00<00:03,  4.99it/s]  60/99 7.3G 0.0846 0.04604 0.02344 54 640:  11%|█         | 2/19 [00:00<00:03,  5.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0845 0.04401 0.02295 86 640:  11%|█         | 2/19 [00:00<00:03,  5.33it/s]60/99 7.3G 0.0845 0.04401 0.02295 86 640:  16%|█▌        | 3/19 [00:00<00:03,  5.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08515 0.05295 0.02253 172 640:  16%|█▌        | 3/19 [00:00<00:03,  5.20it/s]60/99 7.3G 0.08515 0.05295 0.02253 172 640:  21%|██        | 4/19 [00:00<00:03,  4.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0842 0.04922 0.02308 55 640:  21%|██        | 4/19 [00:01<00:03,  4.33it/s]  60/99 7.3G 0.0842 0.04922 0.02308 55 640:  26%|██▋       | 5/19 [00:01<00:03,  4.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08476 0.05003 0.02251 134 640:  26%|██▋       | 5/19 [00:01<00:03,  4.10it/s]60/99 7.3G 0.08476 0.05003 0.02251 134 640:  32%|███▏      | 6/19 [00:01<00:02,  4.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08514 0.05006 0.02255 98 640:  32%|███▏      | 6/19 [00:01<00:02,  4.54it/s] 60/99 7.3G 0.08514 0.05006 0.02255 98 640:  37%|███▋      | 7/19 [00:01<00:02,  4.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08474 0.0499 0.0224 94 640:  37%|███▋      | 7/19 [00:01<00:02,  4.87it/s]  60/99 7.3G 0.08474 0.0499 0.0224 94 640:  42%|████▏     | 8/19 [00:01<00:02,  5.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08391 0.04796 0.02287 54 640:  42%|████▏     | 8/19 [00:01<00:02,  5.10it/s]60/99 7.3G 0.08391 0.04796 0.02287 54 640:  47%|████▋     | 9/19 [00:01<00:01,  5.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08448 0.05193 0.02257 218 640:  47%|████▋     | 9/19 [00:02<00:01,  5.29it/s]60/99 7.3G 0.08448 0.05193 0.02257 218 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0838 0.05088 0.023 57 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.42it/s]    60/99 7.3G 0.0838 0.05088 0.023 57 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.0829 0.04895 0.0228 46 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.52it/s]60/99 7.3G 0.0829 0.04895 0.0228 46 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08212 0.04649 0.02223 27 640:  63%|██████▎   | 12/19 [00:08<00:01,  5.59it/s]60/99 7.3G 0.08212 0.04649 0.02223 27 640:  68%|██████▊   | 13/19 [00:08<00:11,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08195 0.0464 0.02228 82 640:  68%|██████▊   | 13/19 [00:10<00:11,  1.91s/it] 60/99 7.3G 0.08195 0.0464 0.02228 82 640:  74%|███████▎  | 14/19 [00:10<00:09,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08093 0.04512 0.02199 41 640:  74%|███████▎  | 14/19 [00:12<00:09,  1.97s/it]60/99 7.3G 0.08093 0.04512 0.02199 41 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08085 0.04427 0.02193 56 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.93s/it]60/99 7.3G 0.08085 0.04427 0.02193 56 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08066 0.04459 0.02199 94 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.41s/it]60/99 7.3G 0.08066 0.04459 0.02199 94 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08028 0.04381 0.02191 50 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.09s/it]60/99 7.3G 0.08028 0.04381 0.02191 50 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
60/99 7.3G 0.08034 0.04334 0.02196 62 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.25s/it]60/99 7.3G 0.08034 0.04334 0.02196 62 640: 100%|██████████| 19/19 [00:15<00:00,  1.12s/it]60/99 7.3G 0.08034 0.04334 0.02196 62 640: 100%|██████████| 19/19 [00:15<00:00,  1.25it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.96s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.34s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
                   all         55        256      0.224      0.193      0.143     0.0426
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.08597 0.05008 0.02421 103 640:   0%|          | 0/19 [00:00<?, ?it/s]61/99 7.3G 0.08597 0.05008 0.02421 103 640:   5%|▌         | 1/19 [00:00<00:03,  4.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.08034 0.04159 0.02438 58 640:   5%|▌         | 1/19 [00:00<00:03,  4.62it/s] 61/99 7.3G 0.08034 0.04159 0.02438 58 640:  11%|█         | 2/19 [00:00<00:03,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07944 0.04741 0.02358 98 640:  11%|█         | 2/19 [00:00<00:03,  4.69it/s]61/99 7.3G 0.07944 0.04741 0.02358 98 640:  16%|█▌        | 3/19 [00:00<00:03,  4.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07902 0.0473 0.02419 77 640:  16%|█▌        | 3/19 [00:00<00:03,  4.76it/s] 61/99 7.3G 0.07902 0.0473 0.02419 77 640:  21%|██        | 4/19 [00:00<00:03,  4.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07917 0.04636 0.02437 71 640:  21%|██        | 4/19 [00:01<00:03,  4.80it/s]61/99 7.3G 0.07917 0.04636 0.02437 71 640:  26%|██▋       | 5/19 [00:01<00:02,  4.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07852 0.04576 0.0247 74 640:  26%|██▋       | 5/19 [00:01<00:02,  4.82it/s] 61/99 7.3G 0.07852 0.04576 0.0247 74 640:  32%|███▏      | 6/19 [00:01<00:02,  5.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07834 0.04611 0.02537 76 640:  32%|███▏      | 6/19 [00:01<00:02,  5.06it/s]61/99 7.3G 0.07834 0.04611 0.02537 76 640:  37%|███▋      | 7/19 [00:01<00:02,  5.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07823 0.0459 0.02439 77 640:  37%|███▋      | 7/19 [00:01<00:02,  5.27it/s] 61/99 7.3G 0.07823 0.0459 0.02439 77 640:  42%|████▏     | 8/19 [00:01<00:02,  5.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07867 0.04675 0.0244 93 640:  42%|████▏     | 8/19 [00:01<00:02,  5.42it/s]61/99 7.3G 0.07867 0.04675 0.0244 93 640:  47%|████▋     | 9/19 [00:01<00:01,  5.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07878 0.04592 0.02461 61 640:  47%|████▋     | 9/19 [00:06<00:01,  5.34it/s]61/99 7.3G 0.07878 0.04592 0.02461 61 640:  53%|█████▎    | 10/19 [00:06<00:13,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07856 0.04907 0.0243 134 640:  53%|█████▎    | 10/19 [00:09<00:13,  1.53s/it]61/99 7.3G 0.07856 0.04907 0.0243 134 640:  58%|█████▊    | 11/19 [00:09<00:16,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07945 0.04824 0.02372 95 640:  58%|█████▊    | 11/19 [00:09<00:16,  2.07s/it]61/99 7.3G 0.07945 0.04824 0.02372 95 640:  63%|██████▎   | 12/19 [00:09<00:10,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07961 0.04737 0.02357 62 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.55s/it]61/99 7.3G 0.07961 0.04737 0.02357 62 640:  68%|██████▊   | 13/19 [00:10<00:07,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07929 0.04645 0.02359 54 640:  68%|██████▊   | 13/19 [00:10<00:07,  1.19s/it]61/99 7.3G 0.07929 0.04645 0.02359 54 640:  74%|███████▎  | 14/19 [00:10<00:04,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07915 0.04615 0.02376 71 640:  74%|███████▎  | 14/19 [00:12<00:04,  1.05it/s]61/99 7.3G 0.07915 0.04615 0.02376 71 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07933 0.04542 0.02349 86 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.25s/it]61/99 7.3G 0.07933 0.04542 0.02349 86 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07873 0.04491 0.02344 58 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.00it/s]61/99 7.3G 0.07873 0.04491 0.02344 58 640:  89%|████████▉ | 17/19 [00:13<00:01,  1.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.0793 0.04743 0.02361 178 640:  89%|████████▉ | 17/19 [00:19<00:01,  1.25it/s]61/99 7.3G 0.0793 0.04743 0.02361 178 640:  95%|█████████▍| 18/19 [00:19<00:02,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
61/99 7.3G 0.07918 0.04659 0.02348 47 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.27s/it]61/99 7.3G 0.07918 0.04659 0.02348 47 640: 100%|██████████| 19/19 [00:22<00:00,  2.67s/it]61/99 7.3G 0.07918 0.04659 0.02348 47 640: 100%|██████████| 19/19 [00:22<00:00,  1.20s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.15s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
                   all         55        256      0.174       0.18      0.133     0.0465
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08055 0.04797 0.02157 84 640:   0%|          | 0/19 [00:00<?, ?it/s]62/99 7.3G 0.08055 0.04797 0.02157 84 640:   5%|▌         | 1/19 [00:00<00:06,  2.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08287 0.04158 0.02158 72 640:   5%|▌         | 1/19 [00:00<00:06,  2.72it/s]62/99 7.3G 0.08287 0.04158 0.02158 72 640:  11%|█         | 2/19 [00:00<00:06,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08177 0.04512 0.02117 97 640:  11%|█         | 2/19 [00:01<00:06,  2.58it/s]62/99 7.3G 0.08177 0.04512 0.02117 97 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08375 0.04918 0.02204 141 640:  16%|█▌        | 3/19 [00:01<00:06,  2.50it/s]62/99 7.3G 0.08375 0.04918 0.02204 141 640:  21%|██        | 4/19 [00:01<00:06,  2.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08453 0.04695 0.02197 74 640:  21%|██        | 4/19 [00:02<00:06,  2.41it/s] 62/99 7.3G 0.08453 0.04695 0.02197 74 640:  26%|██▋       | 5/19 [00:02<00:06,  2.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08283 0.04636 0.02219 71 640:  26%|██▋       | 5/19 [00:02<00:06,  2.32it/s]62/99 7.3G 0.08283 0.04636 0.02219 71 640:  32%|███▏      | 6/19 [00:02<00:06,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08217 0.04494 0.02181 65 640:  32%|███▏      | 6/19 [00:03<00:06,  2.14it/s]62/99 7.3G 0.08217 0.04494 0.02181 65 640:  37%|███▋      | 7/19 [00:03<00:06,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.0823 0.04631 0.02152 113 640:  37%|███▋      | 7/19 [00:03<00:06,  1.92it/s]62/99 7.3G 0.0823 0.04631 0.02152 113 640:  42%|████▏     | 8/19 [00:03<00:06,  1.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08146 0.04531 0.02106 63 640:  42%|████▏     | 8/19 [00:04<00:06,  1.79it/s]62/99 7.3G 0.08146 0.04531 0.02106 63 640:  47%|████▋     | 9/19 [00:04<00:05,  1.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08156 0.04476 0.02128 70 640:  47%|████▋     | 9/19 [00:04<00:05,  1.80it/s]62/99 7.3G 0.08156 0.04476 0.02128 70 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08199 0.04558 0.02121 116 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.04it/s]62/99 7.3G 0.08199 0.04558 0.02121 116 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08154 0.04532 0.02175 68 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.23it/s] 62/99 7.3G 0.08154 0.04532 0.02175 68 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08199 0.04525 0.02172 94 640:  63%|██████▎   | 12/19 [00:06<00:03,  2.18it/s]62/99 7.3G 0.08199 0.04525 0.02172 94 640:  68%|██████▊   | 13/19 [00:06<00:02,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.0817 0.04466 0.02193 57 640:  68%|██████▊   | 13/19 [00:06<00:02,  2.10it/s] 62/99 7.3G 0.0817 0.04466 0.02193 57 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08165 0.04461 0.02236 65 640:  74%|███████▎  | 14/19 [00:09<00:02,  2.04it/s]62/99 7.3G 0.08165 0.04461 0.02236 65 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.0812 0.04382 0.0222 52 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.11s/it]  62/99 7.3G 0.0812 0.04382 0.0222 52 640:  84%|████████▍ | 16/19 [00:11<00:04,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08095 0.04289 0.02193 47 640:  84%|████████▍ | 16/19 [00:14<00:04,  1.53s/it]62/99 7.3G 0.08095 0.04289 0.02193 47 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.78s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08099 0.04306 0.02185 85 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.78s/it]62/99 7.3G 0.08099 0.04306 0.02185 85 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
62/99 7.3G 0.08072 0.04284 0.02172 65 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.54s/it]62/99 7.3G 0.08072 0.04284 0.02172 65 640: 100%|██████████| 19/19 [00:15<00:00,  1.26s/it]62/99 7.3G 0.08072 0.04284 0.02172 65 640: 100%|██████████| 19/19 [00:15<00:00,  1.21it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.95s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.03s/it]
                   all         55        256      0.317      0.185      0.151      0.049
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07652 0.04215 0.01614 79 640:   0%|          | 0/19 [00:00<?, ?it/s]63/99 7.3G 0.07652 0.04215 0.01614 79 640:   5%|▌         | 1/19 [00:00<00:05,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07379 0.03297 0.01801 36 640:   5%|▌         | 1/19 [00:00<00:05,  3.15it/s]63/99 7.3G 0.07379 0.03297 0.01801 36 640:  11%|█         | 2/19 [00:00<00:04,  3.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07597 0.03694 0.0201 71 640:  11%|█         | 2/19 [00:00<00:04,  3.47it/s] 63/99 7.3G 0.07597 0.03694 0.0201 71 640:  16%|█▌        | 3/19 [00:00<00:03,  4.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07649 0.03986 0.02086 86 640:  16%|█▌        | 3/19 [00:00<00:03,  4.06it/s]63/99 7.3G 0.07649 0.03986 0.02086 86 640:  21%|██        | 4/19 [00:00<00:03,  4.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07662 0.04216 0.022 84 640:  21%|██        | 4/19 [00:01<00:03,  4.29it/s]  63/99 7.3G 0.07662 0.04216 0.022 84 640:  26%|██▋       | 5/19 [00:01<00:03,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07668 0.04219 0.02194 77 640:  26%|██▋       | 5/19 [00:01<00:03,  4.37it/s]63/99 7.3G 0.07668 0.04219 0.02194 77 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07639 0.04148 0.02175 61 640:  32%|███▏      | 6/19 [00:01<00:02,  4.71it/s]63/99 7.3G 0.07639 0.04148 0.02175 61 640:  37%|███▋      | 7/19 [00:01<00:02,  4.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07773 0.04139 0.0224 82 640:  37%|███▋      | 7/19 [00:01<00:02,  4.43it/s] 63/99 7.3G 0.07773 0.04139 0.0224 82 640:  42%|████▏     | 8/19 [00:01<00:02,  4.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07728 0.04146 0.02173 75 640:  42%|████▏     | 8/19 [00:02<00:02,  4.46it/s]63/99 7.3G 0.07728 0.04146 0.02173 75 640:  47%|████▋     | 9/19 [00:02<00:02,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07703 0.04 0.02172 40 640:  47%|████▋     | 9/19 [00:02<00:02,  3.83it/s]   63/99 7.3G 0.07703 0.04 0.02172 40 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07735 0.03868 0.02162 43 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.92it/s]63/99 7.3G 0.07735 0.03868 0.02162 43 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07725 0.03851 0.0221 63 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.51it/s] 63/99 7.3G 0.07725 0.03851 0.0221 63 640:  63%|██████▎   | 12/19 [00:05<00:06,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07728 0.03829 0.02199 62 640:  63%|██████▎   | 12/19 [00:07<00:06,  1.05it/s]63/99 7.3G 0.07728 0.03829 0.02199 62 640:  68%|██████▊   | 13/19 [00:07<00:06,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07714 0.0389 0.02172 83 640:  68%|██████▊   | 13/19 [00:08<00:06,  1.14s/it] 63/99 7.3G 0.07714 0.0389 0.02172 83 640:  74%|███████▎  | 14/19 [00:08<00:06,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07753 0.04135 0.02208 132 640:  74%|███████▎  | 14/19 [00:09<00:06,  1.25s/it]63/99 7.3G 0.07753 0.04135 0.02208 132 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07759 0.04219 0.02195 97 640:  79%|███████▉  | 15/19 [00:10<00:04,  1.23s/it] 63/99 7.3G 0.07759 0.04219 0.02195 97 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.0775 0.04239 0.02214 74 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.16s/it] 63/99 7.3G 0.0775 0.04239 0.02214 74 640:  89%|████████▉ | 17/19 [00:11<00:01,  1.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07747 0.04213 0.02213 59 640:  89%|████████▉ | 17/19 [00:11<00:01,  1.09it/s]63/99 7.3G 0.07747 0.04213 0.02213 59 640:  95%|█████████▍| 18/19 [00:11<00:00,  1.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
63/99 7.3G 0.07762 0.04304 0.02219 105 640:  95%|█████████▍| 18/19 [00:12<00:00,  1.40it/s]63/99 7.3G 0.07762 0.04304 0.02219 105 640: 100%|██████████| 19/19 [00:12<00:00,  1.35it/s]63/99 7.3G 0.07762 0.04304 0.02219 105 640: 100%|██████████| 19/19 [00:12<00:00,  1.57it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.45s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
                   all         55        256      0.406     0.0976      0.158     0.0496
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07437 0.02293 0.0177 36 640:   0%|          | 0/19 [00:00<?, ?it/s]64/99 7.3G 0.07437 0.02293 0.0177 36 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08096 0.03187 0.02048 85 640:   5%|▌         | 1/19 [00:00<00:06,  2.81it/s]64/99 7.3G 0.08096 0.03187 0.02048 85 640:  11%|█         | 2/19 [00:00<00:06,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.0817 0.04194 0.02126 116 640:  11%|█         | 2/19 [00:01<00:06,  2.81it/s]64/99 7.3G 0.0817 0.04194 0.02126 116 640:  16%|█▌        | 3/19 [00:01<00:06,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07986 0.04174 0.02288 61 640:  16%|█▌        | 3/19 [00:01<00:06,  2.62it/s]64/99 7.3G 0.07986 0.04174 0.02288 61 640:  21%|██        | 4/19 [00:01<00:06,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07993 0.0434 0.02334 79 640:  21%|██        | 4/19 [00:01<00:06,  2.42it/s] 64/99 7.3G 0.07993 0.0434 0.02334 79 640:  26%|██▋       | 5/19 [00:01<00:05,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08078 0.04753 0.02258 136 640:  26%|██▋       | 5/19 [00:02<00:05,  2.54it/s]64/99 7.3G 0.08078 0.04753 0.02258 136 640:  32%|███▏      | 6/19 [00:02<00:05,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07999 0.04603 0.02265 56 640:  32%|███▏      | 6/19 [00:02<00:05,  2.50it/s] 64/99 7.3G 0.07999 0.04603 0.02265 56 640:  37%|███▋      | 7/19 [00:02<00:04,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08029 0.04774 0.02282 114 640:  37%|███▋      | 7/19 [00:03<00:04,  2.59it/s]64/99 7.3G 0.08029 0.04774 0.02282 114 640:  42%|████▏     | 8/19 [00:03<00:04,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07994 0.04613 0.02254 58 640:  42%|████▏     | 8/19 [00:03<00:04,  2.65it/s] 64/99 7.3G 0.07994 0.04613 0.02254 58 640:  47%|████▋     | 9/19 [00:03<00:04,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07964 0.04498 0.02255 59 640:  47%|████▋     | 9/19 [00:05<00:04,  2.28it/s]64/99 7.3G 0.07964 0.04498 0.02255 59 640:  53%|█████▎    | 10/19 [00:05<00:06,  1.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08 0.04718 0.02232 134 640:  53%|█████▎    | 10/19 [00:07<00:06,  1.29it/s]  64/99 7.3G 0.08 0.04718 0.02232 134 640:  58%|█████▊    | 11/19 [00:07<00:09,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08018 0.04678 0.02264 67 640:  58%|█████▊    | 11/19 [00:08<00:09,  1.21s/it]64/99 7.3G 0.08018 0.04678 0.02264 67 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.08083 0.04748 0.02247 136 640:  63%|██████▎   | 12/19 [00:09<00:09,  1.32s/it]64/99 7.3G 0.08083 0.04748 0.02247 136 640:  68%|██████▊   | 13/19 [00:09<00:06,  1.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07986 0.04612 0.02206 47 640:  68%|██████▊   | 13/19 [00:09<00:06,  1.02s/it] 64/99 7.3G 0.07986 0.04612 0.02206 47 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07947 0.04554 0.02195 59 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.21it/s]64/99 7.3G 0.07947 0.04554 0.02195 59 640:  79%|███████▉  | 15/19 [00:09<00:02,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07904 0.04468 0.02236 53 640:  79%|███████▉  | 15/19 [00:10<00:02,  1.50it/s]64/99 7.3G 0.07904 0.04468 0.02236 53 640:  84%|████████▍ | 16/19 [00:10<00:01,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.079 0.04512 0.02265 89 640:  84%|████████▍ | 16/19 [00:17<00:01,  1.89it/s]  64/99 7.3G 0.079 0.04512 0.02265 89 640:  89%|████████▉ | 17/19 [00:17<00:05,  2.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07791 0.04383 0.02209 33 640:  89%|████████▉ | 17/19 [00:19<00:05,  2.51s/it]64/99 7.3G 0.07791 0.04383 0.02209 33 640:  95%|█████████▍| 18/19 [00:19<00:02,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
64/99 7.3G 0.07842 0.04456 0.02216 107 640:  95%|█████████▍| 18/19 [00:22<00:02,  2.40s/it]64/99 7.3G 0.07842 0.04456 0.02216 107 640: 100%|██████████| 19/19 [00:22<00:00,  2.60s/it]64/99 7.3G 0.07842 0.04456 0.02216 107 640: 100%|██████████| 19/19 [00:22<00:00,  1.18s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.68s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
                   all         55        256      0.335      0.176      0.143     0.0471
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.0802 0.03039 0.02081 58 640:   0%|          | 0/19 [00:00<?, ?it/s]65/99 7.3G 0.0802 0.03039 0.02081 58 640:   5%|▌         | 1/19 [00:00<00:03,  4.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08207 0.04368 0.02066 121 640:   5%|▌         | 1/19 [00:00<00:03,  4.87it/s]65/99 7.3G 0.08207 0.04368 0.02066 121 640:  11%|█         | 2/19 [00:00<00:05,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08258 0.04107 0.02153 65 640:  11%|█         | 2/19 [00:00<00:05,  3.38it/s] 65/99 7.3G 0.08258 0.04107 0.02153 65 640:  16%|█▌        | 3/19 [00:00<00:05,  3.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08208 0.04039 0.0222 67 640:  16%|█▌        | 3/19 [00:01<00:05,  3.07it/s] 65/99 7.3G 0.08208 0.04039 0.0222 67 640:  21%|██        | 4/19 [00:01<00:05,  2.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08102 0.03878 0.02492 52 640:  21%|██        | 4/19 [00:01<00:05,  2.95it/s]65/99 7.3G 0.08102 0.03878 0.02492 52 640:  26%|██▋       | 5/19 [00:01<00:05,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08039 0.03899 0.02516 65 640:  26%|██▋       | 5/19 [00:02<00:05,  2.64it/s]65/99 7.3G 0.08039 0.03899 0.02516 65 640:  32%|███▏      | 6/19 [00:02<00:05,  2.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08083 0.03863 0.02451 67 640:  32%|███▏      | 6/19 [00:02<00:05,  2.48it/s]65/99 7.3G 0.08083 0.03863 0.02451 67 640:  37%|███▋      | 7/19 [00:02<00:05,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.0813 0.03992 0.02462 93 640:  37%|███▋      | 7/19 [00:03<00:05,  2.21it/s] 65/99 7.3G 0.0813 0.03992 0.02462 93 640:  42%|████▏     | 8/19 [00:03<00:05,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.08095 0.04001 0.02348 75 640:  42%|████▏     | 8/19 [00:03<00:05,  2.11it/s]65/99 7.3G 0.08095 0.04001 0.02348 75 640:  47%|████▋     | 9/19 [00:03<00:04,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07996 0.03955 0.02361 53 640:  47%|████▋     | 9/19 [00:04<00:04,  2.05it/s]65/99 7.3G 0.07996 0.03955 0.02361 53 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07966 0.03937 0.02349 64 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.95it/s]65/99 7.3G 0.07966 0.03937 0.02349 64 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07971 0.03821 0.02348 42 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.97it/s]65/99 7.3G 0.07971 0.03821 0.02348 42 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07949 0.03851 0.02331 68 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.17it/s]65/99 7.3G 0.07949 0.03851 0.02331 68 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07911 0.03808 0.02289 54 640:  68%|██████▊   | 13/19 [00:08<00:02,  2.39it/s]65/99 7.3G 0.07911 0.03808 0.02289 54 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07931 0.0395 0.02317 103 640:  74%|███████▎  | 14/19 [00:09<00:05,  1.05s/it]65/99 7.3G 0.07931 0.0395 0.02317 103 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07918 0.03927 0.02293 59 640:  79%|███████▉  | 15/19 [00:13<00:04,  1.07s/it]65/99 7.3G 0.07918 0.03927 0.02293 59 640:  84%|████████▍ | 16/19 [00:13<00:06,  2.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.0794 0.03997 0.02289 95 640:  84%|████████▍ | 16/19 [00:14<00:06,  2.07s/it] 65/99 7.3G 0.0794 0.03997 0.02289 95 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.75s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.0793 0.04043 0.02303 76 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.75s/it]65/99 7.3G 0.0793 0.04043 0.02303 76 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
65/99 7.3G 0.07911 0.04007 0.02298 60 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.31s/it]65/99 7.3G 0.07911 0.04007 0.02298 60 640: 100%|██████████| 19/19 [00:15<00:00,  1.14s/it]65/99 7.3G 0.07911 0.04007 0.02298 60 640: 100%|██████████| 19/19 [00:15<00:00,  1.22it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
                   all         55        256      0.173      0.149      0.121     0.0428
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07462 0.03292 0.02419 55 640:   0%|          | 0/19 [00:00<?, ?it/s]66/99 7.3G 0.07462 0.03292 0.02419 55 640:   5%|▌         | 1/19 [00:00<00:11,  1.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07277 0.03358 0.02416 50 640:   5%|▌         | 1/19 [00:01<00:11,  1.62it/s]66/99 7.3G 0.07277 0.03358 0.02416 50 640:  11%|█         | 2/19 [00:01<00:09,  1.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07249 0.0329 0.02343 50 640:  11%|█         | 2/19 [00:01<00:09,  1.83it/s] 66/99 7.3G 0.07249 0.0329 0.02343 50 640:  16%|█▌        | 3/19 [00:01<00:07,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07493 0.03566 0.02299 75 640:  16%|█▌        | 3/19 [00:01<00:07,  2.11it/s]66/99 7.3G 0.07493 0.03566 0.02299 75 640:  21%|██        | 4/19 [00:01<00:06,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07529 0.03899 0.02385 83 640:  21%|██        | 4/19 [00:02<00:06,  2.25it/s]66/99 7.3G 0.07529 0.03899 0.02385 83 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.0758 0.03742 0.02377 48 640:  26%|██▋       | 5/19 [00:02<00:05,  2.41it/s] 66/99 7.3G 0.0758 0.03742 0.02377 48 640:  32%|███▏      | 6/19 [00:02<00:05,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07659 0.03697 0.02319 62 640:  32%|███▏      | 6/19 [00:02<00:05,  2.54it/s]66/99 7.3G 0.07659 0.03697 0.02319 62 640:  37%|███▋      | 7/19 [00:02<00:04,  2.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07706 0.0381 0.02249 87 640:  37%|███▋      | 7/19 [00:03<00:04,  2.62it/s] 66/99 7.3G 0.07706 0.0381 0.02249 87 640:  42%|████▏     | 8/19 [00:03<00:04,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07724 0.03807 0.02173 73 640:  42%|████▏     | 8/19 [00:03<00:04,  2.63it/s]66/99 7.3G 0.07724 0.03807 0.02173 73 640:  47%|████▋     | 9/19 [00:03<00:04,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07704 0.03737 0.02199 51 640:  47%|████▋     | 9/19 [00:04<00:04,  2.36it/s]66/99 7.3G 0.07704 0.03737 0.02199 51 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07784 0.03885 0.02208 104 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.17it/s]66/99 7.3G 0.07784 0.03885 0.02208 104 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.0779 0.03902 0.02189 73 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.03it/s]  66/99 7.3G 0.0779 0.03902 0.02189 73 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07797 0.0392 0.02172 73 640:  63%|██████▎   | 12/19 [00:09<00:03,  2.00it/s]66/99 7.3G 0.07797 0.0392 0.02172 73 640:  68%|██████▊   | 13/19 [00:09<00:08,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07844 0.04032 0.02184 107 640:  68%|██████▊   | 13/19 [00:09<00:08,  1.50s/it]66/99 7.3G 0.07844 0.04032 0.02184 107 640:  74%|███████▎  | 14/19 [00:09<00:05,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07854 0.04183 0.02216 102 640:  74%|███████▎  | 14/19 [00:09<00:05,  1.15s/it]66/99 7.3G 0.07854 0.04183 0.02216 102 640:  79%|███████▉  | 15/19 [00:09<00:03,  1.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07897 0.04162 0.02222 79 640:  79%|███████▉  | 15/19 [00:10<00:03,  1.13it/s] 66/99 7.3G 0.07897 0.04162 0.02222 79 640:  84%|████████▍ | 16/19 [00:10<00:02,  1.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07877 0.04176 0.02231 70 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.50it/s]66/99 7.3G 0.07877 0.04176 0.02231 70 640:  89%|████████▉ | 17/19 [00:11<00:01,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07886 0.04114 0.0224 55 640:  89%|████████▉ | 17/19 [00:12<00:01,  1.19it/s] 66/99 7.3G 0.07886 0.04114 0.0224 55 640:  95%|█████████▍| 18/19 [00:12<00:01,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
66/99 7.3G 0.07881 0.04135 0.02231 81 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.03s/it]66/99 7.3G 0.07881 0.04135 0.02231 81 640: 100%|██████████| 19/19 [00:17<00:00,  2.27s/it]66/99 7.3G 0.07881 0.04135 0.02231 81 640: 100%|██████████| 19/19 [00:17<00:00,  1.06it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.94s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.87s/it]
                   all         55        256      0.193      0.205      0.108      0.034
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07592 0.03147 0.02275 50 640:   0%|          | 0/19 [00:00<?, ?it/s]67/99 7.3G 0.07592 0.03147 0.02275 50 640:   5%|▌         | 1/19 [00:00<00:04,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07945 0.0463 0.02342 121 640:   5%|▌         | 1/19 [00:00<00:04,  4.14it/s]67/99 7.3G 0.07945 0.0463 0.02342 121 640:  11%|█         | 2/19 [00:00<00:05,  3.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07929 0.04157 0.02277 59 640:  11%|█         | 2/19 [00:00<00:05,  3.24it/s]67/99 7.3G 0.07929 0.04157 0.02277 59 640:  16%|█▌        | 3/19 [00:00<00:05,  3.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.08089 0.04323 0.02301 100 640:  16%|█▌        | 3/19 [00:01<00:05,  3.03it/s]67/99 7.3G 0.08089 0.04323 0.02301 100 640:  21%|██        | 4/19 [00:01<00:05,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.08152 0.04185 0.02338 66 640:  21%|██        | 4/19 [00:01<00:05,  2.61it/s] 67/99 7.3G 0.08152 0.04185 0.02338 66 640:  26%|██▋       | 5/19 [00:01<00:05,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.08077 0.04056 0.02212 58 640:  26%|██▋       | 5/19 [00:02<00:05,  2.42it/s]67/99 7.3G 0.08077 0.04056 0.02212 58 640:  32%|███▏      | 6/19 [00:02<00:05,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07924 0.03935 0.02203 51 640:  32%|███▏      | 6/19 [00:02<00:05,  2.36it/s]67/99 7.3G 0.07924 0.03935 0.02203 51 640:  37%|███▋      | 7/19 [00:02<00:05,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07803 0.03845 0.02218 54 640:  37%|███▋      | 7/19 [00:03<00:05,  2.18it/s]67/99 7.3G 0.07803 0.03845 0.02218 54 640:  42%|████▏     | 8/19 [00:03<00:05,  1.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07871 0.03894 0.02195 86 640:  42%|████▏     | 8/19 [00:03<00:05,  1.97it/s]67/99 7.3G 0.07871 0.03894 0.02195 86 640:  47%|████▋     | 9/19 [00:03<00:05,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07838 0.03849 0.02172 59 640:  47%|████▋     | 9/19 [00:04<00:05,  1.95it/s]67/99 7.3G 0.07838 0.03849 0.02172 59 640:  53%|█████▎    | 10/19 [00:04<00:04,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.0783 0.03923 0.02146 76 640:  53%|█████▎    | 10/19 [00:05<00:04,  1.93it/s] 67/99 7.3G 0.0783 0.03923 0.02146 76 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07859 0.03938 0.02124 83 640:  58%|█████▊    | 11/19 [00:05<00:04,  1.92it/s]67/99 7.3G 0.07859 0.03938 0.02124 83 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07859 0.03914 0.02108 63 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.03it/s]67/99 7.3G 0.07859 0.03914 0.02108 63 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07895 0.03933 0.02147 73 640:  68%|██████▊   | 13/19 [00:06<00:02,  2.14it/s]67/99 7.3G 0.07895 0.03933 0.02147 73 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07857 0.03935 0.02161 64 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.23it/s]67/99 7.3G 0.07857 0.03935 0.02161 64 640:  79%|███████▉  | 15/19 [00:06<00:01,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07906 0.04037 0.02196 109 640:  79%|███████▉  | 15/19 [00:11<00:01,  2.38it/s]67/99 7.3G 0.07906 0.04037 0.02196 109 640:  84%|████████▍ | 16/19 [00:11<00:05,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07879 0.03974 0.02203 50 640:  84%|████████▍ | 16/19 [00:12<00:05,  1.71s/it] 67/99 7.3G 0.07879 0.03974 0.02203 50 640:  89%|████████▉ | 17/19 [00:12<00:03,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07874 0.03886 0.02215 40 640:  89%|████████▉ | 17/19 [00:19<00:03,  1.68s/it]67/99 7.3G 0.07874 0.03886 0.02215 40 640:  95%|█████████▍| 18/19 [00:19<00:03,  3.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
67/99 7.3G 0.07897 0.03925 0.02214 88 640:  95%|█████████▍| 18/19 [00:20<00:03,  3.15s/it]67/99 7.3G 0.07897 0.03925 0.02214 88 640: 100%|██████████| 19/19 [00:20<00:00,  2.43s/it]67/99 7.3G 0.07897 0.03925 0.02214 88 640: 100%|██████████| 19/19 [00:20<00:00,  1.07s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.26s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.05s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]
                   all         55        256      0.223      0.215      0.121     0.0358
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08285 0.05486 0.02556 97 640:   0%|          | 0/19 [00:00<?, ?it/s]68/99 7.3G 0.08285 0.05486 0.02556 97 640:   5%|▌         | 1/19 [00:00<00:03,  5.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08255 0.04491 0.02685 51 640:   5%|▌         | 1/19 [00:00<00:03,  5.52it/s]68/99 7.3G 0.08255 0.04491 0.02685 51 640:  11%|█         | 2/19 [00:00<00:03,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08193 0.04022 0.02541 52 640:  11%|█         | 2/19 [00:00<00:03,  5.65it/s]68/99 7.3G 0.08193 0.04022 0.02541 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08022 0.04131 0.02482 74 640:  16%|█▌        | 3/19 [00:00<00:02,  5.40it/s]68/99 7.3G 0.08022 0.04131 0.02482 74 640:  21%|██        | 4/19 [00:00<00:03,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08025 0.04112 0.02524 64 640:  21%|██        | 4/19 [00:01<00:03,  3.93it/s]68/99 7.3G 0.08025 0.04112 0.02524 64 640:  26%|██▋       | 5/19 [00:01<00:04,  3.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08057 0.04064 0.02456 77 640:  26%|██▋       | 5/19 [00:01<00:04,  3.45it/s]68/99 7.3G 0.08057 0.04064 0.02456 77 640:  32%|███▏      | 6/19 [00:01<00:03,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07978 0.0393 0.02383 54 640:  32%|███▏      | 6/19 [00:01<00:03,  3.43it/s] 68/99 7.3G 0.07978 0.0393 0.02383 54 640:  37%|███▋      | 7/19 [00:01<00:03,  3.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07983 0.04039 0.02338 83 640:  37%|███▋      | 7/19 [00:01<00:03,  3.95it/s]68/99 7.3G 0.07983 0.04039 0.02338 83 640:  42%|████▏     | 8/19 [00:01<00:02,  4.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07968 0.04032 0.02361 63 640:  42%|████▏     | 8/19 [00:02<00:02,  4.04it/s]68/99 7.3G 0.07968 0.04032 0.02361 63 640:  47%|████▋     | 9/19 [00:02<00:02,  3.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.07991 0.0397 0.02335 62 640:  47%|████▋     | 9/19 [00:02<00:02,  3.84it/s] 68/99 7.3G 0.07991 0.0397 0.02335 62 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08053 0.0417 0.02324 117 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.28it/s]68/99 7.3G 0.08053 0.0417 0.02324 117 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.0805 0.0436 0.02333 109 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.47it/s] 68/99 7.3G 0.0805 0.0436 0.02333 109 640:  63%|██████▎   | 12/19 [00:03<00:02,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08082 0.04669 0.02324 151 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.43it/s]68/99 7.3G 0.08082 0.04669 0.02324 151 640:  68%|██████▊   | 13/19 [00:05<00:05,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.0813 0.04605 0.02285 95 640:  68%|██████▊   | 13/19 [00:05<00:05,  1.07it/s]  68/99 7.3G 0.0813 0.04605 0.02285 95 640:  74%|███████▎  | 14/19 [00:05<00:03,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08091 0.04583 0.02286 77 640:  74%|███████▎  | 14/19 [00:12<00:03,  1.34it/s]68/99 7.3G 0.08091 0.04583 0.02286 77 640:  79%|███████▉  | 15/19 [00:12<00:09,  2.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08109 0.04683 0.02268 129 640:  79%|███████▉  | 15/19 [00:13<00:09,  2.40s/it]68/99 7.3G 0.08109 0.04683 0.02268 129 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08093 0.04686 0.02252 86 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.98s/it] 68/99 7.3G 0.08093 0.04686 0.02252 86 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.08088 0.04774 0.02236 111 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.93s/it]68/99 7.3G 0.08088 0.04774 0.02236 111 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
68/99 7.3G 0.0807 0.04739 0.02227 66 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.44s/it]  68/99 7.3G 0.0807 0.04739 0.02227 66 640: 100%|██████████| 19/19 [00:15<00:00,  1.06s/it]68/99 7.3G 0.0807 0.04739 0.02227 66 640: 100%|██████████| 19/19 [00:15<00:00,  1.24it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.42s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
                   all         55        256      0.235      0.194      0.134     0.0454
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.08525 0.04934 0.01955 111 640:   0%|          | 0/19 [00:00<?, ?it/s]69/99 7.3G 0.08525 0.04934 0.01955 111 640:   5%|▌         | 1/19 [00:00<00:05,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07878 0.0422 0.01948 57 640:   5%|▌         | 1/19 [00:00<00:05,  3.21it/s]  69/99 7.3G 0.07878 0.0422 0.01948 57 640:  11%|█         | 2/19 [00:00<00:07,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07573 0.04087 0.02157 61 640:  11%|█         | 2/19 [00:01<00:07,  2.21it/s]69/99 7.3G 0.07573 0.04087 0.02157 61 640:  16%|█▌        | 3/19 [00:01<00:07,  2.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07724 0.03969 0.02068 70 640:  16%|█▌        | 3/19 [00:01<00:07,  2.02it/s]69/99 7.3G 0.07724 0.03969 0.02068 70 640:  21%|██        | 4/19 [00:01<00:05,  2.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07794 0.04233 0.02098 93 640:  21%|██        | 4/19 [00:01<00:05,  2.71it/s]69/99 7.3G 0.07794 0.04233 0.02098 93 640:  26%|██▋       | 5/19 [00:01<00:04,  3.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.0784 0.04511 0.02174 98 640:  26%|██▋       | 5/19 [00:02<00:04,  3.27it/s] 69/99 7.3G 0.0784 0.04511 0.02174 98 640:  32%|███▏      | 6/19 [00:02<00:04,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07825 0.04514 0.02237 72 640:  32%|███▏      | 6/19 [00:02<00:04,  3.09it/s]69/99 7.3G 0.07825 0.04514 0.02237 72 640:  37%|███▋      | 7/19 [00:02<00:03,  3.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07773 0.04537 0.02229 80 640:  37%|███▋      | 7/19 [00:02<00:03,  3.08it/s]69/99 7.3G 0.07773 0.04537 0.02229 80 640:  42%|████▏     | 8/19 [00:02<00:03,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07902 0.04632 0.02199 125 640:  42%|████▏     | 8/19 [00:02<00:03,  3.55it/s]69/99 7.3G 0.07902 0.04632 0.02199 125 640:  47%|████▋     | 9/19 [00:02<00:02,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07773 0.04525 0.02228 57 640:  47%|████▋     | 9/19 [00:03<00:02,  3.92it/s] 69/99 7.3G 0.07773 0.04525 0.02228 57 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07843 0.04473 0.02212 81 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.43it/s]69/99 7.3G 0.07843 0.04473 0.02212 81 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07839 0.04485 0.02181 79 640:  58%|█████▊    | 11/19 [00:08<00:02,  3.92it/s]69/99 7.3G 0.07839 0.04485 0.02181 79 640:  63%|██████▎   | 12/19 [00:08<00:11,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07931 0.0486 0.02178 205 640:  63%|██████▎   | 12/19 [00:09<00:11,  1.63s/it]69/99 7.3G 0.07931 0.0486 0.02178 205 640:  68%|██████▊   | 13/19 [00:09<00:09,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.0795 0.04845 0.02187 83 640:  68%|██████▊   | 13/19 [00:11<00:09,  1.50s/it] 69/99 7.3G 0.0795 0.04845 0.02187 83 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07932 0.04803 0.02182 72 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.63s/it]69/99 7.3G 0.07932 0.04803 0.02182 72 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07938 0.04756 0.02212 64 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.19s/it]69/99 7.3G 0.07938 0.04756 0.02212 64 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07966 0.04714 0.02228 77 640:  84%|████████▍ | 16/19 [00:12<00:02,  1.13it/s]69/99 7.3G 0.07966 0.04714 0.02228 77 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07968 0.04744 0.02215 93 640:  89%|████████▉ | 17/19 [00:18<00:02,  1.01s/it]69/99 7.3G 0.07968 0.04744 0.02215 93 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
69/99 7.3G 0.07955 0.04763 0.02203 82 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.38s/it]69/99 7.3G 0.07955 0.04763 0.02203 82 640: 100%|██████████| 19/19 [00:18<00:00,  1.71s/it]69/99 7.3G 0.07955 0.04763 0.02203 82 640: 100%|██████████| 19/19 [00:18<00:00,  1.02it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.64s/it]
                   all         55        256      0.274      0.191      0.163     0.0525
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.08345 0.04502 0.01885 94 640:   0%|          | 0/19 [00:00<?, ?it/s]70/99 7.3G 0.08345 0.04502 0.01885 94 640:   5%|▌         | 1/19 [00:00<00:11,  1.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07858 0.04633 0.02008 75 640:   5%|▌         | 1/19 [00:01<00:11,  1.55it/s]70/99 7.3G 0.07858 0.04633 0.02008 75 640:  11%|█         | 2/19 [00:01<00:10,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07613 0.04187 0.01951 58 640:  11%|█         | 2/19 [00:01<00:10,  1.66it/s]70/99 7.3G 0.07613 0.04187 0.01951 58 640:  16%|█▌        | 3/19 [00:01<00:09,  1.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07759 0.04749 0.01949 119 640:  16%|█▌        | 3/19 [00:02<00:09,  1.69it/s]70/99 7.3G 0.07759 0.04749 0.01949 119 640:  21%|██        | 4/19 [00:02<00:07,  1.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07725 0.04808 0.02035 88 640:  21%|██        | 4/19 [00:02<00:07,  1.90it/s] 70/99 7.3G 0.07725 0.04808 0.02035 88 640:  26%|██▋       | 5/19 [00:02<00:06,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07744 0.04944 0.02046 97 640:  26%|██▋       | 5/19 [00:02<00:06,  2.17it/s]70/99 7.3G 0.07744 0.04944 0.02046 97 640:  32%|███▏      | 6/19 [00:02<00:05,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07714 0.04795 0.02113 67 640:  32%|███▏      | 6/19 [00:03<00:05,  2.36it/s]70/99 7.3G 0.07714 0.04795 0.02113 67 640:  37%|███▋      | 7/19 [00:03<00:04,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07835 0.04864 0.02132 128 640:  37%|███▋      | 7/19 [00:03<00:04,  2.50it/s]70/99 7.3G 0.07835 0.04864 0.02132 128 640:  42%|████▏     | 8/19 [00:03<00:04,  2.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07888 0.04748 0.02106 75 640:  42%|████▏     | 8/19 [00:04<00:04,  2.56it/s] 70/99 7.3G 0.07888 0.04748 0.02106 75 640:  47%|████▋     | 9/19 [00:04<00:04,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.079 0.0469 0.02126 68 640:  47%|████▋     | 9/19 [00:04<00:04,  2.05it/s]   70/99 7.3G 0.079 0.0469 0.02126 68 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.0792 0.04685 0.02095 83 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.00it/s]70/99 7.3G 0.0792 0.04685 0.02095 83 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07913 0.04547 0.02081 49 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.25it/s]70/99 7.3G 0.07913 0.04547 0.02081 49 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07926 0.04645 0.02099 114 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.76it/s]70/99 7.3G 0.07926 0.04645 0.02099 114 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07892 0.04594 0.02113 66 640:  68%|██████▊   | 13/19 [00:06<00:01,  3.28it/s] 70/99 7.3G 0.07892 0.04594 0.02113 66 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07858 0.04585 0.02108 70 640:  74%|███████▎  | 14/19 [00:11<00:02,  2.09it/s]70/99 7.3G 0.07858 0.04585 0.02108 70 640:  79%|███████▉  | 15/19 [00:11<00:07,  1.86s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07833 0.04503 0.02111 52 640:  79%|███████▉  | 15/19 [00:11<00:07,  1.86s/it]70/99 7.3G 0.07833 0.04503 0.02111 52 640:  84%|████████▍ | 16/19 [00:11<00:04,  1.40s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07865 0.04546 0.02142 94 640:  84%|████████▍ | 16/19 [00:18<00:04,  1.40s/it]70/99 7.3G 0.07865 0.04546 0.02142 94 640:  89%|████████▉ | 17/19 [00:18<00:05,  2.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07876 0.04557 0.02128 89 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.84s/it]70/99 7.3G 0.07876 0.04557 0.02128 89 640:  95%|█████████▍| 18/19 [00:20<00:02,  2.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
70/99 7.3G 0.07798 0.04482 0.02131 46 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.73s/it]70/99 7.3G 0.07798 0.04482 0.02131 46 640: 100%|██████████| 19/19 [00:21<00:00,  2.25s/it]70/99 7.3G 0.07798 0.04482 0.02131 46 640: 100%|██████████| 19/19 [00:21<00:00,  1.14s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.61s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.20s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
                   all         55        256      0.283      0.246      0.168     0.0493
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.08437 0.04643 0.02113 101 640:   0%|          | 0/19 [00:00<?, ?it/s]71/99 7.3G 0.08437 0.04643 0.02113 101 640:   5%|▌         | 1/19 [00:00<00:03,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07917 0.04048 0.02065 61 640:   5%|▌         | 1/19 [00:00<00:03,  4.74it/s] 71/99 7.3G 0.07917 0.04048 0.02065 61 640:  11%|█         | 2/19 [00:00<00:03,  5.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07864 0.04223 0.01969 86 640:  11%|█         | 2/19 [00:00<00:03,  5.28it/s]71/99 7.3G 0.07864 0.04223 0.01969 86 640:  16%|█▌        | 3/19 [00:00<00:02,  5.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07654 0.03914 0.0199 50 640:  16%|█▌        | 3/19 [00:00<00:02,  5.49it/s] 71/99 7.3G 0.07654 0.03914 0.0199 50 640:  21%|██        | 4/19 [00:00<00:02,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07681 0.03623 0.02031 42 640:  21%|██        | 4/19 [00:00<00:02,  5.59it/s]71/99 7.3G 0.07681 0.03623 0.02031 42 640:  26%|██▋       | 5/19 [00:00<00:02,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07851 0.04184 0.02023 163 640:  26%|██▋       | 5/19 [00:01<00:02,  5.65it/s]71/99 7.3G 0.07851 0.04184 0.02023 163 640:  32%|███▏      | 6/19 [00:01<00:02,  5.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07876 0.04191 0.02083 67 640:  32%|███▏      | 6/19 [00:01<00:02,  5.07it/s] 71/99 7.3G 0.07876 0.04191 0.02083 67 640:  37%|███▋      | 7/19 [00:01<00:03,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07817 0.0428 0.02085 86 640:  37%|███▋      | 7/19 [00:01<00:03,  3.83it/s] 71/99 7.3G 0.07817 0.0428 0.02085 86 640:  42%|████▏     | 8/19 [00:01<00:02,  4.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07678 0.04106 0.0209 38 640:  42%|████▏     | 8/19 [00:02<00:02,  4.28it/s]71/99 7.3G 0.07678 0.04106 0.0209 38 640:  47%|████▋     | 9/19 [00:02<00:02,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.0765 0.04129 0.02053 76 640:  47%|████▋     | 9/19 [00:02<00:02,  3.41it/s]71/99 7.3G 0.0765 0.04129 0.02053 76 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07641 0.04121 0.02088 71 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.15it/s]71/99 7.3G 0.07641 0.04121 0.02088 71 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07702 0.0437 0.02116 128 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.57it/s]71/99 7.3G 0.07702 0.0437 0.02116 128 640:  63%|██████▎   | 12/19 [00:05<00:06,  1.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07746 0.04383 0.02088 92 640:  63%|██████▎   | 12/19 [00:05<00:06,  1.10it/s]71/99 7.3G 0.07746 0.04383 0.02088 92 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07753 0.04398 0.0211 78 640:  68%|██████▊   | 13/19 [00:09<00:04,  1.46it/s] 71/99 7.3G 0.07753 0.04398 0.0211 78 640:  74%|███████▎  | 14/19 [00:09<00:07,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07761 0.04373 0.0211 64 640:  74%|███████▎  | 14/19 [00:13<00:07,  1.59s/it]71/99 7.3G 0.07761 0.04373 0.0211 64 640:  79%|███████▉  | 15/19 [00:13<00:09,  2.32s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.0774 0.04324 0.02108 56 640:  79%|███████▉  | 15/19 [00:13<00:09,  2.32s/it]71/99 7.3G 0.0774 0.04324 0.02108 56 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07752 0.04356 0.02116 82 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.67s/it]71/99 7.3G 0.07752 0.04356 0.02116 82 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07761 0.04371 0.02131 82 640:  89%|████████▉ | 17/19 [00:15<00:02,  1.48s/it]71/99 7.3G 0.07761 0.04371 0.02131 82 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
71/99 7.3G 0.07759 0.04424 0.02129 91 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.38s/it]71/99 7.3G 0.07759 0.04424 0.02129 91 640: 100%|██████████| 19/19 [00:15<00:00,  1.02s/it]71/99 7.3G 0.07759 0.04424 0.02129 91 640: 100%|██████████| 19/19 [00:15<00:00,  1.22it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.54s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.17s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
                   all         55        256      0.244      0.202      0.131     0.0421
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.0809 0.05828 0.02185 103 640:   0%|          | 0/19 [00:00<?, ?it/s]72/99 7.3G 0.0809 0.05828 0.02185 103 640:   5%|▌         | 1/19 [00:00<00:12,  1.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07836 0.05124 0.02246 72 640:   5%|▌         | 1/19 [00:01<00:12,  1.47it/s]72/99 7.3G 0.07836 0.05124 0.02246 72 640:  11%|█         | 2/19 [00:01<00:11,  1.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07781 0.05122 0.02177 90 640:  11%|█         | 2/19 [00:01<00:11,  1.43it/s]72/99 7.3G 0.07781 0.05122 0.02177 90 640:  16%|█▌        | 3/19 [00:01<00:10,  1.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07852 0.0498 0.02136 87 640:  16%|█▌        | 3/19 [00:02<00:10,  1.58it/s] 72/99 7.3G 0.07852 0.0498 0.02136 87 640:  21%|██        | 4/19 [00:02<00:08,  1.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07773 0.04796 0.02129 71 640:  21%|██        | 4/19 [00:02<00:08,  1.70it/s]72/99 7.3G 0.07773 0.04796 0.02129 71 640:  26%|██▋       | 5/19 [00:02<00:07,  1.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07823 0.04839 0.02206 86 640:  26%|██▋       | 5/19 [00:03<00:07,  1.77it/s]72/99 7.3G 0.07823 0.04839 0.02206 86 640:  32%|███▏      | 6/19 [00:03<00:06,  1.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07791 0.04828 0.02266 83 640:  32%|███▏      | 6/19 [00:03<00:06,  1.91it/s]72/99 7.3G 0.07791 0.04828 0.02266 83 640:  37%|███▋      | 7/19 [00:03<00:05,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07741 0.04572 0.02254 43 640:  37%|███▋      | 7/19 [00:04<00:05,  2.12it/s]72/99 7.3G 0.07741 0.04572 0.02254 43 640:  42%|████▏     | 8/19 [00:04<00:04,  2.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07689 0.04406 0.02239 51 640:  42%|████▏     | 8/19 [00:04<00:04,  2.22it/s]72/99 7.3G 0.07689 0.04406 0.02239 51 640:  47%|████▋     | 9/19 [00:04<00:04,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07719 0.04438 0.02224 85 640:  47%|████▋     | 9/19 [00:05<00:04,  2.12it/s]72/99 7.3G 0.07719 0.04438 0.02224 85 640:  53%|█████▎    | 10/19 [00:05<00:04,  2.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07832 0.04825 0.02198 218 640:  53%|█████▎    | 10/19 [00:06<00:04,  2.07it/s]72/99 7.3G 0.07832 0.04825 0.02198 218 640:  58%|█████▊    | 11/19 [00:06<00:06,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.0778 0.04725 0.02229 53 640:  58%|█████▊    | 11/19 [00:10<00:06,  1.19it/s]  72/99 7.3G 0.0778 0.04725 0.02229 53 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.0775 0.04713 0.02205 78 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.56s/it]72/99 7.3G 0.0775 0.04713 0.02205 78 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07728 0.04644 0.02216 64 640:  68%|██████▊   | 13/19 [00:12<00:06,  1.14s/it]72/99 7.3G 0.07728 0.04644 0.02216 64 640:  74%|███████▎  | 14/19 [00:12<00:06,  1.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07693 0.0458 0.0221 64 640:  74%|███████▎  | 14/19 [00:12<00:06,  1.34s/it]  72/99 7.3G 0.07693 0.0458 0.0221 64 640:  79%|███████▉  | 15/19 [00:12<00:04,  1.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07693 0.0455 0.02198 74 640:  79%|███████▉  | 15/19 [00:12<00:04,  1.02s/it]72/99 7.3G 0.07693 0.0455 0.02198 74 640:  84%|████████▍ | 16/19 [00:12<00:02,  1.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07684 0.04487 0.02229 53 640:  84%|████████▍ | 16/19 [00:19<00:02,  1.29it/s]72/99 7.3G 0.07684 0.04487 0.02229 53 640:  89%|████████▉ | 17/19 [00:19<00:05,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07722 0.04554 0.02235 108 640:  89%|████████▉ | 17/19 [00:19<00:05,  2.63s/it]72/99 7.3G 0.07722 0.04554 0.02235 108 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
72/99 7.3G 0.07746 0.04558 0.02239 81 640:  95%|█████████▍| 18/19 [00:23<00:01,  1.89s/it] 72/99 7.3G 0.07746 0.04558 0.02239 81 640: 100%|██████████| 19/19 [00:23<00:00,  2.38s/it]72/99 7.3G 0.07746 0.04558 0.02239 81 640: 100%|██████████| 19/19 [00:23<00:00,  1.22s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.22s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.03s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]
                   all         55        256       0.23      0.206      0.122     0.0392
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07503 0.03131 0.02111 51 640:   0%|          | 0/19 [00:00<?, ?it/s]73/99 7.3G 0.07503 0.03131 0.02111 51 640:   5%|▌         | 1/19 [00:00<00:03,  5.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07632 0.04256 0.01978 97 640:   5%|▌         | 1/19 [00:00<00:03,  5.09it/s]73/99 7.3G 0.07632 0.04256 0.01978 97 640:  11%|█         | 2/19 [00:00<00:03,  5.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07404 0.03903 0.02002 52 640:  11%|█         | 2/19 [00:00<00:03,  5.42it/s]73/99 7.3G 0.07404 0.03903 0.02002 52 640:  16%|█▌        | 3/19 [00:00<00:02,  5.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07464 0.03838 0.02073 61 640:  16%|█▌        | 3/19 [00:00<00:02,  5.54it/s]73/99 7.3G 0.07464 0.03838 0.02073 61 640:  21%|██        | 4/19 [00:00<00:02,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07489 0.04067 0.02114 80 640:  21%|██        | 4/19 [00:00<00:02,  5.60it/s]73/99 7.3G 0.07489 0.04067 0.02114 80 640:  26%|██▋       | 5/19 [00:00<00:02,  5.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07587 0.0429 0.02139 100 640:  26%|██▋       | 5/19 [00:01<00:02,  5.42it/s]73/99 7.3G 0.07587 0.0429 0.02139 100 640:  32%|███▏      | 6/19 [00:01<00:03,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07568 0.04199 0.02244 58 640:  32%|███▏      | 6/19 [00:01<00:03,  4.12it/s]73/99 7.3G 0.07568 0.04199 0.02244 58 640:  37%|███▋      | 7/19 [00:01<00:03,  3.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07747 0.04413 0.02208 150 640:  37%|███▋      | 7/19 [00:01<00:03,  3.57it/s]73/99 7.3G 0.07747 0.04413 0.02208 150 640:  42%|████▏     | 8/19 [00:01<00:03,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07716 0.04528 0.02226 87 640:  42%|████▏     | 8/19 [00:02<00:03,  3.41it/s] 73/99 7.3G 0.07716 0.04528 0.02226 87 640:  47%|████▋     | 9/19 [00:02<00:02,  3.91it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07742 0.0458 0.02219 88 640:  47%|████▋     | 9/19 [00:02<00:02,  3.91it/s] 73/99 7.3G 0.07742 0.0458 0.02219 88 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07854 0.0453 0.02237 91 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.11it/s]73/99 7.3G 0.07854 0.0453 0.02237 91 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07935 0.04534 0.02236 102 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.68it/s]73/99 7.3G 0.07935 0.04534 0.02236 102 640:  63%|██████▎   | 12/19 [00:04<00:03,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07947 0.04616 0.02266 96 640:  63%|██████▎   | 12/19 [00:04<00:03,  1.99it/s] 73/99 7.3G 0.07947 0.04616 0.02266 96 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07975 0.04721 0.02276 102 640:  68%|██████▊   | 13/19 [00:07<00:02,  2.42it/s]73/99 7.3G 0.07975 0.04721 0.02276 102 640:  74%|███████▎  | 14/19 [00:07<00:07,  1.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.0796 0.04648 0.02252 65 640:  74%|███████▎  | 14/19 [00:08<00:07,  1.41s/it]  73/99 7.3G 0.0796 0.04648 0.02252 65 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.0792 0.0459 0.02272 60 640:  79%|███████▉  | 15/19 [00:13<00:04,  1.09s/it] 73/99 7.3G 0.0792 0.0459 0.02272 60 640:  84%|████████▍ | 16/19 [00:13<00:07,  2.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.07984 0.04643 0.02262 133 640:  84%|████████▍ | 16/19 [00:17<00:07,  2.42s/it]73/99 7.3G 0.07984 0.04643 0.02262 133 640:  89%|████████▉ | 17/19 [00:17<00:05,  2.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.08002 0.04698 0.02272 98 640:  89%|████████▉ | 17/19 [00:18<00:05,  2.73s/it] 73/99 7.3G 0.08002 0.04698 0.02272 98 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
73/99 7.3G 0.08017 0.0474 0.02261 103 640:  95%|█████████▍| 18/19 [00:20<00:02,  2.21s/it]73/99 7.3G 0.08017 0.0474 0.02261 103 640: 100%|██████████| 19/19 [00:20<00:00,  2.23s/it]73/99 7.3G 0.08017 0.0474 0.02261 103 640: 100%|██████████| 19/19 [00:20<00:00,  1.08s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]
                   all         55        256      0.209      0.173      0.107     0.0334
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07686 0.0469 0.02246 79 640:   0%|          | 0/19 [00:00<?, ?it/s]74/99 7.3G 0.07686 0.0469 0.02246 79 640:   5%|▌         | 1/19 [00:00<00:05,  3.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08013 0.04625 0.0221 91 640:   5%|▌         | 1/19 [00:00<00:05,  3.19it/s]74/99 7.3G 0.08013 0.04625 0.0221 91 640:  11%|█         | 2/19 [00:00<00:04,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08138 0.04664 0.02258 85 640:  11%|█         | 2/19 [00:01<00:04,  3.41it/s]74/99 7.3G 0.08138 0.04664 0.02258 85 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08192 0.0515 0.02328 121 640:  16%|█▌        | 3/19 [00:01<00:05,  2.85it/s]74/99 7.3G 0.08192 0.0515 0.02328 121 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08226 0.05525 0.02372 118 640:  21%|██        | 4/19 [00:01<00:04,  3.55it/s]74/99 7.3G 0.08226 0.05525 0.02372 118 640:  26%|██▋       | 5/19 [00:01<00:03,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08242 0.05597 0.02346 118 640:  26%|██▋       | 5/19 [00:01<00:03,  4.12it/s]74/99 7.3G 0.08242 0.05597 0.02346 118 640:  32%|███▏      | 6/19 [00:01<00:03,  4.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08128 0.05494 0.02318 84 640:  32%|███▏      | 6/19 [00:02<00:03,  4.09it/s] 74/99 7.3G 0.08128 0.05494 0.02318 84 640:  37%|███▋      | 7/19 [00:02<00:03,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.08083 0.05208 0.02261 61 640:  37%|███▋      | 7/19 [00:02<00:03,  3.38it/s]74/99 7.3G 0.08083 0.05208 0.02261 61 640:  42%|████▏     | 8/19 [00:02<00:03,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07944 0.04957 0.02219 42 640:  42%|████▏     | 8/19 [00:02<00:03,  3.46it/s]74/99 7.3G 0.07944 0.04957 0.02219 42 640:  47%|████▋     | 9/19 [00:02<00:02,  3.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07921 0.04938 0.02229 79 640:  47%|████▋     | 9/19 [00:02<00:02,  3.96it/s]74/99 7.3G 0.07921 0.04938 0.02229 79 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07951 0.04982 0.02188 105 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.39it/s]74/99 7.3G 0.07951 0.04982 0.02188 105 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07933 0.04966 0.02234 79 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.57it/s] 74/99 7.3G 0.07933 0.04966 0.02234 79 640:  63%|██████▎   | 12/19 [00:03<00:02,  3.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07917 0.04923 0.0226 74 640:  63%|██████▎   | 12/19 [00:09<00:02,  3.09it/s] 74/99 7.3G 0.07917 0.04923 0.0226 74 640:  68%|██████▊   | 13/19 [00:09<00:11,  1.97s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07869 0.04921 0.02244 78 640:  68%|██████▊   | 13/19 [00:11<00:11,  1.97s/it]74/99 7.3G 0.07869 0.04921 0.02244 78 640:  74%|███████▎  | 14/19 [00:11<00:09,  1.95s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07851 0.04801 0.02225 54 640:  74%|███████▎  | 14/19 [00:11<00:09,  1.95s/it]74/99 7.3G 0.07851 0.04801 0.02225 54 640:  79%|███████▉  | 15/19 [00:11<00:05,  1.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07845 0.04806 0.02211 86 640:  79%|███████▉  | 15/19 [00:14<00:05,  1.42s/it]74/99 7.3G 0.07845 0.04806 0.02211 86 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.80s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07878 0.0479 0.02215 86 640:  84%|████████▍ | 16/19 [00:15<00:05,  1.80s/it] 74/99 7.3G 0.07878 0.0479 0.02215 86 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07889 0.04787 0.02208 81 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.70s/it]74/99 7.3G 0.07889 0.04787 0.02208 81 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
74/99 7.3G 0.07908 0.04691 0.02213 49 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.35s/it]74/99 7.3G 0.07908 0.04691 0.02213 49 640: 100%|██████████| 19/19 [00:19<00:00,  1.98s/it]74/99 7.3G 0.07908 0.04691 0.02213 49 640: 100%|██████████| 19/19 [00:19<00:00,  1.03s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.94s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.92s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
                   all         55        256      0.263      0.202      0.137     0.0355
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07724 0.0325 0.01959 60 640:   0%|          | 0/19 [00:00<?, ?it/s]75/99 7.3G 0.07724 0.0325 0.01959 60 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.08235 0.05651 0.02129 174 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]75/99 7.3G 0.08235 0.05651 0.02129 174 640:  11%|█         | 2/19 [00:00<00:04,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07873 0.04859 0.01966 56 640:  11%|█         | 2/19 [00:00<00:04,  4.14it/s] 75/99 7.3G 0.07873 0.04859 0.01966 56 640:  16%|█▌        | 3/19 [00:00<00:04,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07549 0.04325 0.01825 42 640:  16%|█▌        | 3/19 [00:01<00:04,  3.46it/s]75/99 7.3G 0.07549 0.04325 0.01825 42 640:  21%|██        | 4/19 [00:01<00:04,  3.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.0771 0.04329 0.01927 77 640:  21%|██        | 4/19 [00:01<00:04,  3.69it/s] 75/99 7.3G 0.0771 0.04329 0.01927 77 640:  26%|██▋       | 5/19 [00:01<00:03,  4.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07734 0.04451 0.01949 93 640:  26%|██▋       | 5/19 [00:01<00:03,  4.25it/s]75/99 7.3G 0.07734 0.04451 0.01949 93 640:  32%|███▏      | 6/19 [00:01<00:02,  4.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07694 0.04306 0.02077 54 640:  32%|███▏      | 6/19 [00:01<00:02,  4.66it/s]75/99 7.3G 0.07694 0.04306 0.02077 54 640:  37%|███▋      | 7/19 [00:01<00:02,  4.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07814 0.04546 0.02036 153 640:  37%|███▋      | 7/19 [00:01<00:02,  4.97it/s]75/99 7.3G 0.07814 0.04546 0.02036 153 640:  42%|████▏     | 8/19 [00:01<00:02,  5.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07771 0.04475 0.02088 62 640:  42%|████▏     | 8/19 [00:01<00:02,  5.19it/s] 75/99 7.3G 0.07771 0.04475 0.02088 62 640:  47%|████▋     | 9/19 [00:01<00:01,  5.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07787 0.04524 0.02113 88 640:  47%|████▋     | 9/19 [00:02<00:01,  5.36it/s]75/99 7.3G 0.07787 0.04524 0.02113 88 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07815 0.04497 0.02164 75 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.29it/s]75/99 7.3G 0.07815 0.04497 0.02164 75 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07766 0.04531 0.02155 81 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.41it/s]75/99 7.3G 0.07766 0.04531 0.02155 81 640:  63%|██████▎   | 12/19 [00:02<00:01,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07683 0.04353 0.02142 36 640:  63%|██████▎   | 12/19 [00:05<00:01,  3.87it/s]75/99 7.3G 0.07683 0.04353 0.02142 36 640:  68%|██████▊   | 13/19 [00:05<00:06,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07651 0.04235 0.02144 40 640:  68%|██████▊   | 13/19 [00:06<00:06,  1.03s/it]75/99 7.3G 0.07651 0.04235 0.02144 40 640:  74%|███████▎  | 14/19 [00:06<00:05,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07662 0.04254 0.02158 71 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.01s/it]75/99 7.3G 0.07662 0.04254 0.02158 71 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.0771 0.04369 0.02165 112 640:  79%|███████▉  | 15/19 [00:13<00:04,  1.23s/it]75/99 7.3G 0.0771 0.04369 0.02165 112 640:  84%|████████▍ | 16/19 [00:13<00:07,  2.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07685 0.04323 0.02176 55 640:  84%|████████▍ | 16/19 [00:15<00:07,  2.34s/it]75/99 7.3G 0.07685 0.04323 0.02176 55 640:  89%|████████▉ | 17/19 [00:15<00:04,  2.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07702 0.04263 0.02165 56 640:  89%|████████▉ | 17/19 [00:18<00:04,  2.18s/it]75/99 7.3G 0.07702 0.04263 0.02165 56 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
75/99 7.3G 0.07723 0.04272 0.02166 86 640:  95%|█████████▍| 18/19 [00:19<00:02,  2.48s/it]75/99 7.3G 0.07723 0.04272 0.02166 86 640: 100%|██████████| 19/19 [00:19<00:00,  2.01s/it]75/99 7.3G 0.07723 0.04272 0.02166 86 640: 100%|██████████| 19/19 [00:19<00:00,  1.01s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.89s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
                   all         55        256      0.188      0.186      0.104     0.0307
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07672 0.03335 0.01833 56 640:   0%|          | 0/19 [00:00<?, ?it/s]76/99 7.3G 0.07672 0.03335 0.01833 56 640:   5%|▌         | 1/19 [00:00<00:03,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07985 0.03045 0.01834 57 640:   5%|▌         | 1/19 [00:00<00:03,  5.61it/s]76/99 7.3G 0.07985 0.03045 0.01834 57 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08071 0.03346 0.02046 69 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]76/99 7.3G 0.08071 0.03346 0.02046 69 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07894 0.03183 0.02062 45 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]76/99 7.3G 0.07894 0.03183 0.02062 45 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.0794 0.03746 0.02137 113 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]76/99 7.3G 0.0794 0.03746 0.02137 113 640:  26%|██▋       | 5/19 [00:00<00:02,  5.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08021 0.03858 0.02095 100 640:  26%|██▋       | 5/19 [00:01<00:02,  5.43it/s]76/99 7.3G 0.08021 0.03858 0.02095 100 640:  32%|███▏      | 6/19 [00:01<00:02,  5.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08083 0.0394 0.02185 80 640:  32%|███▏      | 6/19 [00:01<00:02,  5.26it/s]  76/99 7.3G 0.08083 0.0394 0.02185 80 640:  37%|███▋      | 7/19 [00:01<00:02,  5.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08036 0.04069 0.02237 83 640:  37%|███▋      | 7/19 [00:01<00:02,  5.41it/s]76/99 7.3G 0.08036 0.04069 0.02237 83 640:  42%|████▏     | 8/19 [00:01<00:01,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08049 0.04223 0.02234 100 640:  42%|████▏     | 8/19 [00:01<00:01,  5.51it/s]76/99 7.3G 0.08049 0.04223 0.02234 100 640:  47%|████▋     | 9/19 [00:01<00:01,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08019 0.04448 0.02233 104 640:  47%|████▋     | 9/19 [00:01<00:01,  5.59it/s]76/99 7.3G 0.08019 0.04448 0.02233 104 640:  53%|█████▎    | 10/19 [00:01<00:01,  4.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08073 0.04584 0.02248 113 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.76it/s]76/99 7.3G 0.08073 0.04584 0.02248 113 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.081 0.04759 0.02216 125 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.12it/s]  76/99 7.3G 0.081 0.04759 0.02216 125 640:  63%|██████▎   | 12/19 [00:02<00:02,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08082 0.04541 0.02171 29 640:  63%|██████▎   | 12/19 [00:06<00:02,  2.67it/s]76/99 7.3G 0.08082 0.04541 0.02171 29 640:  68%|██████▊   | 13/19 [00:06<00:08,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08078 0.04482 0.02157 64 640:  68%|██████▊   | 13/19 [00:08<00:08,  1.44s/it]76/99 7.3G 0.08078 0.04482 0.02157 64 640:  74%|███████▎  | 14/19 [00:08<00:07,  1.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08058 0.04461 0.02135 81 640:  74%|███████▎  | 14/19 [00:12<00:07,  1.47s/it]76/99 7.3G 0.08058 0.04461 0.02135 81 640:  79%|███████▉  | 15/19 [00:12<00:08,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08019 0.04423 0.02117 73 640:  79%|███████▉  | 15/19 [00:13<00:08,  2.15s/it]76/99 7.3G 0.08019 0.04423 0.02117 73 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.08024 0.04451 0.02128 89 640:  84%|████████▍ | 16/19 [00:15<00:05,  1.98s/it]76/99 7.3G 0.08024 0.04451 0.02128 89 640:  89%|████████▉ | 17/19 [00:15<00:04,  2.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07989 0.04384 0.02113 56 640:  89%|████████▉ | 17/19 [00:16<00:04,  2.06s/it]76/99 7.3G 0.07989 0.04384 0.02113 56 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
76/99 7.3G 0.07988 0.04393 0.0211 83 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.55s/it] 76/99 7.3G 0.07988 0.04393 0.0211 83 640: 100%|██████████| 19/19 [00:17<00:00,  1.57s/it]76/99 7.3G 0.07988 0.04393 0.0211 83 640: 100%|██████████| 19/19 [00:17<00:00,  1.06it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.42s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.70s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]
                   all         55        256      0.213      0.177      0.112     0.0368
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07984 0.04788 0.01998 86 640:   0%|          | 0/19 [00:00<?, ?it/s]77/99 7.3G 0.07984 0.04788 0.01998 86 640:   5%|▌         | 1/19 [00:00<00:03,  4.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.08131 0.03884 0.01931 51 640:   5%|▌         | 1/19 [00:00<00:03,  4.77it/s]77/99 7.3G 0.08131 0.03884 0.01931 51 640:  11%|█         | 2/19 [00:00<00:03,  4.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.08257 0.04669 0.01958 131 640:  11%|█         | 2/19 [00:00<00:03,  4.80it/s]77/99 7.3G 0.08257 0.04669 0.01958 131 640:  16%|█▌        | 3/19 [00:00<00:03,  4.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.08271 0.04608 0.02079 83 640:  16%|█▌        | 3/19 [00:00<00:03,  4.61it/s] 77/99 7.3G 0.08271 0.04608 0.02079 83 640:  21%|██        | 4/19 [00:00<00:03,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.08151 0.04202 0.02096 43 640:  21%|██        | 4/19 [00:01<00:03,  4.69it/s]77/99 7.3G 0.08151 0.04202 0.02096 43 640:  26%|██▋       | 5/19 [00:01<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.08135 0.04176 0.02143 77 640:  26%|██▋       | 5/19 [00:01<00:02,  4.74it/s]77/99 7.3G 0.08135 0.04176 0.02143 77 640:  32%|███▏      | 6/19 [00:01<00:02,  4.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07925 0.03913 0.02048 39 640:  32%|███▏      | 6/19 [00:01<00:02,  4.75it/s]77/99 7.3G 0.07925 0.03913 0.02048 39 640:  37%|███▋      | 7/19 [00:01<00:02,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07863 0.03851 0.0201 55 640:  37%|███▋      | 7/19 [00:01<00:02,  4.98it/s] 77/99 7.3G 0.07863 0.03851 0.0201 55 640:  42%|████▏     | 8/19 [00:01<00:02,  5.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.0786 0.03747 0.02012 48 640:  42%|████▏     | 8/19 [00:01<00:02,  5.21it/s]77/99 7.3G 0.0786 0.03747 0.02012 48 640:  47%|████▋     | 9/19 [00:01<00:01,  5.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07864 0.03887 0.02071 86 640:  47%|████▋     | 9/19 [00:04<00:01,  5.37it/s]77/99 7.3G 0.07864 0.03887 0.02071 86 640:  53%|█████▎    | 10/19 [00:04<00:07,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07852 0.0397 0.0207 81 640:  53%|█████▎    | 10/19 [00:04<00:07,  1.19it/s]  77/99 7.3G 0.07852 0.0397 0.0207 81 640:  58%|█████▊    | 11/19 [00:04<00:05,  1.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07869 0.04011 0.02063 83 640:  58%|█████▊    | 11/19 [00:08<00:05,  1.57it/s]77/99 7.3G 0.07869 0.04011 0.02063 83 640:  63%|██████▎   | 12/19 [00:08<00:11,  1.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.0782 0.03919 0.02054 46 640:  63%|██████▎   | 12/19 [00:09<00:11,  1.65s/it] 77/99 7.3G 0.0782 0.03919 0.02054 46 640:  68%|██████▊   | 13/19 [00:09<00:10,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07787 0.03848 0.0208 46 640:  68%|██████▊   | 13/19 [00:11<00:10,  1.68s/it]77/99 7.3G 0.07787 0.03848 0.0208 46 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07798 0.03921 0.02122 79 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.76s/it]77/99 7.3G 0.07798 0.03921 0.02122 79 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07793 0.03884 0.02142 52 640:  79%|███████▉  | 15/19 [00:13<00:05,  1.29s/it]77/99 7.3G 0.07793 0.03884 0.02142 52 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07804 0.03941 0.02144 85 640:  84%|████████▍ | 16/19 [00:15<00:03,  1.27s/it]77/99 7.3G 0.07804 0.03941 0.02144 85 640:  89%|████████▉ | 17/19 [00:15<00:02,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07778 0.03911 0.0213 59 640:  89%|████████▉ | 17/19 [00:18<00:02,  1.44s/it] 77/99 7.3G 0.07778 0.03911 0.0213 59 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
77/99 7.3G 0.07755 0.03907 0.02162 60 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.98s/it]77/99 7.3G 0.07755 0.03907 0.02162 60 640: 100%|██████████| 19/19 [00:18<00:00,  1.44s/it]77/99 7.3G 0.07755 0.03907 0.02162 60 640: 100%|██████████| 19/19 [00:18<00:00,  1.02it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.51s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.15s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
                   all         55        256      0.227      0.191      0.128     0.0374
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.08206 0.06065 0.02321 115 640:   0%|          | 0/19 [00:00<?, ?it/s]78/99 7.3G 0.08206 0.06065 0.02321 115 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.082 0.06168 0.02293 118 640:   5%|▌         | 1/19 [00:00<00:06,  2.80it/s]  78/99 7.3G 0.082 0.06168 0.02293 118 640:  11%|█         | 2/19 [00:00<00:05,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.08062 0.04993 0.02182 43 640:  11%|█         | 2/19 [00:00<00:05,  3.22it/s]78/99 7.3G 0.08062 0.04993 0.02182 43 640:  16%|█▌        | 3/19 [00:00<00:04,  3.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07972 0.04554 0.02091 59 640:  16%|█▌        | 3/19 [00:01<00:04,  3.54it/s]78/99 7.3G 0.07972 0.04554 0.02091 59 640:  21%|██        | 4/19 [00:01<00:04,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07855 0.0424 0.02002 50 640:  21%|██        | 4/19 [00:01<00:04,  3.21it/s] 78/99 7.3G 0.07855 0.0424 0.02002 50 640:  26%|██▋       | 5/19 [00:01<00:04,  3.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.0788 0.04424 0.01961 103 640:  26%|██▋       | 5/19 [00:01<00:04,  3.08it/s]78/99 7.3G 0.0788 0.04424 0.01961 103 640:  32%|███▏      | 6/19 [00:01<00:03,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07924 0.04472 0.01983 86 640:  32%|███▏      | 6/19 [00:01<00:03,  3.65it/s]78/99 7.3G 0.07924 0.04472 0.01983 86 640:  37%|███▋      | 7/19 [00:01<00:02,  4.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07883 0.04549 0.02077 78 640:  37%|███▋      | 7/19 [00:02<00:02,  4.12it/s]78/99 7.3G 0.07883 0.04549 0.02077 78 640:  42%|████▏     | 8/19 [00:02<00:02,  4.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07819 0.0435 0.02077 47 640:  42%|████▏     | 8/19 [00:02<00:02,  4.51it/s] 78/99 7.3G 0.07819 0.0435 0.02077 47 640:  47%|████▋     | 9/19 [00:02<00:02,  4.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07811 0.0436 0.02104 77 640:  47%|████▋     | 9/19 [00:03<00:02,  4.74it/s]78/99 7.3G 0.07811 0.0436 0.02104 77 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07797 0.04323 0.02098 70 640:  53%|█████▎    | 10/19 [00:04<00:04,  2.21it/s]78/99 7.3G 0.07797 0.04323 0.02098 70 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07742 0.04275 0.02098 59 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.65it/s]78/99 7.3G 0.07742 0.04275 0.02098 59 640:  63%|██████▎   | 12/19 [00:04<00:03,  1.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07775 0.04369 0.02141 89 640:  63%|██████▎   | 12/19 [00:05<00:03,  1.92it/s]78/99 7.3G 0.07775 0.04369 0.02141 89 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07742 0.04298 0.02117 54 640:  68%|██████▊   | 13/19 [00:07<00:04,  1.27it/s]78/99 7.3G 0.07742 0.04298 0.02117 54 640:  74%|███████▎  | 14/19 [00:07<00:05,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.0775 0.0433 0.02098 84 640:  74%|███████▎  | 14/19 [00:12<00:05,  1.09s/it]  78/99 7.3G 0.0775 0.0433 0.02098 84 640:  79%|███████▉  | 15/19 [00:12<00:09,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07706 0.04278 0.021 53 640:  79%|███████▉  | 15/19 [00:13<00:09,  2.29s/it]78/99 7.3G 0.07706 0.04278 0.021 53 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.67s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07733 0.04267 0.02094 72 640:  84%|████████▍ | 16/19 [00:17<00:05,  1.67s/it]78/99 7.3G 0.07733 0.04267 0.02094 72 640:  89%|████████▉ | 17/19 [00:17<00:05,  2.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07781 0.04315 0.02087 108 640:  89%|████████▉ | 17/19 [00:18<00:05,  2.65s/it]78/99 7.3G 0.07781 0.04315 0.02087 108 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
78/99 7.3G 0.07781 0.04392 0.02067 106 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.11s/it]78/99 7.3G 0.07781 0.04392 0.02067 106 640: 100%|██████████| 19/19 [00:21<00:00,  2.30s/it]78/99 7.3G 0.07781 0.04392 0.02067 106 640: 100%|██████████| 19/19 [00:21<00:00,  1.14s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.01s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.16s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
                   all         55        256      0.214      0.187      0.111     0.0354
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08295 0.0446 0.01799 93 640:   0%|          | 0/19 [00:00<?, ?it/s]79/99 7.3G 0.08295 0.0446 0.01799 93 640:   5%|▌         | 1/19 [00:00<00:03,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07895 0.03601 0.01944 43 640:   5%|▌         | 1/19 [00:00<00:03,  4.98it/s]79/99 7.3G 0.07895 0.03601 0.01944 43 640:  11%|█         | 2/19 [00:00<00:04,  3.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07885 0.03823 0.01877 84 640:  11%|█         | 2/19 [00:00<00:04,  3.51it/s]79/99 7.3G 0.07885 0.03823 0.01877 84 640:  16%|█▌        | 3/19 [00:00<00:04,  3.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07923 0.03842 0.01858 72 640:  16%|█▌        | 3/19 [00:00<00:04,  3.64it/s]79/99 7.3G 0.07923 0.03842 0.01858 72 640:  21%|██        | 4/19 [00:00<00:03,  4.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08019 0.04065 0.01947 89 640:  21%|██        | 4/19 [00:01<00:03,  4.25it/s]79/99 7.3G 0.08019 0.04065 0.01947 89 640:  26%|██▋       | 5/19 [00:01<00:03,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08006 0.04426 0.02047 111 640:  26%|██▋       | 5/19 [00:01<00:03,  4.40it/s]79/99 7.3G 0.08006 0.04426 0.02047 111 640:  32%|███▏      | 6/19 [00:01<00:03,  4.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08051 0.04474 0.02061 87 640:  32%|███▏      | 6/19 [00:01<00:03,  4.00it/s] 79/99 7.3G 0.08051 0.04474 0.02061 87 640:  37%|███▋      | 7/19 [00:01<00:03,  3.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08019 0.04387 0.02106 59 640:  37%|███▋      | 7/19 [00:01<00:03,  3.76it/s]79/99 7.3G 0.08019 0.04387 0.02106 59 640:  42%|████▏     | 8/19 [00:01<00:02,  4.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.0798 0.04327 0.02119 64 640:  42%|████▏     | 8/19 [00:02<00:02,  4.04it/s] 79/99 7.3G 0.0798 0.04327 0.02119 64 640:  47%|████▋     | 9/19 [00:02<00:02,  4.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07961 0.04365 0.02136 84 640:  47%|████▋     | 9/19 [00:02<00:02,  4.45it/s]79/99 7.3G 0.07961 0.04365 0.02136 84 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07925 0.04241 0.02156 50 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.79it/s]79/99 7.3G 0.07925 0.04241 0.02156 50 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08015 0.04335 0.0212 152 640:  58%|█████▊    | 11/19 [00:03<00:01,  5.05it/s]79/99 7.3G 0.08015 0.04335 0.0212 152 640:  63%|██████▎   | 12/19 [00:03<00:04,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07972 0.04317 0.0212 72 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.73it/s] 79/99 7.3G 0.07972 0.04317 0.0212 72 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08043 0.04535 0.02104 188 640:  68%|██████▊   | 13/19 [00:10<00:04,  1.25it/s]79/99 7.3G 0.08043 0.04535 0.02104 188 640:  74%|███████▎  | 14/19 [00:10<00:10,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08046 0.04499 0.0212 69 640:  74%|███████▎  | 14/19 [00:11<00:10,  2.00s/it]  79/99 7.3G 0.08046 0.04499 0.0212 69 640:  79%|███████▉  | 15/19 [00:11<00:07,  1.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08058 0.04578 0.02146 103 640:  79%|███████▉  | 15/19 [00:15<00:07,  1.79s/it]79/99 7.3G 0.08058 0.04578 0.02146 103 640:  84%|████████▍ | 16/19 [00:15<00:07,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.08005 0.04539 0.02163 58 640:  84%|████████▍ | 16/19 [00:16<00:07,  2.63s/it] 79/99 7.3G 0.08005 0.04539 0.02163 58 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07963 0.04522 0.02152 73 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.89s/it]79/99 7.3G 0.07963 0.04522 0.02152 73 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
79/99 7.3G 0.07925 0.04479 0.02163 57 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.37s/it]79/99 7.3G 0.07925 0.04479 0.02163 57 640: 100%|██████████| 19/19 [00:17<00:00,  1.35s/it]79/99 7.3G 0.07925 0.04479 0.02163 57 640: 100%|██████████| 19/19 [00:17<00:00,  1.08it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.55s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
                   all         55        256      0.197      0.195       0.11     0.0332
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07613 0.04643 0.02669 79 640:   0%|          | 0/19 [00:00<?, ?it/s]80/99 7.3G 0.07613 0.04643 0.02669 79 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.08139 0.06969 0.02369 201 640:   5%|▌         | 1/19 [00:00<00:03,  5.57it/s]80/99 7.3G 0.08139 0.06969 0.02369 201 640:  11%|█         | 2/19 [00:00<00:03,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.08089 0.05728 0.02254 58 640:  11%|█         | 2/19 [00:00<00:03,  5.65it/s] 80/99 7.3G 0.08089 0.05728 0.02254 58 640:  16%|█▌        | 3/19 [00:00<00:02,  5.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07886 0.0514 0.02239 55 640:  16%|█▌        | 3/19 [00:00<00:02,  5.40it/s] 80/99 7.3G 0.07886 0.0514 0.02239 55 640:  21%|██        | 4/19 [00:00<00:03,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07848 0.05293 0.02216 106 640:  21%|██        | 4/19 [00:01<00:03,  4.14it/s]80/99 7.3G 0.07848 0.05293 0.02216 106 640:  26%|██▋       | 5/19 [00:01<00:03,  4.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07755 0.04945 0.02249 49 640:  26%|██▋       | 5/19 [00:01<00:03,  4.61it/s] 80/99 7.3G 0.07755 0.04945 0.02249 49 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07862 0.05123 0.02164 132 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]80/99 7.3G 0.07862 0.05123 0.02164 132 640:  37%|███▋      | 7/19 [00:01<00:02,  4.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07884 0.05037 0.02214 75 640:  37%|███▋      | 7/19 [00:01<00:02,  4.65it/s] 80/99 7.3G 0.07884 0.05037 0.02214 75 640:  42%|████▏     | 8/19 [00:01<00:02,  4.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07856 0.05026 0.02177 87 640:  42%|████▏     | 8/19 [00:02<00:02,  4.13it/s]80/99 7.3G 0.07856 0.05026 0.02177 87 640:  47%|████▋     | 9/19 [00:02<00:02,  3.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07856 0.04909 0.02132 77 640:  47%|████▋     | 9/19 [00:02<00:02,  3.95it/s]80/99 7.3G 0.07856 0.04909 0.02132 77 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07862 0.0477 0.02116 61 640:  53%|█████▎    | 10/19 [00:05<00:02,  3.42it/s] 80/99 7.3G 0.07862 0.0477 0.02116 61 640:  58%|█████▊    | 11/19 [00:05<00:09,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07855 0.04813 0.02106 100 640:  58%|█████▊    | 11/19 [00:05<00:09,  1.19s/it]80/99 7.3G 0.07855 0.04813 0.02106 100 640:  63%|██████▎   | 12/19 [00:05<00:06,  1.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07929 0.04859 0.02113 123 640:  63%|██████▎   | 12/19 [00:11<00:06,  1.10it/s]80/99 7.3G 0.07929 0.04859 0.02113 123 640:  68%|██████▊   | 13/19 [00:11<00:13,  2.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07927 0.04775 0.02105 71 640:  68%|██████▊   | 13/19 [00:11<00:13,  2.22s/it] 80/99 7.3G 0.07927 0.04775 0.02105 71 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07927 0.04759 0.02091 77 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.60s/it]80/99 7.3G 0.07927 0.04759 0.02091 77 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07957 0.04817 0.02112 96 640:  79%|███████▉  | 15/19 [00:12<00:04,  1.17s/it]80/99 7.3G 0.07957 0.04817 0.02112 96 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07941 0.04733 0.02132 53 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.08s/it]80/99 7.3G 0.07941 0.04733 0.02132 53 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07937 0.04634 0.02105 59 640:  89%|████████▉ | 17/19 [00:15<00:02,  1.23s/it]80/99 7.3G 0.07937 0.04634 0.02105 59 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
80/99 7.3G 0.07918 0.04561 0.02141 48 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.37s/it]80/99 7.3G 0.07918 0.04561 0.02141 48 640: 100%|██████████| 19/19 [00:18<00:00,  1.83s/it]80/99 7.3G 0.07918 0.04561 0.02141 48 640: 100%|██████████| 19/19 [00:18<00:00,  1.03it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.12s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
                   all         55        256      0.178      0.204     0.0996     0.0294
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07575 0.03228 0.01567 57 640:   0%|          | 0/19 [00:00<?, ?it/s]81/99 7.3G 0.07575 0.03228 0.01567 57 640:   5%|▌         | 1/19 [00:00<00:03,  5.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07273 0.03267 0.01586 54 640:   5%|▌         | 1/19 [00:00<00:03,  5.01it/s]81/99 7.3G 0.07273 0.03267 0.01586 54 640:  11%|█         | 2/19 [00:00<00:03,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07533 0.03821 0.01846 87 640:  11%|█         | 2/19 [00:00<00:03,  4.55it/s]81/99 7.3G 0.07533 0.03821 0.01846 87 640:  16%|█▌        | 3/19 [00:00<00:03,  4.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07711 0.04325 0.01986 104 640:  16%|█▌        | 3/19 [00:00<00:03,  4.53it/s]81/99 7.3G 0.07711 0.04325 0.01986 104 640:  21%|██        | 4/19 [00:00<00:03,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07781 0.04623 0.01975 108 640:  21%|██        | 4/19 [00:01<00:03,  4.88it/s]81/99 7.3G 0.07781 0.04623 0.01975 108 640:  26%|██▋       | 5/19 [00:01<00:02,  5.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.0791 0.04916 0.01953 141 640:  26%|██▋       | 5/19 [00:01<00:02,  5.17it/s] 81/99 7.3G 0.0791 0.04916 0.01953 141 640:  32%|███▏      | 6/19 [00:01<00:03,  4.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07904 0.04602 0.02018 49 640:  32%|███▏      | 6/19 [00:01<00:03,  4.05it/s]81/99 7.3G 0.07904 0.04602 0.02018 49 640:  37%|███▋      | 7/19 [00:01<00:02,  4.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07924 0.04626 0.02071 80 640:  37%|███▋      | 7/19 [00:01<00:02,  4.47it/s]81/99 7.3G 0.07924 0.04626 0.02071 80 640:  42%|████▏     | 8/19 [00:01<00:02,  4.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07967 0.04628 0.02061 87 640:  42%|████▏     | 8/19 [00:01<00:02,  4.80it/s]81/99 7.3G 0.07967 0.04628 0.02061 87 640:  47%|████▋     | 9/19 [00:01<00:01,  5.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07987 0.04527 0.02058 67 640:  47%|████▋     | 9/19 [00:03<00:01,  5.07it/s]81/99 7.3G 0.07987 0.04527 0.02058 67 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07945 0.0451 0.02059 72 640:  53%|█████▎    | 10/19 [00:03<00:04,  2.04it/s] 81/99 7.3G 0.07945 0.0451 0.02059 72 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07983 0.04571 0.0205 89 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.54it/s]81/99 7.3G 0.07983 0.04571 0.0205 89 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07994 0.04604 0.02034 96 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.96it/s]81/99 7.3G 0.07994 0.04604 0.02034 96 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07956 0.04647 0.02055 77 640:  68%|██████▊   | 13/19 [00:06<00:04,  1.37it/s]81/99 7.3G 0.07956 0.04647 0.02055 77 640:  74%|███████▎  | 14/19 [00:06<00:05,  1.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07942 0.0458 0.0206 63 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.04s/it]  81/99 7.3G 0.07942 0.0458 0.0206 63 640:  79%|███████▉  | 15/19 [00:08<00:05,  1.34s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07896 0.04537 0.02049 64 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.34s/it]81/99 7.3G 0.07896 0.04537 0.02049 64 640:  84%|████████▍ | 16/19 [00:12<00:05,  1.98s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07861 0.04474 0.02041 55 640:  84%|████████▍ | 16/19 [00:13<00:05,  1.98s/it]81/99 7.3G 0.07861 0.04474 0.02041 55 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.0785 0.04462 0.02052 68 640:  89%|████████▉ | 17/19 [00:18<00:03,  1.61s/it] 81/99 7.3G 0.0785 0.04462 0.02052 68 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
81/99 7.3G 0.07811 0.04396 0.02074 53 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.66s/it]81/99 7.3G 0.07811 0.04396 0.02074 53 640: 100%|██████████| 19/19 [00:18<00:00,  1.97s/it]81/99 7.3G 0.07811 0.04396 0.02074 53 640: 100%|██████████| 19/19 [00:18<00:00,  1.03it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.91s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
                   all         55        256      0.202      0.206      0.114     0.0359
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.08001 0.04269 0.01673 78 640:   0%|          | 0/19 [00:00<?, ?it/s]82/99 7.3G 0.08001 0.04269 0.01673 78 640:   5%|▌         | 1/19 [00:00<00:07,  2.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07557 0.04038 0.01823 64 640:   5%|▌         | 1/19 [00:00<00:07,  2.49it/s]82/99 7.3G 0.07557 0.04038 0.01823 64 640:  11%|█         | 2/19 [00:00<00:07,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07657 0.04641 0.01746 110 640:  11%|█         | 2/19 [00:01<00:07,  2.28it/s]82/99 7.3G 0.07657 0.04641 0.01746 110 640:  16%|█▌        | 3/19 [00:01<00:07,  2.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07624 0.04606 0.01862 76 640:  16%|█▌        | 3/19 [00:01<00:07,  2.24it/s] 82/99 7.3G 0.07624 0.04606 0.01862 76 640:  21%|██        | 4/19 [00:01<00:07,  2.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07583 0.04657 0.0194 80 640:  21%|██        | 4/19 [00:02<00:07,  2.10it/s] 82/99 7.3G 0.07583 0.04657 0.0194 80 640:  26%|██▋       | 5/19 [00:02<00:07,  1.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07516 0.0457 0.01925 65 640:  26%|██▋       | 5/19 [00:02<00:07,  1.99it/s]82/99 7.3G 0.07516 0.0457 0.01925 65 640:  32%|███▏      | 6/19 [00:02<00:06,  2.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07595 0.04479 0.01924 79 640:  32%|███▏      | 6/19 [00:03<00:06,  2.07it/s]82/99 7.3G 0.07595 0.04479 0.01924 79 640:  37%|███▋      | 7/19 [00:03<00:05,  2.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07605 0.04416 0.01895 70 640:  37%|███▋      | 7/19 [00:03<00:05,  2.12it/s]82/99 7.3G 0.07605 0.04416 0.01895 70 640:  42%|████▏     | 8/19 [00:03<00:04,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07644 0.04359 0.01973 64 640:  42%|████▏     | 8/19 [00:04<00:04,  2.28it/s]82/99 7.3G 0.07644 0.04359 0.01973 64 640:  47%|████▋     | 9/19 [00:04<00:04,  2.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07638 0.04426 0.01958 91 640:  47%|████▋     | 9/19 [00:04<00:04,  2.42it/s]82/99 7.3G 0.07638 0.04426 0.01958 91 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07516 0.0425 0.01936 36 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.50it/s] 82/99 7.3G 0.07516 0.0425 0.01936 36 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07571 0.04329 0.01919 106 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.64it/s]82/99 7.3G 0.07571 0.04329 0.01919 106 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07621 0.04573 0.01952 137 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.15it/s]82/99 7.3G 0.07621 0.04573 0.01952 137 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.0759 0.04504 0.01943 62 640:  68%|██████▊   | 13/19 [00:06<00:03,  1.81it/s]  82/99 7.3G 0.0759 0.04504 0.01943 62 640:  74%|███████▎  | 14/19 [00:06<00:02,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.076 0.04494 0.01973 74 640:  74%|███████▎  | 14/19 [00:12<00:02,  2.28it/s] 82/99 7.3G 0.076 0.04494 0.01973 74 640:  79%|███████▉  | 15/19 [00:12<00:08,  2.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07606 0.04515 0.01993 86 640:  79%|███████▉  | 15/19 [00:12<00:08,  2.20s/it]82/99 7.3G 0.07606 0.04515 0.01993 86 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07588 0.04534 0.01986 77 640:  84%|████████▍ | 16/19 [00:13<00:04,  1.59s/it]82/99 7.3G 0.07588 0.04534 0.01986 77 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07583 0.04539 0.0197 80 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.39s/it] 82/99 7.3G 0.07583 0.04539 0.0197 80 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
82/99 7.3G 0.07603 0.04485 0.01958 67 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.37s/it]82/99 7.3G 0.07603 0.04485 0.01958 67 640: 100%|██████████| 19/19 [00:15<00:00,  1.08s/it]82/99 7.3G 0.07603 0.04485 0.01958 67 640: 100%|██████████| 19/19 [00:15<00:00,  1.24it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.01s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.95s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]
                   all         55        256      0.136      0.208      0.118     0.0363
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07889 0.05919 0.02374 99 640:   0%|          | 0/19 [00:00<?, ?it/s]83/99 7.3G 0.07889 0.05919 0.02374 99 640:   5%|▌         | 1/19 [00:00<00:03,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07918 0.04752 0.0207 70 640:   5%|▌         | 1/19 [00:00<00:03,  5.50it/s] 83/99 7.3G 0.07918 0.04752 0.0207 70 640:  11%|█         | 2/19 [00:00<00:03,  5.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07902 0.04605 0.02065 80 640:  11%|█         | 2/19 [00:00<00:03,  5.48it/s]83/99 7.3G 0.07902 0.04605 0.02065 80 640:  16%|█▌        | 3/19 [00:00<00:02,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.08005 0.0449 0.02036 90 640:  16%|█▌        | 3/19 [00:00<00:02,  5.51it/s] 83/99 7.3G 0.08005 0.0449 0.02036 90 640:  21%|██        | 4/19 [00:00<00:02,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.0793 0.04582 0.01978 91 640:  21%|██        | 4/19 [00:00<00:02,  5.53it/s]83/99 7.3G 0.0793 0.04582 0.01978 91 640:  26%|██▋       | 5/19 [00:00<00:02,  5.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07833 0.04376 0.02076 56 640:  26%|██▋       | 5/19 [00:01<00:02,  5.20it/s]83/99 7.3G 0.07833 0.04376 0.02076 56 640:  32%|███▏      | 6/19 [00:01<00:03,  4.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07813 0.04539 0.02161 91 640:  32%|███▏      | 6/19 [00:01<00:03,  4.32it/s]83/99 7.3G 0.07813 0.04539 0.02161 91 640:  37%|███▋      | 7/19 [00:01<00:02,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07758 0.04595 0.02132 88 640:  37%|███▋      | 7/19 [00:01<00:02,  4.56it/s]83/99 7.3G 0.07758 0.04595 0.02132 88 640:  42%|████▏     | 8/19 [00:01<00:02,  4.84it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07666 0.04394 0.02113 43 640:  42%|████▏     | 8/19 [00:01<00:02,  4.84it/s]83/99 7.3G 0.07666 0.04394 0.02113 43 640:  47%|████▋     | 9/19 [00:01<00:02,  4.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07783 0.04681 0.0213 149 640:  47%|████▋     | 9/19 [00:03<00:02,  4.96it/s]83/99 7.3G 0.07783 0.04681 0.0213 149 640:  53%|█████▎    | 10/19 [00:03<00:05,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07836 0.04715 0.02107 99 640:  53%|█████▎    | 10/19 [00:03<00:05,  1.73it/s]83/99 7.3G 0.07836 0.04715 0.02107 99 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.0781 0.04552 0.02059 44 640:  58%|█████▊    | 11/19 [00:07<00:04,  1.95it/s] 83/99 7.3G 0.0781 0.04552 0.02059 44 640:  63%|██████▎   | 12/19 [00:07<00:11,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07825 0.04527 0.02045 79 640:  63%|██████▎   | 12/19 [00:08<00:11,  1.61s/it]83/99 7.3G 0.07825 0.04527 0.02045 79 640:  68%|██████▊   | 13/19 [00:08<00:07,  1.26s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07867 0.04813 0.02053 167 640:  68%|██████▊   | 13/19 [00:08<00:07,  1.26s/it]83/99 7.3G 0.07867 0.04813 0.02053 167 640:  74%|███████▎  | 14/19 [00:08<00:04,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.0786 0.04779 0.02061 81 640:  74%|███████▎  | 14/19 [00:09<00:04,  1.05it/s]  83/99 7.3G 0.0786 0.04779 0.02061 81 640:  79%|███████▉  | 15/19 [00:09<00:03,  1.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07918 0.0473 0.02057 104 640:  79%|███████▉  | 15/19 [00:10<00:03,  1.12it/s]83/99 7.3G 0.07918 0.0473 0.02057 104 640:  84%|████████▍ | 16/19 [00:10<00:02,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07948 0.04804 0.02066 132 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.08it/s]83/99 7.3G 0.07948 0.04804 0.02066 132 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07961 0.04858 0.02105 84 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.64s/it] 83/99 7.3G 0.07961 0.04858 0.02105 84 640:  95%|█████████▍| 18/19 [00:16<00:02,  2.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
83/99 7.3G 0.07919 0.04792 0.02109 58 640:  95%|█████████▍| 18/19 [00:16<00:02,  2.02s/it]83/99 7.3G 0.07919 0.04792 0.02109 58 640: 100%|██████████| 19/19 [00:16<00:00,  1.56s/it]83/99 7.3G 0.07919 0.04792 0.02109 58 640: 100%|██████████| 19/19 [00:16<00:00,  1.13it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.59s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.60s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]
                   all         55        256      0.162      0.227      0.133     0.0387
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07584 0.03905 0.01995 66 640:   0%|          | 0/19 [00:00<?, ?it/s]84/99 7.3G 0.07584 0.03905 0.01995 66 640:   5%|▌         | 1/19 [00:00<00:03,  4.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07767 0.04735 0.0211 100 640:   5%|▌         | 1/19 [00:00<00:03,  4.75it/s]84/99 7.3G 0.07767 0.04735 0.0211 100 640:  11%|█         | 2/19 [00:00<00:04,  3.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07846 0.04953 0.02124 94 640:  11%|█         | 2/19 [00:01<00:04,  3.44it/s]84/99 7.3G 0.07846 0.04953 0.02124 94 640:  16%|█▌        | 3/19 [00:01<00:06,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07721 0.04669 0.0232 60 640:  16%|█▌        | 3/19 [00:01<00:06,  2.33it/s] 84/99 7.3G 0.07721 0.04669 0.0232 60 640:  21%|██        | 4/19 [00:01<00:06,  2.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07625 0.04194 0.0223 36 640:  21%|██        | 4/19 [00:02<00:06,  2.34it/s]84/99 7.3G 0.07625 0.04194 0.0223 36 640:  26%|██▋       | 5/19 [00:02<00:06,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07467 0.04013 0.02133 49 640:  26%|██▋       | 5/19 [00:02<00:06,  2.29it/s]84/99 7.3G 0.07467 0.04013 0.02133 49 640:  32%|███▏      | 6/19 [00:02<00:05,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07508 0.03832 0.02179 46 640:  32%|███▏      | 6/19 [00:02<00:05,  2.28it/s]84/99 7.3G 0.07508 0.03832 0.02179 46 640:  37%|███▋      | 7/19 [00:02<00:05,  2.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0762 0.04286 0.02203 124 640:  37%|███▋      | 7/19 [00:03<00:05,  2.39it/s]84/99 7.3G 0.0762 0.04286 0.02203 124 640:  42%|████▏     | 8/19 [00:03<00:04,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07601 0.04296 0.02138 75 640:  42%|████▏     | 8/19 [00:03<00:04,  2.26it/s]84/99 7.3G 0.07601 0.04296 0.02138 75 640:  47%|████▋     | 9/19 [00:03<00:04,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07558 0.04234 0.02122 53 640:  47%|████▋     | 9/19 [00:04<00:04,  2.38it/s]84/99 7.3G 0.07558 0.04234 0.02122 53 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07658 0.04594 0.02132 171 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.35it/s]84/99 7.3G 0.07658 0.04594 0.02132 171 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07689 0.04692 0.02115 106 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.43it/s]84/99 7.3G 0.07689 0.04692 0.02115 106 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07686 0.04696 0.02123 86 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.50it/s] 84/99 7.3G 0.07686 0.04696 0.02123 86 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07697 0.04641 0.02115 71 640:  68%|██████▊   | 13/19 [00:07<00:02,  2.93it/s]84/99 7.3G 0.07697 0.04641 0.02115 71 640:  74%|███████▎  | 14/19 [00:07<00:05,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0771 0.04702 0.02125 97 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.10s/it] 84/99 7.3G 0.0771 0.04702 0.02125 97 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07692 0.0465 0.02094 66 640:  79%|███████▉  | 15/19 [00:10<00:03,  1.00it/s]84/99 7.3G 0.07692 0.0465 0.02094 66 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.07737 0.04675 0.02076 114 640:  84%|████████▍ | 16/19 [00:17<00:03,  1.10s/it]84/99 7.3G 0.07737 0.04675 0.02076 114 640:  89%|████████▉ | 17/19 [00:17<00:06,  3.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0775 0.04669 0.02079 80 640:  89%|████████▉ | 17/19 [00:18<00:06,  3.11s/it]  84/99 7.3G 0.0775 0.04669 0.02079 80 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
84/99 7.3G 0.0775 0.0459 0.02074 52 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.23s/it] 84/99 7.3G 0.0775 0.0459 0.02074 52 640: 100%|██████████| 19/19 [00:18<00:00,  1.61s/it]84/99 7.3G 0.0775 0.0459 0.02074 52 640: 100%|██████████| 19/19 [00:18<00:00,  1.04it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.25s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]
                   all         55        256      0.167      0.233      0.132     0.0362
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.08511 0.05424 0.02468 105 640:   0%|          | 0/19 [00:00<?, ?it/s]85/99 7.3G 0.08511 0.05424 0.02468 105 640:   5%|▌         | 1/19 [00:00<00:04,  4.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.08266 0.06265 0.02283 137 640:   5%|▌         | 1/19 [00:00<00:04,  4.45it/s]85/99 7.3G 0.08266 0.06265 0.02283 137 640:  11%|█         | 2/19 [00:00<00:03,  5.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.08248 0.06254 0.02213 129 640:  11%|█         | 2/19 [00:00<00:03,  5.13it/s]85/99 7.3G 0.08248 0.06254 0.02213 129 640:  16%|█▌        | 3/19 [00:00<00:02,  5.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.08141 0.05629 0.02161 63 640:  16%|█▌        | 3/19 [00:00<00:02,  5.39it/s] 85/99 7.3G 0.08141 0.05629 0.02161 63 640:  21%|██        | 4/19 [00:00<00:02,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.08013 0.05223 0.02083 61 640:  21%|██        | 4/19 [00:00<00:02,  5.53it/s]85/99 7.3G 0.08013 0.05223 0.02083 61 640:  26%|██▋       | 5/19 [00:00<00:02,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0786 0.05128 0.02116 71 640:  26%|██▋       | 5/19 [00:01<00:02,  5.61it/s] 85/99 7.3G 0.0786 0.05128 0.02116 71 640:  32%|███▏      | 6/19 [00:01<00:02,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07784 0.04746 0.02177 37 640:  32%|███▏      | 6/19 [00:01<00:02,  5.64it/s]85/99 7.3G 0.07784 0.04746 0.02177 37 640:  37%|███▋      | 7/19 [00:01<00:02,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07778 0.04739 0.02168 79 640:  37%|███▋      | 7/19 [00:01<00:02,  5.68it/s]85/99 7.3G 0.07778 0.04739 0.02168 79 640:  42%|████▏     | 8/19 [00:01<00:02,  4.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07854 0.04881 0.02169 114 640:  42%|████▏     | 8/19 [00:01<00:02,  4.77it/s]85/99 7.3G 0.07854 0.04881 0.02169 114 640:  47%|████▋     | 9/19 [00:01<00:02,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07924 0.05353 0.02143 190 640:  47%|████▋     | 9/19 [00:01<00:02,  4.56it/s]85/99 7.3G 0.07924 0.05353 0.02143 190 640:  53%|█████▎    | 10/19 [00:01<00:01,  4.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0786 0.05235 0.02141 68 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.86it/s]  85/99 7.3G 0.0786 0.05235 0.02141 68 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07858 0.05071 0.02149 56 640:  58%|█████▊    | 11/19 [00:04<00:04,  1.64it/s]85/99 7.3G 0.07858 0.05071 0.02149 56 640:  63%|██████▎   | 12/19 [00:04<00:06,  1.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07822 0.04917 0.02121 52 640:  63%|██████▎   | 12/19 [00:05<00:06,  1.14it/s]85/99 7.3G 0.07822 0.04917 0.02121 52 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07775 0.04869 0.02126 63 640:  68%|██████▊   | 13/19 [00:12<00:04,  1.37it/s]85/99 7.3G 0.07775 0.04869 0.02126 63 640:  74%|███████▎  | 14/19 [00:12<00:13,  2.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07777 0.04945 0.02126 106 640:  74%|███████▎  | 14/19 [00:12<00:13,  2.69s/it]85/99 7.3G 0.07777 0.04945 0.02126 106 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.93s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07779 0.04911 0.02113 82 640:  79%|███████▉  | 15/19 [00:13<00:07,  1.93s/it] 85/99 7.3G 0.07779 0.04911 0.02113 82 640:  84%|████████▍ | 16/19 [00:13<00:04,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.0782 0.05015 0.0212 122 640:  84%|████████▍ | 16/19 [00:14<00:04,  1.59s/it] 85/99 7.3G 0.0782 0.05015 0.0212 122 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07822 0.04902 0.02113 56 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.53s/it]85/99 7.3G 0.07822 0.04902 0.02113 56 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
85/99 7.3G 0.07805 0.04901 0.021 85 640:  95%|█████████▍| 18/19 [00:19<00:01,  1.55s/it]  85/99 7.3G 0.07805 0.04901 0.021 85 640: 100%|██████████| 19/19 [00:19<00:00,  1.84s/it]85/99 7.3G 0.07805 0.04901 0.021 85 640: 100%|██████████| 19/19 [00:19<00:00,  1.00s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.00s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.94s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
                   all         55        256      0.166      0.229      0.137     0.0376
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.08282 0.03098 0.02056 64 640:   0%|          | 0/19 [00:00<?, ?it/s]86/99 7.3G 0.08282 0.03098 0.02056 64 640:   5%|▌         | 1/19 [00:00<00:05,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.08074 0.03969 0.02055 83 640:   5%|▌         | 1/19 [00:00<00:05,  3.16it/s]86/99 7.3G 0.08074 0.03969 0.02055 83 640:  11%|█         | 2/19 [00:00<00:05,  2.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07751 0.03745 0.01978 56 640:  11%|█         | 2/19 [00:01<00:05,  2.99it/s]86/99 7.3G 0.07751 0.03745 0.01978 56 640:  16%|█▌        | 3/19 [00:01<00:05,  2.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07737 0.03845 0.01976 75 640:  16%|█▌        | 3/19 [00:01<00:05,  2.70it/s]86/99 7.3G 0.07737 0.03845 0.01976 75 640:  21%|██        | 4/19 [00:01<00:05,  2.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07695 0.03913 0.02053 73 640:  21%|██        | 4/19 [00:01<00:05,  2.98it/s]86/99 7.3G 0.07695 0.03913 0.02053 73 640:  26%|██▋       | 5/19 [00:01<00:03,  3.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07739 0.0421 0.02106 94 640:  26%|██▋       | 5/19 [00:01<00:03,  3.61it/s] 86/99 7.3G 0.07739 0.0421 0.02106 94 640:  32%|███▏      | 6/19 [00:01<00:03,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.0776 0.04244 0.0214 81 640:  32%|███▏      | 6/19 [00:02<00:03,  3.97it/s] 86/99 7.3G 0.0776 0.04244 0.0214 81 640:  37%|███▋      | 7/19 [00:02<00:03,  3.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07789 0.04274 0.0213 73 640:  37%|███▋      | 7/19 [00:02<00:03,  3.60it/s]86/99 7.3G 0.07789 0.04274 0.0213 73 640:  42%|████▏     | 8/19 [00:02<00:02,  4.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07808 0.04521 0.0214 109 640:  42%|████▏     | 8/19 [00:02<00:02,  4.04it/s]86/99 7.3G 0.07808 0.04521 0.0214 109 640:  47%|████▋     | 9/19 [00:02<00:02,  3.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07806 0.04494 0.02109 82 640:  47%|████▋     | 9/19 [00:03<00:02,  3.59it/s]86/99 7.3G 0.07806 0.04494 0.02109 82 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07827 0.04532 0.02119 90 640:  53%|█████▎    | 10/19 [00:05<00:02,  3.14it/s]86/99 7.3G 0.07827 0.04532 0.02119 90 640:  58%|█████▊    | 11/19 [00:05<00:08,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07824 0.04469 0.02146 69 640:  58%|█████▊    | 11/19 [00:05<00:08,  1.01s/it]86/99 7.3G 0.07824 0.04469 0.02146 69 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07822 0.04572 0.02141 105 640:  63%|██████▎   | 12/19 [00:06<00:05,  1.33it/s]86/99 7.3G 0.07822 0.04572 0.02141 105 640:  68%|██████▊   | 13/19 [00:06<00:03,  1.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07856 0.04514 0.02128 70 640:  68%|██████▊   | 13/19 [00:07<00:03,  1.52it/s] 86/99 7.3G 0.07856 0.04514 0.02128 70 640:  74%|███████▎  | 14/19 [00:07<00:04,  1.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07846 0.04407 0.02118 51 640:  74%|███████▎  | 14/19 [00:08<00:04,  1.22it/s]86/99 7.3G 0.07846 0.04407 0.02118 51 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07829 0.04399 0.0213 71 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.00s/it] 86/99 7.3G 0.07829 0.04399 0.0213 71 640:  84%|████████▍ | 16/19 [00:11<00:04,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07905 0.04448 0.0211 140 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.38s/it]86/99 7.3G 0.07905 0.04448 0.0211 140 640:  89%|████████▉ | 17/19 [00:12<00:03,  1.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.07911 0.04431 0.0211 78 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.51s/it] 86/99 7.3G 0.07911 0.04431 0.0211 78 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.11s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
86/99 7.3G 0.0788 0.04381 0.02083 68 640:  95%|█████████▍| 18/19 [00:21<00:01,  1.11s/it]86/99 7.3G 0.0788 0.04381 0.02083 68 640: 100%|██████████| 19/19 [00:21<00:00,  3.35s/it]86/99 7.3G 0.0788 0.04381 0.02083 68 640: 100%|██████████| 19/19 [00:21<00:00,  1.14s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.00s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.83s/it]
                   all         55        256      0.144      0.225      0.125     0.0369
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07954 0.02556 0.01931 41 640:   0%|          | 0/19 [00:00<?, ?it/s]87/99 7.3G 0.07954 0.02556 0.01931 41 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07884 0.03412 0.01715 85 640:   5%|▌         | 1/19 [00:00<00:03,  5.65it/s]87/99 7.3G 0.07884 0.03412 0.01715 85 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07799 0.04122 0.0189 93 640:  11%|█         | 2/19 [00:00<00:02,  5.69it/s] 87/99 7.3G 0.07799 0.04122 0.0189 93 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07981 0.04334 0.01944 111 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]87/99 7.3G 0.07981 0.04334 0.01944 111 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.08 0.04423 0.02028 76 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]    87/99 7.3G 0.08 0.04423 0.02028 76 640:  26%|██▋       | 5/19 [00:00<00:02,  5.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07907 0.04327 0.02 70 640:  26%|██▋       | 5/19 [00:01<00:02,  5.75it/s]87/99 7.3G 0.07907 0.04327 0.02 70 640:  32%|███▏      | 6/19 [00:01<00:02,  5.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07962 0.04529 0.02042 101 640:  32%|███▏      | 6/19 [00:01<00:02,  5.76it/s]87/99 7.3G 0.07962 0.04529 0.02042 101 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07909 0.04466 0.02064 69 640:  37%|███▋      | 7/19 [00:01<00:02,  5.74it/s] 87/99 7.3G 0.07909 0.04466 0.02064 69 640:  42%|████▏     | 8/19 [00:01<00:01,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07791 0.04331 0.0208 47 640:  42%|████▏     | 8/19 [00:01<00:01,  5.73it/s] 87/99 7.3G 0.07791 0.04331 0.0208 47 640:  47%|████▋     | 9/19 [00:01<00:01,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07765 0.04175 0.02082 41 640:  47%|████▋     | 9/19 [00:01<00:01,  5.74it/s]87/99 7.3G 0.07765 0.04175 0.02082 41 640:  53%|█████▎    | 10/19 [00:01<00:01,  4.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07824 0.04173 0.02071 94 640:  53%|█████▎    | 10/19 [00:02<00:01,  4.51it/s]87/99 7.3G 0.07824 0.04173 0.02071 94 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07835 0.04234 0.02058 89 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.34it/s]87/99 7.3G 0.07835 0.04234 0.02058 89 640:  63%|██████▎   | 12/19 [00:02<00:02,  2.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07745 0.04081 0.02047 32 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.89it/s]87/99 7.3G 0.07745 0.04081 0.02047 32 640:  68%|██████▊   | 13/19 [00:03<00:02,  2.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07738 0.04031 0.02083 63 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.82it/s]87/99 7.3G 0.07738 0.04031 0.02083 63 640:  74%|███████▎  | 14/19 [00:04<00:03,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07702 0.03964 0.02073 50 640:  74%|███████▎  | 14/19 [00:04<00:03,  1.66it/s]87/99 7.3G 0.07702 0.03964 0.02073 50 640:  79%|███████▉  | 15/19 [00:04<00:01,  2.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07694 0.03968 0.0207 66 640:  79%|███████▉  | 15/19 [00:12<00:01,  2.11it/s] 87/99 7.3G 0.07694 0.03968 0.0207 66 640:  84%|████████▍ | 16/19 [00:12<00:08,  2.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07695 0.03987 0.02097 71 640:  84%|████████▍ | 16/19 [00:16<00:08,  2.87s/it]87/99 7.3G 0.07695 0.03987 0.02097 71 640:  89%|████████▉ | 17/19 [00:16<00:06,  3.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07696 0.04055 0.0209 97 640:  89%|████████▉ | 17/19 [00:17<00:06,  3.18s/it] 87/99 7.3G 0.07696 0.04055 0.0209 97 640:  95%|█████████▍| 18/19 [00:17<00:02,  2.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
87/99 7.3G 0.07707 0.04074 0.02094 77 640:  95%|█████████▍| 18/19 [00:17<00:02,  2.27s/it]87/99 7.3G 0.07707 0.04074 0.02094 77 640: 100%|██████████| 19/19 [00:17<00:00,  1.64s/it]87/99 7.3G 0.07707 0.04074 0.02094 77 640: 100%|██████████| 19/19 [00:17<00:00,  1.10it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.54s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.75s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
                   all         55        256      0.165      0.248      0.137     0.0424
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07955 0.06526 0.02069 116 640:   0%|          | 0/19 [00:00<?, ?it/s]88/99 7.3G 0.07955 0.06526 0.02069 116 640:   5%|▌         | 1/19 [00:00<00:05,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.08314 0.05231 0.0206 85 640:   5%|▌         | 1/19 [00:00<00:05,  3.16it/s]  88/99 7.3G 0.08314 0.05231 0.0206 85 640:  11%|█         | 2/19 [00:00<00:04,  3.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.08381 0.04689 0.01926 76 640:  11%|█         | 2/19 [00:00<00:04,  3.51it/s]88/99 7.3G 0.08381 0.04689 0.01926 76 640:  16%|█▌        | 3/19 [00:00<00:04,  3.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.08117 0.04305 0.01889 52 640:  16%|█▌        | 3/19 [00:01<00:04,  3.68it/s]88/99 7.3G 0.08117 0.04305 0.01889 52 640:  21%|██        | 4/19 [00:01<00:03,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.08026 0.04283 0.01886 84 640:  21%|██        | 4/19 [00:01<00:03,  4.27it/s]88/99 7.3G 0.08026 0.04283 0.01886 84 640:  26%|██▋       | 5/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07968 0.04201 0.01947 60 640:  26%|██▋       | 5/19 [00:01<00:02,  4.71it/s]88/99 7.3G 0.07968 0.04201 0.01947 60 640:  32%|███▏      | 6/19 [00:01<00:02,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07944 0.04255 0.01947 84 640:  32%|███▏      | 6/19 [00:01<00:02,  5.02it/s]88/99 7.3G 0.07944 0.04255 0.01947 84 640:  37%|███▋      | 7/19 [00:01<00:02,  5.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.0787 0.04073 0.01921 47 640:  37%|███▋      | 7/19 [00:01<00:02,  5.25it/s] 88/99 7.3G 0.0787 0.04073 0.01921 47 640:  42%|████▏     | 8/19 [00:01<00:02,  5.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07788 0.04033 0.01934 62 640:  42%|████▏     | 8/19 [00:01<00:02,  5.40it/s]88/99 7.3G 0.07788 0.04033 0.01934 62 640:  47%|████▋     | 9/19 [00:01<00:01,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07801 0.04141 0.02012 87 640:  47%|████▋     | 9/19 [00:02<00:01,  5.51it/s]88/99 7.3G 0.07801 0.04141 0.02012 87 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07823 0.04407 0.02009 122 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.59it/s]88/99 7.3G 0.07823 0.04407 0.02009 122 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07846 0.04496 0.0202 98 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.50it/s]  88/99 7.3G 0.07846 0.04496 0.0202 98 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.0784 0.0452 0.02013 89 640:  63%|██████▎   | 12/19 [00:08<00:01,  4.27it/s] 88/99 7.3G 0.0784 0.0452 0.02013 89 640:  68%|██████▊   | 13/19 [00:08<00:12,  2.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.078 0.04507 0.02005 69 640:  68%|██████▊   | 13/19 [00:11<00:12,  2.06s/it]88/99 7.3G 0.078 0.04507 0.02005 69 640:  74%|███████▎  | 14/19 [00:11<00:10,  2.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07796 0.04445 0.01988 67 640:  74%|███████▎  | 14/19 [00:11<00:10,  2.15s/it]88/99 7.3G 0.07796 0.04445 0.01988 67 640:  79%|███████▉  | 15/19 [00:11<00:06,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07831 0.04496 0.01991 101 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.64s/it]88/99 7.3G 0.07831 0.04496 0.01991 101 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.0782 0.04506 0.0201 75 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.24s/it]   88/99 7.3G 0.0782 0.04506 0.0201 75 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07783 0.04501 0.0202 70 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.31s/it]88/99 7.3G 0.07783 0.04501 0.0202 70 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
88/99 7.3G 0.07774 0.04391 0.0201 42 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.06s/it]88/99 7.3G 0.07774 0.04391 0.0201 42 640: 100%|██████████| 19/19 [00:14<00:00,  1.19it/s]88/99 7.3G 0.07774 0.04391 0.0201 42 640: 100%|██████████| 19/19 [00:14<00:00,  1.33it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.26s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.13s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
                   all         55        256      0.202       0.22      0.135     0.0377
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07965 0.04316 0.02609 68 640:   0%|          | 0/19 [00:00<?, ?it/s]89/99 7.3G 0.07965 0.04316 0.02609 68 640:   5%|▌         | 1/19 [00:00<00:03,  5.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07766 0.03997 0.0226 63 640:   5%|▌         | 1/19 [00:00<00:03,  5.34it/s] 89/99 7.3G 0.07766 0.03997 0.0226 63 640:  11%|█         | 2/19 [00:00<00:03,  5.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07627 0.04378 0.02135 91 640:  11%|█         | 2/19 [00:00<00:03,  5.46it/s]89/99 7.3G 0.07627 0.04378 0.02135 91 640:  16%|█▌        | 3/19 [00:00<00:02,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07781 0.04732 0.02129 112 640:  16%|█▌        | 3/19 [00:00<00:02,  5.50it/s]89/99 7.3G 0.07781 0.04732 0.02129 112 640:  21%|██        | 4/19 [00:00<00:02,  5.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07915 0.04701 0.02128 98 640:  21%|██        | 4/19 [00:01<00:02,  5.52it/s] 89/99 7.3G 0.07915 0.04701 0.02128 98 640:  26%|██▋       | 5/19 [00:01<00:03,  4.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07808 0.04558 0.02076 67 640:  26%|██▋       | 5/19 [00:01<00:03,  4.61it/s]89/99 7.3G 0.07808 0.04558 0.02076 67 640:  32%|███▏      | 6/19 [00:01<00:02,  4.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07702 0.04646 0.02064 87 640:  32%|███▏      | 6/19 [00:01<00:02,  4.49it/s]89/99 7.3G 0.07702 0.04646 0.02064 87 640:  37%|███▋      | 7/19 [00:01<00:02,  4.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07666 0.04607 0.0211 62 640:  37%|███▋      | 7/19 [00:01<00:02,  4.79it/s] 89/99 7.3G 0.07666 0.04607 0.0211 62 640:  42%|████▏     | 8/19 [00:01<00:02,  4.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07622 0.04446 0.0209 54 640:  42%|████▏     | 8/19 [00:01<00:02,  4.97it/s]89/99 7.3G 0.07622 0.04446 0.0209 54 640:  47%|████▋     | 9/19 [00:01<00:01,  5.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07585 0.04325 0.02092 53 640:  47%|████▋     | 9/19 [00:04<00:01,  5.15it/s]89/99 7.3G 0.07585 0.04325 0.02092 53 640:  53%|█████▎    | 10/19 [00:04<00:09,  1.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07651 0.04443 0.02131 108 640:  53%|█████▎    | 10/19 [00:07<00:09,  1.10s/it]89/99 7.3G 0.07651 0.04443 0.02131 108 640:  58%|█████▊    | 11/19 [00:07<00:11,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07739 0.04549 0.02118 137 640:  58%|█████▊    | 11/19 [00:07<00:11,  1.44s/it]89/99 7.3G 0.07739 0.04549 0.02118 137 640:  63%|██████▎   | 12/19 [00:07<00:07,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07782 0.04601 0.02128 100 640:  63%|██████▎   | 12/19 [00:07<00:07,  1.05s/it]89/99 7.3G 0.07782 0.04601 0.02128 100 640:  68%|██████▊   | 13/19 [00:07<00:04,  1.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07787 0.04623 0.02176 74 640:  68%|██████▊   | 13/19 [00:11<00:04,  1.27it/s] 89/99 7.3G 0.07787 0.04623 0.02176 74 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07793 0.04596 0.02178 66 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.71s/it]89/99 7.3G 0.07793 0.04596 0.02178 66 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07819 0.04565 0.02158 90 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.25s/it]89/99 7.3G 0.07819 0.04565 0.02158 90 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07788 0.04521 0.02149 66 640:  84%|████████▍ | 16/19 [00:13<00:02,  1.08it/s]89/99 7.3G 0.07788 0.04521 0.02149 66 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07782 0.04536 0.02179 74 640:  89%|████████▉ | 17/19 [00:22<00:02,  1.33s/it]89/99 7.3G 0.07782 0.04536 0.02179 74 640:  95%|█████████▍| 18/19 [00:22<00:03,  3.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
89/99 7.3G 0.07771 0.0445 0.02139 46 640:  95%|█████████▍| 18/19 [00:24<00:03,  3.37s/it] 89/99 7.3G 0.07771 0.0445 0.02139 46 640: 100%|██████████| 19/19 [00:24<00:00,  3.10s/it]89/99 7.3G 0.07771 0.0445 0.02139 46 640: 100%|██████████| 19/19 [00:24<00:00,  1.29s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.07s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.15s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
                   all         55        256      0.206      0.212      0.134     0.0371
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0896 0.05186 0.02421 117 640:   0%|          | 0/19 [00:00<?, ?it/s]90/99 7.3G 0.0896 0.05186 0.02421 117 640:   5%|▌         | 1/19 [00:00<00:03,  4.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08704 0.07038 0.02574 149 640:   5%|▌         | 1/19 [00:00<00:03,  4.73it/s]90/99 7.3G 0.08704 0.07038 0.02574 149 640:  11%|█         | 2/19 [00:00<00:04,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0851 0.05914 0.02457 62 640:  11%|█         | 2/19 [00:00<00:04,  3.82it/s]  90/99 7.3G 0.0851 0.05914 0.02457 62 640:  16%|█▌        | 3/19 [00:00<00:04,  3.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08548 0.0618 0.02325 173 640:  16%|█▌        | 3/19 [00:01<00:04,  3.39it/s]90/99 7.3G 0.08548 0.0618 0.02325 173 640:  21%|██        | 4/19 [00:01<00:03,  4.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0838 0.05538 0.02205 50 640:  21%|██        | 4/19 [00:01<00:03,  4.05it/s] 90/99 7.3G 0.0838 0.05538 0.02205 50 640:  26%|██▋       | 5/19 [00:01<00:03,  4.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08236 0.05322 0.02246 67 640:  26%|██▋       | 5/19 [00:01<00:03,  4.53it/s]90/99 7.3G 0.08236 0.05322 0.02246 67 640:  32%|███▏      | 6/19 [00:01<00:02,  4.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08169 0.05136 0.0223 68 640:  32%|███▏      | 6/19 [00:01<00:02,  4.86it/s] 90/99 7.3G 0.08169 0.05136 0.0223 68 640:  37%|███▋      | 7/19 [00:01<00:02,  5.12it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08132 0.04958 0.02152 76 640:  37%|███▋      | 7/19 [00:01<00:02,  5.12it/s]90/99 7.3G 0.08132 0.04958 0.02152 76 640:  42%|████▏     | 8/19 [00:01<00:02,  5.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0804 0.04729 0.02095 50 640:  42%|████▏     | 8/19 [00:01<00:02,  5.31it/s] 90/99 7.3G 0.0804 0.04729 0.02095 50 640:  47%|████▋     | 9/19 [00:01<00:01,  5.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.08047 0.04819 0.0209 105 640:  47%|████▋     | 9/19 [00:02<00:01,  5.44it/s]90/99 7.3G 0.08047 0.04819 0.0209 105 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07985 0.04766 0.0212 67 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.54it/s] 90/99 7.3G 0.07985 0.04766 0.0212 67 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07967 0.04795 0.02126 90 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.61it/s]90/99 7.3G 0.07967 0.04795 0.02126 90 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07899 0.04679 0.02075 50 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.66it/s]90/99 7.3G 0.07899 0.04679 0.02075 50 640:  68%|██████▊   | 13/19 [00:02<00:01,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0786 0.04564 0.02118 46 640:  68%|██████▊   | 13/19 [00:04<00:01,  5.69it/s] 90/99 7.3G 0.0786 0.04564 0.02118 46 640:  74%|███████▎  | 14/19 [00:04<00:03,  1.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07852 0.04529 0.02121 68 640:  74%|███████▎  | 14/19 [00:11<00:03,  1.55it/s]90/99 7.3G 0.07852 0.04529 0.02121 68 640:  79%|███████▉  | 15/19 [00:11<00:10,  2.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.0786 0.04458 0.0211 60 640:  79%|███████▉  | 15/19 [00:15<00:10,  2.51s/it]  90/99 7.3G 0.0786 0.04458 0.0211 60 640:  84%|████████▍ | 16/19 [00:15<00:09,  3.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07799 0.04405 0.02136 58 640:  84%|████████▍ | 16/19 [00:16<00:09,  3.01s/it]90/99 7.3G 0.07799 0.04405 0.02136 58 640:  89%|████████▉ | 17/19 [00:16<00:04,  2.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07808 0.04414 0.02147 85 640:  89%|████████▉ | 17/19 [00:17<00:04,  2.50s/it]90/99 7.3G 0.07808 0.04414 0.02147 85 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
90/99 7.3G 0.07812 0.04422 0.0214 86 640:  95%|█████████▍| 18/19 [00:18<00:01,  1.89s/it] 90/99 7.3G 0.07812 0.04422 0.0214 86 640: 100%|██████████| 19/19 [00:18<00:00,  1.67s/it]90/99 7.3G 0.07812 0.04422 0.0214 86 640: 100%|██████████| 19/19 [00:18<00:00,  1.04it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:04<00:04,  4.27s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]
                   all         55        256      0.217      0.212      0.139     0.0393
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07712 0.04924 0.01926 83 640:   0%|          | 0/19 [00:00<?, ?it/s]91/99 7.3G 0.07712 0.04924 0.01926 83 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07682 0.04251 0.01979 59 640:   5%|▌         | 1/19 [00:00<00:03,  5.68it/s]91/99 7.3G 0.07682 0.04251 0.01979 59 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0758 0.04341 0.02023 71 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s] 91/99 7.3G 0.0758 0.04341 0.02023 71 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07425 0.03958 0.02023 47 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]91/99 7.3G 0.07425 0.03958 0.02023 47 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07496 0.03944 0.02206 62 640:  21%|██        | 4/19 [00:00<00:02,  5.74it/s]91/99 7.3G 0.07496 0.03944 0.02206 62 640:  26%|██▋       | 5/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07446 0.03814 0.02157 51 640:  26%|██▋       | 5/19 [00:01<00:02,  5.74it/s]91/99 7.3G 0.07446 0.03814 0.02157 51 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0744 0.03855 0.02128 73 640:  32%|███▏      | 6/19 [00:01<00:02,  4.95it/s] 91/99 7.3G 0.0744 0.03855 0.02128 73 640:  37%|███▋      | 7/19 [00:01<00:03,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07505 0.03958 0.02217 81 640:  37%|███▋      | 7/19 [00:01<00:03,  3.83it/s]91/99 7.3G 0.07505 0.03958 0.02217 81 640:  42%|████▏     | 8/19 [00:01<00:03,  3.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07539 0.03841 0.02268 48 640:  42%|████▏     | 8/19 [00:02<00:03,  3.21it/s]91/99 7.3G 0.07539 0.03841 0.02268 48 640:  47%|████▋     | 9/19 [00:02<00:02,  3.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07574 0.03892 0.02278 77 640:  47%|████▋     | 9/19 [00:02<00:02,  3.54it/s]91/99 7.3G 0.07574 0.03892 0.02278 77 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0763 0.04034 0.02275 91 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.82it/s] 91/99 7.3G 0.0763 0.04034 0.02275 91 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07626 0.04101 0.02277 76 640:  58%|█████▊    | 11/19 [00:02<00:01,  4.07it/s]91/99 7.3G 0.07626 0.04101 0.02277 76 640:  63%|██████▎   | 12/19 [00:02<00:01,  3.77it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0764 0.04061 0.0228 60 640:  63%|██████▎   | 12/19 [00:07<00:01,  3.77it/s]  91/99 7.3G 0.0764 0.04061 0.0228 60 640:  68%|██████▊   | 13/19 [00:07<00:09,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07617 0.04049 0.02291 65 640:  68%|██████▊   | 13/19 [00:07<00:09,  1.59s/it]91/99 7.3G 0.07617 0.04049 0.02291 65 640:  74%|███████▎  | 14/19 [00:07<00:05,  1.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07626 0.03979 0.02244 55 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.17s/it]91/99 7.3G 0.07626 0.03979 0.02244 55 640:  79%|███████▉  | 15/19 [00:08<00:04,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07692 0.04063 0.02231 114 640:  79%|███████▉  | 15/19 [00:12<00:04,  1.06s/it]91/99 7.3G 0.07692 0.04063 0.02231 114 640:  84%|████████▍ | 16/19 [00:12<00:06,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07723 0.04118 0.02208 101 640:  84%|████████▍ | 16/19 [00:13<00:06,  2.01s/it]91/99 7.3G 0.07723 0.04118 0.02208 101 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.0769 0.04024 0.02226 37 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.64s/it]  91/99 7.3G 0.0769 0.04024 0.02226 37 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.42s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
91/99 7.3G 0.07716 0.04025 0.02207 79 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.42s/it]91/99 7.3G 0.07716 0.04025 0.02207 79 640: 100%|██████████| 19/19 [00:14<00:00,  1.08s/it]91/99 7.3G 0.07716 0.04025 0.02207 79 640: 100%|██████████| 19/19 [00:14<00:00,  1.29it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.60s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]
                   all         55        256      0.227      0.227       0.14     0.0378
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07674 0.0314 0.01785 50 640:   0%|          | 0/19 [00:00<?, ?it/s]92/99 7.3G 0.07674 0.0314 0.01785 50 640:   5%|▌         | 1/19 [00:00<00:07,  2.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.08138 0.04172 0.02222 88 640:   5%|▌         | 1/19 [00:01<00:07,  2.33it/s]92/99 7.3G 0.08138 0.04172 0.02222 88 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.08094 0.03982 0.02187 59 640:  11%|█         | 2/19 [00:01<00:09,  1.87it/s]92/99 7.3G 0.08094 0.03982 0.02187 59 640:  16%|█▌        | 3/19 [00:01<00:09,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.08095 0.0484 0.02328 120 640:  16%|█▌        | 3/19 [00:02<00:09,  1.73it/s]92/99 7.3G 0.08095 0.0484 0.02328 120 640:  21%|██        | 4/19 [00:02<00:08,  1.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.08048 0.04549 0.02333 64 640:  21%|██        | 4/19 [00:02<00:08,  1.73it/s]92/99 7.3G 0.08048 0.04549 0.02333 64 640:  26%|██▋       | 5/19 [00:02<00:06,  2.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.08046 0.04913 0.02331 118 640:  26%|██▋       | 5/19 [00:02<00:06,  2.07it/s]92/99 7.3G 0.08046 0.04913 0.02331 118 640:  32%|███▏      | 6/19 [00:02<00:04,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.0804 0.04928 0.02384 76 640:  32%|███▏      | 6/19 [00:02<00:04,  2.64it/s]  92/99 7.3G 0.0804 0.04928 0.02384 76 640:  37%|███▋      | 7/19 [00:02<00:03,  3.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07996 0.04729 0.02332 60 640:  37%|███▋      | 7/19 [00:03<00:03,  3.20it/s]92/99 7.3G 0.07996 0.04729 0.02332 60 640:  42%|████▏     | 8/19 [00:03<00:02,  3.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07858 0.04545 0.0227 47 640:  42%|████▏     | 8/19 [00:03<00:02,  3.73it/s] 92/99 7.3G 0.07858 0.04545 0.0227 47 640:  47%|████▋     | 9/19 [00:03<00:02,  4.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07733 0.04361 0.02248 42 640:  47%|████▋     | 9/19 [00:04<00:02,  4.19it/s]92/99 7.3G 0.07733 0.04361 0.02248 42 640:  53%|█████▎    | 10/19 [00:04<00:05,  1.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07745 0.04332 0.02198 81 640:  53%|█████▎    | 10/19 [00:04<00:05,  1.60it/s]92/99 7.3G 0.07745 0.04332 0.02198 81 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07694 0.04268 0.02172 56 640:  58%|█████▊    | 11/19 [00:05<00:03,  2.06it/s]92/99 7.3G 0.07694 0.04268 0.02172 56 640:  63%|██████▎   | 12/19 [00:05<00:04,  1.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07677 0.04295 0.0217 76 640:  63%|██████▎   | 12/19 [00:10<00:04,  1.59it/s] 92/99 7.3G 0.07677 0.04295 0.0217 76 640:  68%|██████▊   | 13/19 [00:10<00:11,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07662 0.04227 0.02207 54 640:  68%|██████▊   | 13/19 [00:11<00:11,  1.91s/it]92/99 7.3G 0.07662 0.04227 0.02207 54 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07598 0.04185 0.02204 51 640:  74%|███████▎  | 14/19 [00:12<00:07,  1.46s/it]92/99 7.3G 0.07598 0.04185 0.02204 51 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.57s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07578 0.04087 0.02171 44 640:  79%|███████▉  | 15/19 [00:13<00:06,  1.57s/it]92/99 7.3G 0.07578 0.04087 0.02171 44 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.15s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.0755 0.04041 0.02171 52 640:  84%|████████▍ | 16/19 [00:16<00:03,  1.15s/it] 92/99 7.3G 0.0755 0.04041 0.02171 52 640:  89%|████████▉ | 17/19 [00:16<00:03,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07546 0.04104 0.02185 91 640:  89%|████████▉ | 17/19 [00:20<00:03,  1.89s/it]92/99 7.3G 0.07546 0.04104 0.02185 91 640:  95%|█████████▍| 18/19 [00:20<00:02,  2.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
92/99 7.3G 0.07536 0.04133 0.02197 81 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.52s/it]92/99 7.3G 0.07536 0.04133 0.02197 81 640: 100%|██████████| 19/19 [00:21<00:00,  1.88s/it]92/99 7.3G 0.07536 0.04133 0.02197 81 640: 100%|██████████| 19/19 [00:21<00:00,  1.11s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.90s/it]
                   all         55        256      0.215      0.205      0.115     0.0373
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.0766 0.03539 0.02596 59 640:   0%|          | 0/19 [00:00<?, ?it/s]93/99 7.3G 0.0766 0.03539 0.02596 59 640:   5%|▌         | 1/19 [00:00<00:04,  4.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07891 0.03717 0.02241 74 640:   5%|▌         | 1/19 [00:00<00:04,  4.18it/s]93/99 7.3G 0.07891 0.03717 0.02241 74 640:  11%|█         | 2/19 [00:00<00:03,  4.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07749 0.03933 0.02054 79 640:  11%|█         | 2/19 [00:00<00:03,  4.96it/s]93/99 7.3G 0.07749 0.03933 0.02054 79 640:  16%|█▌        | 3/19 [00:00<00:03,  5.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.0748 0.03776 0.02087 49 640:  16%|█▌        | 3/19 [00:00<00:03,  5.30it/s] 93/99 7.3G 0.0748 0.03776 0.02087 49 640:  21%|██        | 4/19 [00:00<00:02,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07634 0.03928 0.0203 90 640:  21%|██        | 4/19 [00:00<00:02,  5.35it/s]93/99 7.3G 0.07634 0.03928 0.0203 90 640:  26%|██▋       | 5/19 [00:00<00:02,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07666 0.04089 0.02004 90 640:  26%|██▋       | 5/19 [00:01<00:02,  5.50it/s]93/99 7.3G 0.07666 0.04089 0.02004 90 640:  32%|███▏      | 6/19 [00:01<00:02,  4.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.0776 0.04561 0.01985 158 640:  32%|███▏      | 6/19 [00:01<00:02,  4.63it/s]93/99 7.3G 0.0776 0.04561 0.01985 158 640:  37%|███▋      | 7/19 [00:01<00:03,  3.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.0775 0.04459 0.01976 70 640:  37%|███▋      | 7/19 [00:01<00:03,  3.82it/s] 93/99 7.3G 0.0775 0.04459 0.01976 70 640:  42%|████▏     | 8/19 [00:01<00:03,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07711 0.04305 0.02042 53 640:  42%|████▏     | 8/19 [00:02<00:03,  3.46it/s]93/99 7.3G 0.07711 0.04305 0.02042 53 640:  47%|████▋     | 9/19 [00:02<00:03,  3.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07726 0.04439 0.02012 102 640:  47%|████▋     | 9/19 [00:02<00:03,  3.25it/s]93/99 7.3G 0.07726 0.04439 0.02012 102 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07687 0.04478 0.02022 80 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.11it/s] 93/99 7.3G 0.07687 0.04478 0.02022 80 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07729 0.04564 0.02055 89 640:  58%|█████▊    | 11/19 [00:04<00:02,  3.03it/s]93/99 7.3G 0.07729 0.04564 0.02055 89 640:  63%|██████▎   | 12/19 [00:04<00:05,  1.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07724 0.04527 0.02066 70 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.36it/s]93/99 7.3G 0.07724 0.04527 0.02066 70 640:  68%|██████▊   | 13/19 [00:05<00:04,  1.39it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07728 0.04533 0.02061 80 640:  68%|██████▊   | 13/19 [00:08<00:04,  1.39it/s]93/99 7.3G 0.07728 0.04533 0.02061 80 640:  74%|███████▎  | 14/19 [00:08<00:07,  1.58s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07814 0.04624 0.02061 153 640:  74%|███████▎  | 14/19 [00:14<00:07,  1.58s/it]93/99 7.3G 0.07814 0.04624 0.02061 153 640:  79%|███████▉  | 15/19 [00:14<00:11,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07811 0.04528 0.02058 57 640:  79%|███████▉  | 15/19 [00:14<00:11,  2.79s/it] 93/99 7.3G 0.07811 0.04528 0.02058 57 640:  84%|████████▍ | 16/19 [00:14<00:06,  2.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07798 0.04434 0.02032 55 640:  84%|████████▍ | 16/19 [00:15<00:06,  2.06s/it]93/99 7.3G 0.07798 0.04434 0.02032 55 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07754 0.0434 0.02006 46 640:  89%|████████▉ | 17/19 [00:17<00:03,  1.55s/it] 93/99 7.3G 0.07754 0.0434 0.02006 46 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
93/99 7.3G 0.07741 0.04311 0.02022 64 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.68s/it]93/99 7.3G 0.07741 0.04311 0.02022 64 640: 100%|██████████| 19/19 [00:17<00:00,  1.23s/it]93/99 7.3G 0.07741 0.04311 0.02022 64 640: 100%|██████████| 19/19 [00:17<00:00,  1.09it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.31s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.66s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]
                   all         55        256      0.195      0.219      0.144     0.0411
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.06996 0.03466 0.01752 61 640:   0%|          | 0/19 [00:00<?, ?it/s]94/99 7.3G 0.06996 0.03466 0.01752 61 640:   5%|▌         | 1/19 [00:00<00:04,  4.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07264 0.03897 0.02085 71 640:   5%|▌         | 1/19 [00:00<00:04,  4.21it/s]94/99 7.3G 0.07264 0.03897 0.02085 71 640:  11%|█         | 2/19 [00:00<00:03,  4.99it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07344 0.03877 0.02068 67 640:  11%|█         | 2/19 [00:00<00:03,  4.99it/s]94/99 7.3G 0.07344 0.03877 0.02068 67 640:  16%|█▌        | 3/19 [00:00<00:03,  4.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07459 0.04172 0.02236 77 640:  16%|█▌        | 3/19 [00:01<00:03,  4.17it/s]94/99 7.3G 0.07459 0.04172 0.02236 77 640:  21%|██        | 4/19 [00:01<00:04,  3.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07458 0.04059 0.0231 58 640:  21%|██        | 4/19 [00:01<00:04,  3.38it/s] 94/99 7.3G 0.07458 0.04059 0.0231 58 640:  26%|██▋       | 5/19 [00:01<00:04,  2.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07482 0.04307 0.02201 100 640:  26%|██▋       | 5/19 [00:01<00:04,  2.92it/s]94/99 7.3G 0.07482 0.04307 0.02201 100 640:  32%|███▏      | 6/19 [00:01<00:04,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07559 0.04445 0.02246 91 640:  32%|███▏      | 6/19 [00:02<00:04,  2.67it/s] 94/99 7.3G 0.07559 0.04445 0.02246 91 640:  37%|███▋      | 7/19 [00:02<00:04,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07635 0.04682 0.02243 110 640:  37%|███▋      | 7/19 [00:02<00:04,  2.74it/s]94/99 7.3G 0.07635 0.04682 0.02243 110 640:  42%|████▏     | 8/19 [00:02<00:03,  2.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07685 0.04875 0.02245 110 640:  42%|████▏     | 8/19 [00:02<00:03,  2.78it/s]94/99 7.3G 0.07685 0.04875 0.02245 110 640:  47%|████▋     | 9/19 [00:02<00:03,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07664 0.0473 0.02309 56 640:  47%|████▋     | 9/19 [00:03<00:03,  2.81it/s]  94/99 7.3G 0.07664 0.0473 0.02309 56 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07636 0.04626 0.02307 51 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.83it/s]94/99 7.3G 0.07636 0.04626 0.02307 51 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.0762 0.04601 0.02283 72 640:  58%|█████▊    | 11/19 [00:07<00:02,  2.85it/s] 94/99 7.3G 0.0762 0.04601 0.02283 72 640:  63%|██████▎   | 12/19 [00:07<00:09,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07636 0.04612 0.02278 82 640:  63%|██████▎   | 12/19 [00:07<00:09,  1.31s/it]94/99 7.3G 0.07636 0.04612 0.02278 82 640:  68%|██████▊   | 13/19 [00:07<00:05,  1.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07634 0.04589 0.02267 68 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.04it/s]94/99 7.3G 0.07634 0.04589 0.02267 68 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07647 0.04645 0.02262 93 640:  74%|███████▎  | 14/19 [00:10<00:05,  1.03s/it]94/99 7.3G 0.07647 0.04645 0.02262 93 640:  79%|███████▉  | 15/19 [00:10<00:05,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07617 0.04547 0.02296 46 640:  79%|███████▉  | 15/19 [00:11<00:05,  1.44s/it]94/99 7.3G 0.07617 0.04547 0.02296 46 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.0764 0.04554 0.02317 74 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.06s/it] 94/99 7.3G 0.0764 0.04554 0.02317 74 640:  89%|████████▉ | 17/19 [00:11<00:01,  1.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07678 0.04511 0.02306 81 640:  89%|████████▉ | 17/19 [00:12<00:01,  1.05it/s]94/99 7.3G 0.07678 0.04511 0.02306 81 640:  95%|█████████▍| 18/19 [00:12<00:00,  1.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
94/99 7.3G 0.07651 0.04458 0.02294 54 640:  95%|█████████▍| 18/19 [00:14<00:00,  1.15it/s]94/99 7.3G 0.07651 0.04458 0.02294 54 640: 100%|██████████| 19/19 [00:14<00:00,  1.09s/it]94/99 7.3G 0.07651 0.04458 0.02294 54 640: 100%|██████████| 19/19 [00:14<00:00,  1.35it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.23s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.22s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]
                   all         55        256      0.191      0.221      0.145     0.0394
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07107 0.03933 0.01957 64 640:   0%|          | 0/19 [00:00<?, ?it/s]95/99 7.3G 0.07107 0.03933 0.01957 64 640:   5%|▌         | 1/19 [00:00<00:03,  5.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07354 0.03194 0.02172 41 640:   5%|▌         | 1/19 [00:00<00:03,  5.20it/s]95/99 7.3G 0.07354 0.03194 0.02172 41 640:  11%|█         | 2/19 [00:00<00:03,  5.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07592 0.03451 0.02162 79 640:  11%|█         | 2/19 [00:00<00:03,  5.51it/s]95/99 7.3G 0.07592 0.03451 0.02162 79 640:  16%|█▌        | 3/19 [00:00<00:02,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07621 0.035 0.02168 63 640:  16%|█▌        | 3/19 [00:00<00:02,  5.61it/s]  95/99 7.3G 0.07621 0.035 0.02168 63 640:  21%|██        | 4/19 [00:00<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07696 0.03752 0.02122 87 640:  21%|██        | 4/19 [00:00<00:02,  5.67it/s]95/99 7.3G 0.07696 0.03752 0.02122 87 640:  26%|██▋       | 5/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07715 0.03805 0.02107 70 640:  26%|██▋       | 5/19 [00:01<00:02,  5.71it/s]95/99 7.3G 0.07715 0.03805 0.02107 70 640:  32%|███▏      | 6/19 [00:01<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07679 0.03728 0.02142 50 640:  32%|███▏      | 6/19 [00:01<00:02,  5.69it/s]95/99 7.3G 0.07679 0.03728 0.02142 50 640:  37%|███▋      | 7/19 [00:01<00:02,  5.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07782 0.0397 0.02126 109 640:  37%|███▋      | 7/19 [00:01<00:02,  5.47it/s]95/99 7.3G 0.07782 0.0397 0.02126 109 640:  42%|████▏     | 8/19 [00:01<00:02,  4.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07753 0.04133 0.022 88 640:  42%|████▏     | 8/19 [00:06<00:02,  4.81it/s]  95/99 7.3G 0.07753 0.04133 0.022 88 640:  47%|████▋     | 9/19 [00:06<00:18,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07767 0.04074 0.02216 59 640:  47%|████▋     | 9/19 [00:07<00:18,  1.85s/it]95/99 7.3G 0.07767 0.04074 0.02216 59 640:  53%|█████▎    | 10/19 [00:07<00:11,  1.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07806 0.04241 0.02184 115 640:  53%|█████▎    | 10/19 [00:07<00:11,  1.33s/it]95/99 7.3G 0.07806 0.04241 0.02184 115 640:  58%|█████▊    | 11/19 [00:07<00:07,  1.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.0777 0.04259 0.02168 77 640:  58%|█████▊    | 11/19 [00:10<00:07,  1.00it/s]  95/99 7.3G 0.0777 0.04259 0.02168 77 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.50s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07753 0.04143 0.02137 50 640:  63%|██████▎   | 12/19 [00:10<00:10,  1.50s/it]95/99 7.3G 0.07753 0.04143 0.02137 50 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07796 0.04342 0.02139 133 640:  68%|██████▊   | 13/19 [00:10<00:06,  1.14s/it]95/99 7.3G 0.07796 0.04342 0.02139 133 640:  74%|███████▎  | 14/19 [00:10<00:04,  1.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07747 0.04307 0.02119 58 640:  74%|███████▎  | 14/19 [00:11<00:04,  1.06it/s] 95/99 7.3G 0.07747 0.04307 0.02119 58 640:  79%|███████▉  | 15/19 [00:11<00:02,  1.34it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07759 0.04294 0.02138 65 640:  79%|███████▉  | 15/19 [00:12<00:02,  1.34it/s]95/99 7.3G 0.07759 0.04294 0.02138 65 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.06s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07839 0.0442 0.02135 159 640:  84%|████████▍ | 16/19 [00:20<00:03,  1.06s/it]95/99 7.3G 0.07839 0.0442 0.02135 159 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.92s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07841 0.04437 0.02133 92 640:  89%|████████▉ | 17/19 [00:20<00:05,  2.92s/it]95/99 7.3G 0.07841 0.04437 0.02133 92 640:  95%|█████████▍| 18/19 [00:20<00:02,  2.20s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
95/99 7.3G 0.07824 0.04443 0.02125 74 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.20s/it]95/99 7.3G 0.07824 0.04443 0.02125 74 640: 100%|██████████| 19/19 [00:21<00:00,  1.72s/it]95/99 7.3G 0.07824 0.04443 0.02125 74 640: 100%|██████████| 19/19 [00:21<00:00,  1.12s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.44s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.78s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.33s/it]
                   all         55        256      0.192      0.223      0.149     0.0405
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07426 0.03304 0.0256 54 640:   0%|          | 0/19 [00:00<?, ?it/s]96/99 7.3G 0.07426 0.03304 0.0256 54 640:   5%|▌         | 1/19 [00:00<00:07,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.0732 0.03436 0.02253 58 640:   5%|▌         | 1/19 [00:00<00:07,  2.55it/s]96/99 7.3G 0.0732 0.03436 0.02253 58 640:  11%|█         | 2/19 [00:00<00:06,  2.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07327 0.03753 0.02227 74 640:  11%|█         | 2/19 [00:01<00:06,  2.61it/s]96/99 7.3G 0.07327 0.03753 0.02227 74 640:  16%|█▌        | 3/19 [00:01<00:07,  2.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07563 0.04254 0.02366 94 640:  16%|█▌        | 3/19 [00:01<00:07,  2.04it/s]96/99 7.3G 0.07563 0.04254 0.02366 94 640:  21%|██        | 4/19 [00:01<00:07,  1.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07579 0.04372 0.02347 84 640:  21%|██        | 4/19 [00:02<00:07,  1.93it/s]96/99 7.3G 0.07579 0.04372 0.02347 84 640:  26%|██▋       | 5/19 [00:02<00:07,  1.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07546 0.04634 0.02366 93 640:  26%|██▋       | 5/19 [00:02<00:07,  1.98it/s]96/99 7.3G 0.07546 0.04634 0.02366 93 640:  32%|███▏      | 6/19 [00:02<00:06,  2.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07596 0.04763 0.02312 105 640:  32%|███▏      | 6/19 [00:03<00:06,  2.01it/s]96/99 7.3G 0.07596 0.04763 0.02312 105 640:  37%|███▋      | 7/19 [00:03<00:05,  2.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07545 0.04528 0.02279 44 640:  37%|███▋      | 7/19 [00:03<00:05,  2.15it/s] 96/99 7.3G 0.07545 0.04528 0.02279 44 640:  42%|████▏     | 8/19 [00:03<00:04,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07624 0.04767 0.02229 136 640:  42%|████▏     | 8/19 [00:04<00:04,  2.28it/s]96/99 7.3G 0.07624 0.04767 0.02229 136 640:  47%|████▋     | 9/19 [00:04<00:04,  2.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07535 0.04562 0.0226 42 640:  47%|████▋     | 9/19 [00:04<00:04,  2.38it/s]  96/99 7.3G 0.07535 0.04562 0.0226 42 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07527 0.04412 0.02234 47 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.47it/s]96/99 7.3G 0.07527 0.04412 0.02234 47 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07569 0.04449 0.02235 85 640:  58%|█████▊    | 11/19 [00:04<00:02,  2.73it/s]96/99 7.3G 0.07569 0.04449 0.02235 85 640:  63%|██████▎   | 12/19 [00:04<00:02,  3.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07652 0.04603 0.02205 144 640:  63%|██████▎   | 12/19 [00:05<00:02,  3.15it/s]96/99 7.3G 0.07652 0.04603 0.02205 144 640:  68%|██████▊   | 13/19 [00:05<00:01,  3.52it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07682 0.04537 0.02188 76 640:  68%|██████▊   | 13/19 [00:08<00:01,  3.52it/s] 96/99 7.3G 0.07682 0.04537 0.02188 76 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07716 0.04673 0.02199 123 640:  74%|███████▎  | 14/19 [00:08<00:05,  1.14s/it]96/99 7.3G 0.07716 0.04673 0.02199 123 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.13it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07786 0.04723 0.0219 149 640:  79%|███████▉  | 15/19 [00:08<00:03,  1.13it/s] 96/99 7.3G 0.07786 0.04723 0.0219 149 640:  84%|████████▍ | 16/19 [00:08<00:02,  1.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07794 0.04774 0.02199 98 640:  84%|████████▍ | 16/19 [00:10<00:02,  1.46it/s]96/99 7.3G 0.07794 0.04774 0.02199 98 640:  89%|████████▉ | 17/19 [00:10<00:02,  1.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07816 0.04768 0.02175 85 640:  89%|████████▉ | 17/19 [00:11<00:02,  1.03s/it]96/99 7.3G 0.07816 0.04768 0.02175 85 640:  95%|█████████▍| 18/19 [00:11<00:01,  1.08s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
96/99 7.3G 0.07834 0.04838 0.02176 117 640:  95%|█████████▍| 18/19 [00:11<00:01,  1.08s/it]96/99 7.3G 0.07834 0.04838 0.02176 117 640: 100%|██████████| 19/19 [00:11<00:00,  1.24it/s]96/99 7.3G 0.07834 0.04838 0.02176 117 640: 100%|██████████| 19/19 [00:11<00:00,  1.59it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.13s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.47s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]
                   all         55        256      0.188       0.22      0.146      0.041
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.08482 0.08237 0.02268 162 640:   0%|          | 0/19 [00:00<?, ?it/s]97/99 7.3G 0.08482 0.08237 0.02268 162 640:   5%|▌         | 1/19 [00:00<00:03,  5.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07573 0.05775 0.02132 49 640:   5%|▌         | 1/19 [00:00<00:03,  5.32it/s] 97/99 7.3G 0.07573 0.05775 0.02132 49 640:  11%|█         | 2/19 [00:00<00:03,  5.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07619 0.0544 0.0193 85 640:  11%|█         | 2/19 [00:00<00:03,  5.55it/s]  97/99 7.3G 0.07619 0.0544 0.0193 85 640:  16%|█▌        | 3/19 [00:00<00:02,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07779 0.05494 0.0191 110 640:  16%|█▌        | 3/19 [00:00<00:02,  5.64it/s]97/99 7.3G 0.07779 0.05494 0.0191 110 640:  21%|██        | 4/19 [00:00<00:02,  5.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07737 0.05269 0.0196 74 640:  21%|██        | 4/19 [00:01<00:02,  5.37it/s] 97/99 7.3G 0.07737 0.05269 0.0196 74 640:  26%|██▋       | 5/19 [00:01<00:03,  3.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07748 0.05359 0.01934 112 640:  26%|██▋       | 5/19 [00:01<00:03,  3.85it/s]97/99 7.3G 0.07748 0.05359 0.01934 112 640:  32%|███▏      | 6/19 [00:01<00:03,  3.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07808 0.05101 0.01922 62 640:  32%|███▏      | 6/19 [00:01<00:03,  3.35it/s] 97/99 7.3G 0.07808 0.05101 0.01922 62 640:  37%|███▋      | 7/19 [00:01<00:03,  3.10it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.0787 0.04853 0.01912 54 640:  37%|███▋      | 7/19 [00:02<00:03,  3.10it/s] 97/99 7.3G 0.0787 0.04853 0.01912 54 640:  42%|████▏     | 8/19 [00:02<00:03,  2.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07928 0.04883 0.01958 124 640:  42%|████▏     | 8/19 [00:02<00:03,  2.97it/s]97/99 7.3G 0.07928 0.04883 0.01958 124 640:  47%|████▋     | 9/19 [00:02<00:03,  2.85it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07931 0.04813 0.01986 78 640:  47%|████▋     | 9/19 [00:03<00:03,  2.85it/s] 97/99 7.3G 0.07931 0.04813 0.01986 78 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.0788 0.04828 0.02008 80 640:  53%|█████▎    | 10/19 [00:08<00:03,  2.73it/s] 97/99 7.3G 0.0788 0.04828 0.02008 80 640:  58%|█████▊    | 11/19 [00:08<00:14,  1.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07979 0.0516 0.02008 214 640:  58%|█████▊    | 11/19 [00:08<00:14,  1.81s/it]97/99 7.3G 0.07979 0.0516 0.02008 214 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07895 0.04999 0.02012 52 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.31s/it]97/99 7.3G 0.07895 0.04999 0.02012 52 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07836 0.04998 0.02025 77 640:  68%|██████▊   | 13/19 [00:11<00:05,  1.03it/s]97/99 7.3G 0.07836 0.04998 0.02025 77 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07867 0.04985 0.02045 92 640:  74%|███████▎  | 14/19 [00:12<00:08,  1.61s/it]97/99 7.3G 0.07867 0.04985 0.02045 92 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07867 0.04985 0.02038 85 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.36s/it]97/99 7.3G 0.07867 0.04985 0.02038 85 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07818 0.04852 0.02006 46 640:  84%|████████▍ | 16/19 [00:14<00:03,  1.02s/it]97/99 7.3G 0.07818 0.04852 0.02006 46 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07831 0.04821 0.02018 72 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.31s/it]97/99 7.3G 0.07831 0.04821 0.02018 72 640:  95%|█████████▍| 18/19 [00:14<00:00,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
97/99 7.3G 0.07795 0.04797 0.0204 69 640:  95%|█████████▍| 18/19 [00:21<00:00,  1.01it/s] 97/99 7.3G 0.07795 0.04797 0.0204 69 640: 100%|██████████| 19/19 [00:21<00:00,  2.65s/it]97/99 7.3G 0.07795 0.04797 0.0204 69 640: 100%|██████████| 19/19 [00:21<00:00,  1.12s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]
                   all         55        256      0.189      0.225      0.152     0.0419
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.06912 0.04842 0.02387 74 640:   0%|          | 0/19 [00:00<?, ?it/s]98/99 7.3G 0.06912 0.04842 0.02387 74 640:   5%|▌         | 1/19 [00:00<00:08,  2.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07423 0.04417 0.02192 68 640:   5%|▌         | 1/19 [00:00<00:08,  2.08it/s]98/99 7.3G 0.07423 0.04417 0.02192 68 640:  11%|█         | 2/19 [00:00<00:08,  2.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07443 0.03997 0.02137 55 640:  11%|█         | 2/19 [00:01<00:08,  2.09it/s]98/99 7.3G 0.07443 0.03997 0.02137 55 640:  16%|█▌        | 3/19 [00:01<00:07,  2.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07654 0.04074 0.02066 83 640:  16%|█▌        | 3/19 [00:01<00:07,  2.18it/s]98/99 7.3G 0.07654 0.04074 0.02066 83 640:  21%|██        | 4/19 [00:01<00:06,  2.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07701 0.03877 0.02067 51 640:  21%|██        | 4/19 [00:02<00:06,  2.26it/s]98/99 7.3G 0.07701 0.03877 0.02067 51 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07696 0.03919 0.02039 70 640:  26%|██▋       | 5/19 [00:02<00:06,  2.28it/s]98/99 7.3G 0.07696 0.03919 0.02039 70 640:  32%|███▏      | 6/19 [00:02<00:05,  2.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07657 0.03986 0.02082 67 640:  32%|███▏      | 6/19 [00:03<00:05,  2.36it/s]98/99 7.3G 0.07657 0.03986 0.02082 67 640:  37%|███▋      | 7/19 [00:03<00:04,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07648 0.04044 0.02097 83 640:  37%|███▋      | 7/19 [00:03<00:04,  2.43it/s]98/99 7.3G 0.07648 0.04044 0.02097 83 640:  42%|████▏     | 8/19 [00:03<00:04,  2.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07674 0.04206 0.021 104 640:  42%|████▏     | 8/19 [00:03<00:04,  2.49it/s] 98/99 7.3G 0.07674 0.04206 0.021 104 640:  47%|████▋     | 9/19 [00:03<00:03,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07651 0.04218 0.02082 74 640:  47%|████▋     | 9/19 [00:04<00:03,  2.54it/s]98/99 7.3G 0.07651 0.04218 0.02082 74 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.48it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07699 0.04255 0.02032 108 640:  53%|█████▎    | 10/19 [00:04<00:03,  2.48it/s]98/99 7.3G 0.07699 0.04255 0.02032 108 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07687 0.04226 0.02044 66 640:  58%|█████▊    | 11/19 [00:04<00:03,  2.50it/s] 98/99 7.3G 0.07687 0.04226 0.02044 66 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07697 0.04186 0.02079 65 640:  63%|██████▎   | 12/19 [00:05<00:02,  2.51it/s]98/99 7.3G 0.07697 0.04186 0.02079 65 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07727 0.04206 0.02095 87 640:  68%|██████▊   | 13/19 [00:05<00:02,  2.55it/s]98/99 7.3G 0.07727 0.04206 0.02095 87 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07755 0.04199 0.02082 77 640:  74%|███████▎  | 14/19 [00:05<00:01,  2.90it/s]98/99 7.3G 0.07755 0.04199 0.02082 77 640:  79%|███████▉  | 15/19 [00:05<00:01,  3.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07743 0.04226 0.02062 77 640:  79%|███████▉  | 15/19 [00:12<00:01,  3.20it/s]98/99 7.3G 0.07743 0.04226 0.02062 77 640:  84%|████████▍ | 16/19 [00:12<00:07,  2.36s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07727 0.04216 0.0205 76 640:  84%|████████▍ | 16/19 [00:13<00:07,  2.36s/it] 98/99 7.3G 0.07727 0.04216 0.0205 76 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07703 0.04294 0.02064 88 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.81s/it]98/99 7.3G 0.07703 0.04294 0.02064 88 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.39s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
98/99 7.3G 0.07715 0.04268 0.02059 69 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.39s/it]98/99 7.3G 0.07715 0.04268 0.02059 69 640: 100%|██████████| 19/19 [00:14<00:00,  1.28s/it]98/99 7.3G 0.07715 0.04268 0.02059 69 640: 100%|██████████| 19/19 [00:14<00:00,  1.27it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.18s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.17s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
                   all         55        256      0.184      0.216      0.124     0.0384
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07308 0.03989 0.02066 62 640:   0%|          | 0/19 [00:00<?, ?it/s]99/99 7.3G 0.07308 0.03989 0.02066 62 640:   5%|▌         | 1/19 [00:00<00:08,  2.17it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07495 0.04373 0.01947 87 640:   5%|▌         | 1/19 [00:00<00:08,  2.17it/s]99/99 7.3G 0.07495 0.04373 0.01947 87 640:  11%|█         | 2/19 [00:00<00:06,  2.51it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07698 0.04922 0.01985 106 640:  11%|█         | 2/19 [00:01<00:06,  2.51it/s]99/99 7.3G 0.07698 0.04922 0.01985 106 640:  16%|█▌        | 3/19 [00:01<00:06,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07585 0.0477 0.01986 69 640:  16%|█▌        | 3/19 [00:01<00:06,  2.54it/s]  99/99 7.3G 0.07585 0.0477 0.01986 69 640:  21%|██        | 4/19 [00:01<00:05,  2.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07648 0.04734 0.0204 79 640:  21%|██        | 4/19 [00:01<00:05,  2.66it/s]99/99 7.3G 0.07648 0.04734 0.0204 79 640:  26%|██▋       | 5/19 [00:01<00:05,  2.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07657 0.04638 0.02055 71 640:  26%|██▋       | 5/19 [00:02<00:05,  2.74it/s]99/99 7.3G 0.07657 0.04638 0.02055 71 640:  32%|███▏      | 6/19 [00:02<00:04,  2.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07589 0.04386 0.02161 44 640:  32%|███▏      | 6/19 [00:02<00:04,  2.94it/s]99/99 7.3G 0.07589 0.04386 0.02161 44 640:  37%|███▋      | 7/19 [00:02<00:03,  3.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07479 0.04178 0.02176 44 640:  37%|███▋      | 7/19 [00:02<00:03,  3.50it/s]99/99 7.3G 0.07479 0.04178 0.02176 44 640:  42%|████▏     | 8/19 [00:02<00:02,  4.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07357 0.03975 0.02197 36 640:  42%|████▏     | 8/19 [00:02<00:02,  4.00it/s]99/99 7.3G 0.07357 0.03975 0.02197 36 640:  47%|████▋     | 9/19 [00:02<00:02,  4.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07438 0.04251 0.02176 133 640:  47%|████▋     | 9/19 [00:02<00:02,  4.42it/s]99/99 7.3G 0.07438 0.04251 0.02176 133 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.49it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07512 0.04562 0.02173 151 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.49it/s]99/99 7.3G 0.07512 0.04562 0.02173 151 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07572 0.04615 0.02149 106 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.56it/s]99/99 7.3G 0.07572 0.04615 0.02149 106 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07581 0.04672 0.02197 85 640:  63%|██████▎   | 12/19 [00:09<00:01,  4.65it/s] 99/99 7.3G 0.07581 0.04672 0.02197 85 640:  68%|██████▊   | 13/19 [00:09<00:12,  2.03s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07574 0.04562 0.02186 56 640:  68%|██████▊   | 13/19 [00:09<00:12,  2.03s/it]99/99 7.3G 0.07574 0.04562 0.02186 56 640:  74%|███████▎  | 14/19 [00:09<00:07,  1.49s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07561 0.04496 0.02207 55 640:  74%|███████▎  | 14/19 [00:09<00:07,  1.49s/it]99/99 7.3G 0.07561 0.04496 0.02207 55 640:  79%|███████▉  | 15/19 [00:09<00:04,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07573 0.04517 0.02189 85 640:  79%|███████▉  | 15/19 [00:12<00:04,  1.09s/it]99/99 7.3G 0.07573 0.04517 0.02189 85 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.65s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.0761 0.04629 0.0221 117 640:  84%|████████▍ | 16/19 [00:14<00:04,  1.65s/it] 99/99 7.3G 0.0761 0.04629 0.0221 117 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07621 0.04648 0.02193 90 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.48s/it]99/99 7.3G 0.07621 0.04648 0.02193 90 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
99/99 7.3G 0.07679 0.04729 0.02176 146 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.29s/it]99/99 7.3G 0.07679 0.04729 0.02176 146 640: 100%|██████████| 19/19 [00:15<00:00,  1.04s/it]99/99 7.3G 0.07679 0.04729 0.02176 146 640: 100%|██████████| 19/19 [00:15<00:00,  1.24it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.68s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.82s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]
                   all         55        256      0.188      0.212      0.124     0.0383
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
[34m[1mtrain_dp: [0mweights=yolov5s.pt, cfg=, data=/mnt/bst/hxu10/hxu10/chanti/dataset/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, device=6, multi_scale=False, single_cls=False, optimizer=SGD, workers=8, project=runs/train/train_dp, name=noise_0.3, exist_ok=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, dp=True, noise_multiplier=0.3, max_grad_norm=5.0, delta=1e-05
[34m[1mgithub: [0mup to date with https://github.com/ultralytics/yolov5 ✅
YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.5 torch-2.6.0+cu124 CUDA:6 (NVIDIA A100-SXM4-80GB, 81154MiB)

[34m[1mhyperparameters: [0mlr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0005, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
Overriding model.yaml nc=80 with nc=3

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs

Transferred 342/349 layers from yolov5s.pt
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
/mnt/bst/hxu10/hxu10/chanti/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(autocast):
[34m[1mAMP: [0mchecks passed ✅
[34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s][34m[1mtrain: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/train.cache... 304 images, 0 backgrounds, 0 corrupt: 100%|██████████| 304/304 [00:00<?, ?it/s]
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
[34m[1moptimizer:[0m SGD(lr=0.001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias
Privacy Engine attached: Noise Multiplier=0.3, Max Grad Norm=5.0
/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=amp and not opt.dp)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to [1mruns/train/train_dp/noise_0.34[0m
Starting training for 100 epochs...
✅ Model is now private: False
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1102 0.03655 0.03916 62 640:   0%|          | 0/19 [00:19<?, ?it/s]0/99 7.18G 0.1102 0.03655 0.03916 62 640:   5%|▌         | 1/19 [00:19<05:55, 19.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1095 0.03756 0.03874 61 640:   5%|▌         | 1/19 [00:19<05:55, 19.73s/it]0/99 7.18G 0.1095 0.03756 0.03874 61 640:  11%|█         | 2/19 [00:19<02:20,  8.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1103 0.03569 0.03851 52 640:  11%|█         | 2/19 [00:20<02:20,  8.28s/it]0/99 7.18G 0.1103 0.03569 0.03851 52 640:  16%|█▌        | 3/19 [00:20<01:13,  4.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1096 0.03625 0.03842 61 640:  16%|█▌        | 3/19 [00:20<01:13,  4.60s/it]0/99 7.18G 0.1096 0.03625 0.03842 61 640:  21%|██        | 4/19 [00:20<00:42,  2.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.109 0.03706 0.03842 66 640:  21%|██        | 4/19 [00:20<00:42,  2.87s/it] 0/99 7.18G 0.109 0.03706 0.03842 66 640:  26%|██▋       | 5/19 [00:20<00:26,  1.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1095 0.04165 0.03829 175 640:  26%|██▋       | 5/19 [00:20<00:26,  1.91s/it]0/99 7.18G 0.1095 0.04165 0.03829 175 640:  32%|███▏      | 6/19 [00:20<00:17,  1.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1094 0.04099 0.03817 69 640:  32%|███▏      | 6/19 [00:21<00:17,  1.33s/it] 0/99 7.18G 0.1094 0.04099 0.03817 69 640:  37%|███▋      | 7/19 [00:21<00:11,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1096 0.04397 0.03804 148 640:  37%|███▋      | 7/19 [00:21<00:11,  1.03it/s]0/99 7.18G 0.1096 0.04397 0.03804 148 640:  42%|████▏     | 8/19 [00:21<00:08,  1.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1094 0.04425 0.038 88 640:  42%|████▏     | 8/19 [00:29<00:08,  1.37it/s]   0/99 7.18G 0.1094 0.04425 0.038 88 640:  47%|████▋     | 9/19 [00:29<00:31,  3.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1091 0.04296 0.03797 45 640:  47%|████▋     | 9/19 [00:30<00:31,  3.18s/it]0/99 7.18G 0.1091 0.04296 0.03797 45 640:  53%|█████▎    | 10/19 [00:30<00:20,  2.31s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1088 0.04237 0.03797 63 640:  53%|█████▎    | 10/19 [00:30<00:20,  2.31s/it]0/99 7.18G 0.1088 0.04237 0.03797 63 640:  58%|█████▊    | 11/19 [00:30<00:13,  1.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1088 0.04369 0.03786 134 640:  58%|█████▊    | 11/19 [00:30<00:13,  1.66s/it]0/99 7.18G 0.1088 0.04369 0.03786 134 640:  63%|██████▎   | 12/19 [00:30<00:08,  1.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1084 0.04317 0.03792 58 640:  63%|██████▎   | 12/19 [00:30<00:08,  1.22s/it] 0/99 7.18G 0.1084 0.04317 0.03792 58 640:  68%|██████▊   | 13/19 [00:30<00:05,  1.11it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.108 0.04272 0.03793 59 640:  68%|██████▊   | 13/19 [00:30<00:05,  1.11it/s] 0/99 7.18G 0.108 0.04272 0.03793 59 640:  74%|███████▎  | 14/19 [00:30<00:03,  1.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1078 0.04254 0.03792 67 640:  74%|███████▎  | 14/19 [00:31<00:03,  1.47it/s]0/99 7.18G 0.1078 0.04254 0.03792 67 640:  79%|███████▉  | 15/19 [00:31<00:02,  1.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1078 0.04303 0.03786 107 640:  79%|███████▉  | 15/19 [00:31<00:02,  1.89it/s]0/99 7.18G 0.1078 0.04303 0.03786 107 640:  84%|████████▍ | 16/19 [00:31<00:01,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1076 0.04286 0.03778 71 640:  84%|████████▍ | 16/19 [00:42<00:01,  2.29it/s] 0/99 7.18G 0.1076 0.04286 0.03778 71 640:  89%|████████▉ | 17/19 [00:42<00:07,  3.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1073 0.0425 0.03774 61 640:  89%|████████▉ | 17/19 [00:43<00:07,  3.69s/it] 0/99 7.18G 0.1073 0.0425 0.03774 61 640:  95%|█████████▍| 18/19 [00:43<00:02,  2.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
0/99 7.18G 0.1072 0.04242 0.03771 72 640:  95%|█████████▍| 18/19 [00:43<00:02,  2.70s/it]0/99 7.18G 0.1072 0.04242 0.03771 72 640: 100%|██████████| 19/19 [00:43<00:00,  2.09s/it]0/99 7.18G 0.1072 0.04242 0.03771 72 640: 100%|██████████| 19/19 [00:43<00:00,  2.30s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.85s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.63s/it]
                   all         55        256    0.00176      0.235    0.00162   0.000341
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1008 0.04523 0.0379 74 640:   0%|          | 0/19 [00:01<?, ?it/s]1/99 7.2G 0.1008 0.04523 0.0379 74 640:   5%|▌         | 1/19 [00:01<00:24,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1028 0.04573 0.0375 87 640:   5%|▌         | 1/19 [00:01<00:24,  1.37s/it]1/99 7.2G 0.1028 0.04573 0.0375 87 640:  11%|█         | 2/19 [00:01<00:11,  1.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1027 0.04504 0.03733 89 640:  11%|█         | 2/19 [00:01<00:11,  1.45it/s]1/99 7.2G 0.1027 0.04504 0.03733 89 640:  16%|█▌        | 3/19 [00:01<00:07,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1026 0.04277 0.03726 65 640:  16%|█▌        | 3/19 [00:02<00:07,  2.05it/s]1/99 7.2G 0.1026 0.04277 0.03726 65 640:  21%|██        | 4/19 [00:02<00:05,  2.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1025 0.04336 0.03721 85 640:  21%|██        | 4/19 [00:02<00:05,  2.59it/s]1/99 7.2G 0.1025 0.04336 0.03721 85 640:  26%|██▋       | 5/19 [00:02<00:04,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1024 0.04241 0.03721 66 640:  26%|██▋       | 5/19 [00:02<00:04,  2.86it/s]1/99 7.2G 0.1024 0.04241 0.03721 66 640:  32%|███▏      | 6/19 [00:02<00:03,  3.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1023 0.04182 0.03718 63 640:  32%|███▏      | 6/19 [00:02<00:03,  3.30it/s]1/99 7.2G 0.1023 0.04182 0.03718 63 640:  37%|███▋      | 7/19 [00:02<00:03,  3.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1018 0.04118 0.03721 55 640:  37%|███▋      | 7/19 [00:02<00:03,  3.66it/s]1/99 7.2G 0.1018 0.04118 0.03721 55 640:  42%|████▏     | 8/19 [00:02<00:02,  3.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1016 0.04139 0.03706 73 640:  42%|████▏     | 8/19 [00:03<00:02,  3.97it/s]1/99 7.2G 0.1016 0.04139 0.03706 73 640:  47%|████▋     | 9/19 [00:03<00:02,  4.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1019 0.04384 0.03696 143 640:  47%|████▋     | 9/19 [00:03<00:02,  4.19it/s]1/99 7.2G 0.1019 0.04384 0.03696 143 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1018 0.04247 0.03696 43 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.29it/s] 1/99 7.2G 0.1018 0.04247 0.03696 43 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1023 0.04502 0.0369 181 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.92it/s]1/99 7.2G 0.1023 0.04502 0.0369 181 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.93it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1021 0.04552 0.03685 95 640:  63%|██████▎   | 12/19 [00:04<00:01,  3.93it/s]1/99 7.2G 0.1021 0.04552 0.03685 95 640:  68%|██████▊   | 13/19 [00:04<00:01,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1021 0.04585 0.03682 92 640:  68%|██████▊   | 13/19 [00:11<00:01,  4.14it/s]1/99 7.2G 0.1021 0.04585 0.03682 92 640:  74%|███████▎  | 14/19 [00:11<00:12,  2.41s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1019 0.04545 0.03678 70 640:  74%|███████▎  | 14/19 [00:12<00:12,  2.41s/it]1/99 7.2G 0.1019 0.04545 0.03678 70 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.81s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1016 0.04507 0.03672 63 640:  79%|███████▉  | 15/19 [00:12<00:07,  1.81s/it]1/99 7.2G 0.1016 0.04507 0.03672 63 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1014 0.04479 0.03667 66 640:  84%|████████▍ | 16/19 [00:13<00:04,  1.38s/it]1/99 7.2G 0.1014 0.04479 0.03667 66 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.22s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.1013 0.04432 0.0366 58 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.22s/it] 1/99 7.2G 0.1013 0.04432 0.0366 58 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
1/99 7.2G 0.101 0.04401 0.03654 60 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.29s/it]1/99 7.2G 0.101 0.04401 0.03654 60 640: 100%|██████████| 19/19 [00:15<00:00,  1.01s/it]1/99 7.2G 0.101 0.04401 0.03654 60 640: 100%|██████████| 19/19 [00:15<00:00,  1.26it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.14s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.42s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
                   all         55        256    0.00182      0.288    0.00165   0.000372
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1089 0.05159 0.0346 144 640:   0%|          | 0/19 [00:00<?, ?it/s]2/99 7.21G 0.1089 0.05159 0.0346 144 640:   5%|▌         | 1/19 [00:00<00:10,  1.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.1035 0.05336 0.03476 107 640:   5%|▌         | 1/19 [00:00<00:10,  1.67it/s]2/99 7.21G 0.1035 0.05336 0.03476 107 640:  11%|█         | 2/19 [00:00<00:05,  2.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.102 0.04865 0.03495 66 640:  11%|█         | 2/19 [00:00<00:05,  2.86it/s]  2/99 7.21G 0.102 0.04865 0.03495 66 640:  16%|█▌        | 3/19 [00:00<00:04,  3.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09975 0.04714 0.03493 68 640:  16%|█▌        | 3/19 [00:01<00:04,  3.71it/s]2/99 7.21G 0.09975 0.04714 0.03493 68 640:  21%|██        | 4/19 [00:01<00:03,  4.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09916 0.04707 0.03514 76 640:  21%|██        | 4/19 [00:01<00:03,  4.30it/s]2/99 7.21G 0.09916 0.04707 0.03514 76 640:  26%|██▋       | 5/19 [00:01<00:02,  4.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09848 0.04556 0.03508 61 640:  26%|██▋       | 5/19 [00:01<00:02,  4.71it/s]2/99 7.21G 0.09848 0.04556 0.03508 61 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09881 0.04658 0.03502 106 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]2/99 7.21G 0.09881 0.04658 0.03502 106 640:  37%|███▋      | 7/19 [00:01<00:02,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09852 0.04648 0.03488 79 640:  37%|███▋      | 7/19 [00:02<00:02,  4.37it/s] 2/99 7.21G 0.09852 0.04648 0.03488 79 640:  42%|████▏     | 8/19 [00:02<00:02,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09836 0.04587 0.03489 69 640:  42%|████▏     | 8/19 [00:02<00:02,  3.74it/s]2/99 7.21G 0.09836 0.04587 0.03489 69 640:  47%|████▋     | 9/19 [00:02<00:02,  3.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09814 0.0447 0.03481 55 640:  47%|████▋     | 9/19 [00:02<00:02,  3.43it/s] 2/99 7.21G 0.09814 0.0447 0.03481 55 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09806 0.04459 0.03475 76 640:  53%|█████▎    | 10/19 [00:07<00:03,  2.94it/s]2/99 7.21G 0.09806 0.04459 0.03475 76 640:  58%|█████▊    | 11/19 [00:07<00:13,  1.64s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09782 0.04286 0.03465 35 640:  58%|█████▊    | 11/19 [00:07<00:13,  1.64s/it]2/99 7.21G 0.09782 0.04286 0.03465 35 640:  63%|██████▎   | 12/19 [00:07<00:08,  1.21s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09771 0.04228 0.03465 55 640:  63%|██████▎   | 12/19 [00:08<00:08,  1.21s/it]2/99 7.21G 0.09771 0.04228 0.03465 55 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09786 0.04322 0.03463 101 640:  68%|██████▊   | 13/19 [00:08<00:05,  1.06it/s]2/99 7.21G 0.09786 0.04322 0.03463 101 640:  74%|███████▎  | 14/19 [00:08<00:03,  1.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09761 0.04282 0.03461 58 640:  74%|███████▎  | 14/19 [00:09<00:03,  1.28it/s] 2/99 7.21G 0.09761 0.04282 0.03461 58 640:  79%|███████▉  | 15/19 [00:09<00:03,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09737 0.04325 0.03469 78 640:  79%|███████▉  | 15/19 [00:10<00:03,  1.03it/s]2/99 7.21G 0.09737 0.04325 0.03469 78 640:  84%|████████▍ | 16/19 [00:10<00:02,  1.08it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.0978 0.04517 0.03465 194 640:  84%|████████▍ | 16/19 [00:12<00:02,  1.08it/s]2/99 7.21G 0.0978 0.04517 0.03465 194 640:  89%|████████▉ | 17/19 [00:12<00:02,  1.30s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.0976 0.04562 0.03466 86 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.30s/it] 2/99 7.21G 0.0976 0.04562 0.03466 86 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.09s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
2/99 7.21G 0.09776 0.0456 0.03468 82 640:  95%|█████████▍| 18/19 [00:20<00:01,  1.09s/it]2/99 7.21G 0.09776 0.0456 0.03468 82 640: 100%|██████████| 19/19 [00:20<00:00,  2.92s/it]2/99 7.21G 0.09776 0.0456 0.03468 82 640: 100%|██████████| 19/19 [00:20<00:00,  1.09s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.18s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.44s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
                   all         55        256    0.00171      0.309    0.00259   0.000533
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.095 0.03831 0.03513 59 640:   0%|          | 0/19 [00:00<?, ?it/s]3/99 7.23G 0.095 0.03831 0.03513 59 640:   5%|▌         | 1/19 [00:00<00:03,  4.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09476 0.04089 0.03515 73 640:   5%|▌         | 1/19 [00:00<00:03,  4.80it/s]3/99 7.23G 0.09476 0.04089 0.03515 73 640:  11%|█         | 2/19 [00:00<00:03,  5.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09587 0.04179 0.03462 84 640:  11%|█         | 2/19 [00:00<00:03,  5.25it/s]3/99 7.23G 0.09587 0.04179 0.03462 84 640:  16%|█▌        | 3/19 [00:00<00:04,  3.79it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09566 0.04335 0.03526 75 640:  16%|█▌        | 3/19 [00:00<00:04,  3.79it/s]3/99 7.23G 0.09566 0.04335 0.03526 75 640:  21%|██        | 4/19 [00:00<00:03,  4.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09548 0.04312 0.03502 66 640:  21%|██        | 4/19 [00:01<00:03,  4.38it/s]3/99 7.23G 0.09548 0.04312 0.03502 66 640:  26%|██▋       | 5/19 [00:01<00:02,  4.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09506 0.04159 0.03471 58 640:  26%|██▋       | 5/19 [00:01<00:02,  4.78it/s]3/99 7.23G 0.09506 0.04159 0.03471 58 640:  32%|███▏      | 6/19 [00:01<00:03,  3.86it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09599 0.04404 0.0347 119 640:  32%|███▏      | 6/19 [00:01<00:03,  3.86it/s]3/99 7.23G 0.09599 0.04404 0.0347 119 640:  37%|███▋      | 7/19 [00:01<00:02,  4.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09527 0.044 0.03457 67 640:  37%|███▋      | 7/19 [00:01<00:02,  4.32it/s]  3/99 7.23G 0.09527 0.044 0.03457 67 640:  42%|████▏     | 8/19 [00:01<00:02,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09546 0.04441 0.03476 80 640:  42%|████▏     | 8/19 [00:01<00:02,  4.69it/s]3/99 7.23G 0.09546 0.04441 0.03476 80 640:  47%|████▋     | 9/19 [00:01<00:02,  4.97it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.0953 0.04386 0.03484 59 640:  47%|████▋     | 9/19 [00:02<00:02,  4.97it/s] 3/99 7.23G 0.0953 0.04386 0.03484 59 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09539 0.04406 0.03477 91 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.18it/s]3/99 7.23G 0.09539 0.04406 0.03477 91 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09534 0.04504 0.03476 93 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.33it/s]3/99 7.23G 0.09534 0.04504 0.03476 93 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09495 0.04425 0.03467 50 640:  63%|██████▎   | 12/19 [00:02<00:01,  4.88it/s]3/99 7.23G 0.09495 0.04425 0.03467 50 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09481 0.04436 0.0345 75 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.40it/s] 3/99 7.23G 0.09481 0.04436 0.0345 75 640:  74%|███████▎  | 14/19 [00:03<00:02,  2.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09478 0.04484 0.03444 85 640:  74%|███████▎  | 14/19 [00:04<00:02,  2.43it/s]3/99 7.23G 0.09478 0.04484 0.03444 85 640:  79%|███████▉  | 15/19 [00:04<00:02,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09489 0.0451 0.03434 88 640:  79%|███████▉  | 15/19 [00:12<00:02,  1.82it/s] 3/99 7.23G 0.09489 0.0451 0.03434 88 640:  84%|████████▍ | 16/19 [00:12<00:07,  2.63s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09439 0.04442 0.03426 45 640:  84%|████████▍ | 16/19 [00:13<00:07,  2.63s/it]3/99 7.23G 0.09439 0.04442 0.03426 45 640:  89%|████████▉ | 17/19 [00:13<00:04,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09435 0.04416 0.03427 66 640:  89%|████████▉ | 17/19 [00:13<00:04,  2.29s/it]3/99 7.23G 0.09435 0.04416 0.03427 66 640:  95%|█████████▍| 18/19 [00:13<00:01,  1.69s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
3/99 7.23G 0.09428 0.04447 0.03421 83 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.69s/it]3/99 7.23G 0.09428 0.04447 0.03421 83 640: 100%|██████████| 19/19 [00:14<00:00,  1.32s/it]3/99 7.23G 0.09428 0.04447 0.03421 83 640: 100%|██████████| 19/19 [00:14<00:00,  1.33it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.11s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.41s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
                   all         55        256    0.00212      0.272    0.00459   0.000853
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09395 0.04186 0.03164 69 640:   0%|          | 0/19 [00:00<?, ?it/s]4/99 7.25G 0.09395 0.04186 0.03164 69 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09221 0.04156 0.03368 65 640:   5%|▌         | 1/19 [00:00<00:09,  1.82it/s]4/99 7.25G 0.09221 0.04156 0.03368 65 640:  11%|█         | 2/19 [00:00<00:07,  2.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09204 0.0436 0.0337 77 640:  11%|█         | 2/19 [00:01<00:07,  2.30it/s]  4/99 7.25G 0.09204 0.0436 0.0337 77 640:  16%|█▌        | 3/19 [00:01<00:06,  2.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09208 0.04044 0.03385 47 640:  16%|█▌        | 3/19 [00:01<00:06,  2.44it/s]4/99 7.25G 0.09208 0.04044 0.03385 47 640:  21%|██        | 4/19 [00:01<00:05,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09164 0.04046 0.03423 57 640:  21%|██        | 4/19 [00:01<00:05,  2.54it/s]4/99 7.25G 0.09164 0.04046 0.03423 57 640:  26%|██▋       | 5/19 [00:01<00:04,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09094 0.03898 0.0338 46 640:  26%|██▋       | 5/19 [00:02<00:04,  2.83it/s] 4/99 7.25G 0.09094 0.03898 0.0338 46 640:  32%|███▏      | 6/19 [00:02<00:03,  3.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09166 0.04111 0.03362 102 640:  32%|███▏      | 6/19 [00:02<00:03,  3.27it/s]4/99 7.25G 0.09166 0.04111 0.03362 102 640:  37%|███▋      | 7/19 [00:02<00:03,  3.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09146 0.04105 0.03345 65 640:  37%|███▋      | 7/19 [00:02<00:03,  3.22it/s] 4/99 7.25G 0.09146 0.04105 0.03345 65 640:  42%|████▏     | 8/19 [00:02<00:03,  3.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09149 0.04347 0.03369 99 640:  42%|████▏     | 8/19 [00:03<00:03,  3.14it/s]4/99 7.25G 0.09149 0.04347 0.03369 99 640:  47%|████▋     | 9/19 [00:03<00:03,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.0921 0.04349 0.03365 74 640:  47%|████▋     | 9/19 [00:03<00:03,  2.96it/s] 4/99 7.25G 0.0921 0.04349 0.03365 74 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09227 0.04427 0.0335 87 640:  53%|█████▎    | 10/19 [00:03<00:02,  3.30it/s]4/99 7.25G 0.09227 0.04427 0.0335 87 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09252 0.04577 0.03333 108 640:  58%|█████▊    | 11/19 [00:03<00:02,  3.63it/s]4/99 7.25G 0.09252 0.04577 0.03333 108 640:  63%|██████▎   | 12/19 [00:03<00:01,  3.92it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09252 0.04549 0.03334 73 640:  63%|██████▎   | 12/19 [00:09<00:01,  3.92it/s] 4/99 7.25G 0.09252 0.04549 0.03334 73 640:  68%|██████▊   | 13/19 [00:09<00:12,  2.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.0923 0.04456 0.03346 50 640:  68%|██████▊   | 13/19 [00:12<00:12,  2.01s/it] 4/99 7.25G 0.0923 0.04456 0.03346 50 640:  74%|███████▎  | 14/19 [00:12<00:10,  2.14s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09285 0.04609 0.03335 146 640:  74%|███████▎  | 14/19 [00:12<00:10,  2.14s/it]4/99 7.25G 0.09285 0.04609 0.03335 146 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.59s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09311 0.04632 0.03335 96 640:  79%|███████▉  | 15/19 [00:13<00:06,  1.59s/it] 4/99 7.25G 0.09311 0.04632 0.03335 96 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09333 0.04757 0.03324 121 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.27s/it]4/99 7.25G 0.09333 0.04757 0.03324 121 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.0932 0.04758 0.03328 75 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.07s/it]  4/99 7.25G 0.0932 0.04758 0.03328 75 640:  95%|█████████▍| 18/19 [00:14<00:00,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
4/99 7.25G 0.09311 0.04788 0.03321 94 640:  95%|█████████▍| 18/19 [00:16<00:00,  1.01it/s]4/99 7.25G 0.09311 0.04788 0.03321 94 640: 100%|██████████| 19/19 [00:16<00:00,  1.28s/it]4/99 7.25G 0.09311 0.04788 0.03321 94 640: 100%|██████████| 19/19 [00:16<00:00,  1.15it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.99s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
                   all         55        256    0.00164      0.266    0.00161   0.000589
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09013 0.03837 0.03351 55 640:   0%|          | 0/19 [00:00<?, ?it/s]5/99 7.25G 0.09013 0.03837 0.03351 55 640:   5%|▌         | 1/19 [00:00<00:07,  2.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.08999 0.03969 0.03302 62 640:   5%|▌         | 1/19 [00:00<00:07,  2.46it/s]5/99 7.25G 0.08999 0.03969 0.03302 62 640:  11%|█         | 2/19 [00:00<00:04,  3.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09318 0.04395 0.03258 131 640:  11%|█         | 2/19 [00:00<00:04,  3.65it/s]5/99 7.25G 0.09318 0.04395 0.03258 131 640:  16%|█▌        | 3/19 [00:00<00:03,  4.38it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09303 0.04539 0.03287 89 640:  16%|█▌        | 3/19 [00:01<00:03,  4.38it/s] 5/99 7.25G 0.09303 0.04539 0.03287 89 640:  21%|██        | 4/19 [00:01<00:03,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09253 0.04269 0.03264 48 640:  21%|██        | 4/19 [00:01<00:03,  3.87it/s]5/99 7.25G 0.09253 0.04269 0.03264 48 640:  26%|██▋       | 5/19 [00:01<00:03,  3.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.0922 0.0434 0.03257 73 640:  26%|██▋       | 5/19 [00:01<00:03,  3.80it/s]  5/99 7.25G 0.0922 0.0434 0.03257 73 640:  32%|███▏      | 6/19 [00:01<00:03,  4.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09378 0.04868 0.0325 190 640:  32%|███▏      | 6/19 [00:01<00:03,  4.29it/s]5/99 7.25G 0.09378 0.04868 0.0325 190 640:  37%|███▋      | 7/19 [00:01<00:02,  4.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09446 0.04916 0.03246 100 640:  37%|███▋      | 7/19 [00:01<00:02,  4.37it/s]5/99 7.25G 0.09446 0.04916 0.03246 100 640:  42%|████▏     | 8/19 [00:01<00:02,  4.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09437 0.04863 0.03231 72 640:  42%|████▏     | 8/19 [00:02<00:02,  4.44it/s] 5/99 7.25G 0.09437 0.04863 0.03231 72 640:  47%|████▋     | 9/19 [00:02<00:02,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09436 0.04701 0.03229 56 640:  47%|████▋     | 9/19 [00:06<00:02,  4.55it/s]5/99 7.25G 0.09436 0.04701 0.03229 56 640:  53%|█████▎    | 10/19 [00:06<00:14,  1.56s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09419 0.04666 0.03255 66 640:  53%|█████▎    | 10/19 [00:07<00:14,  1.56s/it]5/99 7.25G 0.09419 0.04666 0.03255 66 640:  58%|█████▊    | 11/19 [00:07<00:10,  1.33s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09395 0.04638 0.03264 68 640:  58%|█████▊    | 11/19 [00:07<00:10,  1.33s/it]5/99 7.25G 0.09395 0.04638 0.03264 68 640:  63%|██████▎   | 12/19 [00:07<00:06,  1.01it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09447 0.0475 0.03275 123 640:  63%|██████▎   | 12/19 [00:07<00:06,  1.01it/s]5/99 7.25G 0.09447 0.0475 0.03275 123 640:  68%|██████▊   | 13/19 [00:07<00:04,  1.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09444 0.04815 0.0326 101 640:  68%|██████▊   | 13/19 [00:08<00:04,  1.33it/s]5/99 7.25G 0.09444 0.04815 0.0326 101 640:  74%|███████▎  | 14/19 [00:08<00:02,  1.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09403 0.04749 0.03261 56 640:  74%|███████▎  | 14/19 [00:08<00:02,  1.71it/s]5/99 7.25G 0.09403 0.04749 0.03261 56 640:  79%|███████▉  | 15/19 [00:08<00:02,  1.59it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09377 0.04722 0.03258 66 640:  79%|███████▉  | 15/19 [00:10<00:02,  1.59it/s]5/99 7.25G 0.09377 0.04722 0.03258 66 640:  84%|████████▍ | 16/19 [00:10<00:03,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09364 0.04772 0.03253 96 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.01s/it]5/99 7.25G 0.09364 0.04772 0.03253 96 640:  89%|████████▉ | 17/19 [00:13<00:02,  1.45s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09394 0.0484 0.03246 126 640:  89%|████████▉ | 17/19 [00:22<00:02,  1.45s/it]5/99 7.25G 0.09394 0.0484 0.03246 126 640:  95%|█████████▍| 18/19 [00:22<00:03,  3.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
5/99 7.25G 0.09389 0.04846 0.03244 87 640:  95%|█████████▍| 18/19 [00:22<00:03,  3.68s/it]5/99 7.25G 0.09389 0.04846 0.03244 87 640: 100%|██████████| 19/19 [00:22<00:00,  2.63s/it]5/99 7.25G 0.09389 0.04846 0.03244 87 640: 100%|██████████| 19/19 [00:22<00:00,  1.17s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.13s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.95s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.72s/it]
                   all         55        256    0.00377      0.311     0.0023   0.000572
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09661 0.04652 0.03239 96 640:   0%|          | 0/19 [00:00<?, ?it/s]6/99 7.25G 0.09661 0.04652 0.03239 96 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09645 0.04598 0.03213 83 640:   5%|▌         | 1/19 [00:00<00:07,  2.25it/s]6/99 7.25G 0.09645 0.04598 0.03213 83 640:  11%|█         | 2/19 [00:00<00:04,  3.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09556 0.04803 0.03269 90 640:  11%|█         | 2/19 [00:00<00:04,  3.46it/s]6/99 7.25G 0.09556 0.04803 0.03269 90 640:  16%|█▌        | 3/19 [00:00<00:03,  4.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09282 0.04545 0.03289 53 640:  16%|█▌        | 3/19 [00:00<00:03,  4.18it/s]6/99 7.25G 0.09282 0.04545 0.03289 53 640:  21%|██        | 4/19 [00:00<00:03,  4.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09221 0.04482 0.03255 67 640:  21%|██        | 4/19 [00:01<00:03,  4.56it/s]6/99 7.25G 0.09221 0.04482 0.03255 67 640:  26%|██▋       | 5/19 [00:01<00:02,  4.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09302 0.05025 0.03229 145 640:  26%|██▋       | 5/19 [00:01<00:02,  4.88it/s]6/99 7.25G 0.09302 0.05025 0.03229 145 640:  32%|███▏      | 6/19 [00:01<00:02,  5.09it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09236 0.0492 0.0323 63 640:  32%|███▏      | 6/19 [00:01<00:02,  5.09it/s]   6/99 7.25G 0.09236 0.0492 0.0323 63 640:  37%|███▋      | 7/19 [00:01<00:02,  5.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09298 0.05044 0.0321 123 640:  37%|███▋      | 7/19 [00:01<00:02,  5.19it/s]6/99 7.25G 0.09298 0.05044 0.0321 123 640:  42%|████▏     | 8/19 [00:01<00:02,  4.00it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09287 0.05102 0.0322 98 640:  42%|████▏     | 8/19 [00:02<00:02,  4.00it/s] 6/99 7.25G 0.09287 0.05102 0.0322 98 640:  47%|████▋     | 9/19 [00:02<00:03,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.0931 0.05018 0.03227 73 640:  47%|████▋     | 9/19 [00:02<00:03,  2.68it/s]6/99 7.25G 0.0931 0.05018 0.03227 73 640:  53%|█████▎    | 10/19 [00:02<00:03,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09325 0.05015 0.03207 87 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.63it/s]6/99 7.25G 0.09325 0.05015 0.03207 87 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09275 0.05076 0.0321 97 640:  58%|█████▊    | 11/19 [00:03<00:03,  2.63it/s] 6/99 7.25G 0.09275 0.05076 0.0321 97 640:  63%|██████▎   | 12/19 [00:03<00:02,  2.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09224 0.04949 0.0321 48 640:  63%|██████▎   | 12/19 [00:04<00:02,  2.65it/s]6/99 7.25G 0.09224 0.04949 0.0321 48 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09226 0.05076 0.032 114 640:  68%|██████▊   | 13/19 [00:04<00:02,  2.67it/s]6/99 7.25G 0.09226 0.05076 0.032 114 640:  74%|███████▎  | 14/19 [00:04<00:01,  2.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09226 0.05086 0.03194 89 640:  74%|███████▎  | 14/19 [00:13<00:01,  2.68it/s]6/99 7.25G 0.09226 0.05086 0.03194 89 640:  79%|███████▉  | 15/19 [00:13<00:11,  2.91s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09213 0.05019 0.03179 61 640:  79%|███████▉  | 15/19 [00:13<00:11,  2.91s/it]6/99 7.25G 0.09213 0.05019 0.03179 61 640:  84%|████████▍ | 16/19 [00:13<00:06,  2.23s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09205 0.04926 0.0318 50 640:  84%|████████▍ | 16/19 [00:14<00:06,  2.23s/it] 6/99 7.25G 0.09205 0.04926 0.0318 50 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.092 0.04967 0.03182 90 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.87s/it] 6/99 7.25G 0.092 0.04967 0.03182 90 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.46s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
6/99 7.25G 0.09209 0.04994 0.03178 89 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.46s/it]6/99 7.25G 0.09209 0.04994 0.03178 89 640: 100%|██████████| 19/19 [00:15<00:00,  1.08s/it]6/99 7.25G 0.09209 0.04994 0.03178 89 640: 100%|██████████| 19/19 [00:15<00:00,  1.22it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.10s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
                   all         55        256    0.00278      0.308    0.00325    0.00072
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09128 0.04081 0.03081 69 640:   0%|          | 0/19 [00:00<?, ?it/s]7/99 7.25G 0.09128 0.04081 0.03081 69 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09356 0.05166 0.03098 124 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]7/99 7.25G 0.09356 0.05166 0.03098 124 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09252 0.05497 0.0314 101 640:  11%|█         | 2/19 [00:00<00:02,  5.71it/s] 7/99 7.25G 0.09252 0.05497 0.0314 101 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09199 0.05245 0.03166 72 640:  16%|█▌        | 3/19 [00:00<00:02,  5.72it/s]7/99 7.25G 0.09199 0.05245 0.03166 72 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09363 0.05987 0.0316 187 640:  21%|██        | 4/19 [00:00<00:02,  5.73it/s]7/99 7.25G 0.09363 0.05987 0.0316 187 640:  26%|██▋       | 5/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09275 0.05474 0.03125 46 640:  26%|██▋       | 5/19 [00:01<00:02,  5.72it/s]7/99 7.25G 0.09275 0.05474 0.03125 46 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09303 0.05385 0.0312 85 640:  32%|███▏      | 6/19 [00:01<00:02,  5.73it/s] 7/99 7.25G 0.09303 0.05385 0.0312 85 640:  37%|███▋      | 7/19 [00:01<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09338 0.05135 0.03074 59 640:  37%|███▋      | 7/19 [00:01<00:02,  5.73it/s]7/99 7.25G 0.09338 0.05135 0.03074 59 640:  42%|████▏     | 8/19 [00:01<00:02,  5.43it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09412 0.05264 0.03046 143 640:  42%|████▏     | 8/19 [00:01<00:02,  5.43it/s]7/99 7.25G 0.09412 0.05264 0.03046 143 640:  47%|████▋     | 9/19 [00:01<00:01,  5.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09378 0.05007 0.03056 43 640:  47%|████▋     | 9/19 [00:01<00:01,  5.46it/s] 7/99 7.25G 0.09378 0.05007 0.03056 43 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.50it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09277 0.04833 0.03047 43 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.50it/s]7/99 7.25G 0.09277 0.04833 0.03047 43 640:  58%|█████▊    | 11/19 [00:01<00:01,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09226 0.04747 0.03033 56 640:  58%|█████▊    | 11/19 [00:08<00:01,  5.53it/s]7/99 7.25G 0.09226 0.04747 0.03033 56 640:  63%|██████▎   | 12/19 [00:08<00:13,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09265 0.04933 0.03024 132 640:  63%|██████▎   | 12/19 [00:10<00:13,  1.99s/it]7/99 7.25G 0.09265 0.04933 0.03024 132 640:  68%|██████▊   | 13/19 [00:10<00:12,  2.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0926 0.0487 0.03017 67 640:  68%|██████▊   | 13/19 [00:10<00:12,  2.00s/it]   7/99 7.25G 0.0926 0.0487 0.03017 67 640:  74%|███████▎  | 14/19 [00:10<00:07,  1.54s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09247 0.04898 0.03031 84 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.54s/it]7/99 7.25G 0.09247 0.04898 0.03031 84 640:  79%|███████▉  | 15/19 [00:11<00:05,  1.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09218 0.04861 0.03051 64 640:  79%|███████▉  | 15/19 [00:11<00:05,  1.37s/it]7/99 7.25G 0.09218 0.04861 0.03051 64 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09245 0.04896 0.03041 113 640:  84%|████████▍ | 16/19 [00:14<00:03,  1.01s/it]7/99 7.25G 0.09245 0.04896 0.03041 113 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.09174 0.04791 0.0305 38 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.62s/it]  7/99 7.25G 0.09174 0.04791 0.0305 38 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
7/99 7.25G 0.0914 0.04764 0.03048 59 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.18s/it]7/99 7.25G 0.0914 0.04764 0.03048 59 640: 100%|██████████| 19/19 [00:16<00:00,  1.17s/it]7/99 7.25G 0.0914 0.04764 0.03048 59 640: 100%|██████████| 19/19 [00:16<00:00,  1.18it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.99s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.36s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
                   all         55        256    0.00507      0.414    0.00498    0.00112
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09411 0.05702 0.03123 101 640:   0%|          | 0/19 [00:00<?, ?it/s]8/99 7.25G 0.09411 0.05702 0.03123 101 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08884 0.04283 0.03098 38 640:   5%|▌         | 1/19 [00:00<00:03,  5.67it/s] 8/99 7.25G 0.08884 0.04283 0.03098 38 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08877 0.0416 0.0311 58 640:  11%|█         | 2/19 [00:00<00:02,  5.73it/s]  8/99 7.25G 0.08877 0.0416 0.0311 58 640:  16%|█▌        | 3/19 [00:00<00:03,  4.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.0885 0.03994 0.03104 53 640:  16%|█▌        | 3/19 [00:00<00:03,  4.63it/s]8/99 7.25G 0.0885 0.03994 0.03104 53 640:  21%|██        | 4/19 [00:00<00:04,  3.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.08764 0.03988 0.03096 59 640:  21%|██        | 4/19 [00:01<00:04,  3.58it/s]8/99 7.25G 0.08764 0.03988 0.03096 59 640:  26%|██▋       | 5/19 [00:01<00:03,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09005 0.04321 0.03068 146 640:  26%|██▋       | 5/19 [00:01<00:03,  4.14it/s]8/99 7.25G 0.09005 0.04321 0.03068 146 640:  32%|███▏      | 6/19 [00:01<00:02,  4.46it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09051 0.04341 0.03077 76 640:  32%|███▏      | 6/19 [00:01<00:02,  4.46it/s] 8/99 7.25G 0.09051 0.04341 0.03077 76 640:  37%|███▋      | 7/19 [00:01<00:02,  4.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09048 0.04276 0.03068 66 640:  37%|███▋      | 7/19 [00:01<00:02,  4.60it/s]8/99 7.25G 0.09048 0.04276 0.03068 66 640:  42%|████▏     | 8/19 [00:01<00:02,  4.80it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09119 0.04959 0.03064 209 640:  42%|████▏     | 8/19 [00:05<00:02,  4.80it/s]8/99 7.25G 0.09119 0.04959 0.03064 209 640:  47%|████▋     | 9/19 [00:05<00:12,  1.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09059 0.04762 0.03041 45 640:  47%|████▋     | 9/19 [00:05<00:12,  1.24s/it] 8/99 7.25G 0.09059 0.04762 0.03041 45 640:  53%|█████▎    | 10/19 [00:05<00:08,  1.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09028 0.04727 0.03043 68 640:  53%|█████▎    | 10/19 [00:06<00:08,  1.03it/s]8/99 7.25G 0.09028 0.04727 0.03043 68 640:  58%|█████▊    | 11/19 [00:06<00:07,  1.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09084 0.04886 0.03039 153 640:  58%|█████▊    | 11/19 [00:07<00:07,  1.06it/s]8/99 7.25G 0.09084 0.04886 0.03039 153 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09073 0.04816 0.03068 68 640:  63%|██████▎   | 12/19 [00:07<00:05,  1.19it/s] 8/99 7.25G 0.09073 0.04816 0.03068 68 640:  68%|██████▊   | 13/19 [00:07<00:04,  1.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09076 0.04862 0.03076 88 640:  68%|██████▊   | 13/19 [00:09<00:04,  1.42it/s]8/99 7.25G 0.09076 0.04862 0.03076 88 640:  74%|███████▎  | 14/19 [00:09<00:05,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09094 0.04811 0.03076 69 640:  74%|███████▎  | 14/19 [00:09<00:05,  1.13s/it]8/99 7.25G 0.09094 0.04811 0.03076 69 640:  79%|███████▉  | 15/19 [00:09<00:03,  1.19it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09087 0.04796 0.03064 76 640:  79%|███████▉  | 15/19 [00:11<00:03,  1.19it/s]8/99 7.25G 0.09087 0.04796 0.03064 76 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.01s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09117 0.04841 0.03061 111 640:  84%|████████▍ | 16/19 [00:20<00:03,  1.01s/it]8/99 7.25G 0.09117 0.04841 0.03061 111 640:  89%|████████▉ | 17/19 [00:20<00:07,  3.53s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.09116 0.04806 0.03051 69 640:  89%|████████▉ | 17/19 [00:21<00:07,  3.53s/it] 8/99 7.25G 0.09116 0.04806 0.03051 69 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.61s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
8/99 7.25G 0.0908 0.04707 0.03041 41 640:  95%|█████████▍| 18/19 [00:21<00:02,  2.61s/it] 8/99 7.25G 0.0908 0.04707 0.03041 41 640: 100%|██████████| 19/19 [00:21<00:00,  1.93s/it]8/99 7.25G 0.0908 0.04707 0.03041 41 640: 100%|██████████| 19/19 [00:21<00:00,  1.13s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.97s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
                   all         55        256    0.00618      0.379    0.00587    0.00138
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08971 0.04508 0.03123 75 640:   0%|          | 0/19 [00:00<?, ?it/s]9/99 7.26G 0.08971 0.04508 0.03123 75 640:   5%|▌         | 1/19 [00:00<00:14,  1.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08989 0.05861 0.03126 121 640:   5%|▌         | 1/19 [00:00<00:14,  1.26it/s]9/99 7.26G 0.08989 0.05861 0.03126 121 640:  11%|█         | 2/19 [00:00<00:07,  2.29it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09073 0.05428 0.03071 80 640:  11%|█         | 2/19 [00:01<00:07,  2.29it/s] 9/99 7.26G 0.09073 0.05428 0.03071 80 640:  16%|█▌        | 3/19 [00:01<00:05,  2.81it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09008 0.05037 0.03046 58 640:  16%|█▌        | 3/19 [00:01<00:05,  2.81it/s]9/99 7.26G 0.09008 0.05037 0.03046 58 640:  21%|██        | 4/19 [00:01<00:04,  3.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09065 0.04717 0.0304 58 640:  21%|██        | 4/19 [00:01<00:04,  3.14it/s] 9/99 7.26G 0.09065 0.04717 0.0304 58 640:  26%|██▋       | 5/19 [00:01<00:03,  3.76it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08943 0.04649 0.03026 62 640:  26%|██▋       | 5/19 [00:01<00:03,  3.76it/s]9/99 7.26G 0.08943 0.04649 0.03026 62 640:  32%|███▏      | 6/19 [00:01<00:03,  3.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09014 0.04777 0.02996 100 640:  32%|███▏      | 6/19 [00:02<00:03,  3.64it/s]9/99 7.26G 0.09014 0.04777 0.02996 100 640:  37%|███▋      | 7/19 [00:02<00:03,  3.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09084 0.04764 0.02977 99 640:  37%|███▋      | 7/19 [00:02<00:03,  3.45it/s] 9/99 7.26G 0.09084 0.04764 0.02977 99 640:  42%|████▏     | 8/19 [00:02<00:02,  3.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09036 0.04627 0.0299 53 640:  42%|████▏     | 8/19 [00:02<00:02,  3.78it/s] 9/99 7.26G 0.09036 0.04627 0.0299 53 640:  47%|████▋     | 9/19 [00:02<00:02,  4.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08962 0.04459 0.0301 42 640:  47%|████▋     | 9/19 [00:02<00:02,  4.06it/s]9/99 7.26G 0.08962 0.04459 0.0301 42 640:  53%|█████▎    | 10/19 [00:02<00:02,  4.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08966 0.04361 0.03028 50 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.27it/s]9/99 7.26G 0.08966 0.04361 0.03028 50 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08942 0.04398 0.03055 72 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.42it/s]9/99 7.26G 0.08942 0.04398 0.03055 72 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08928 0.04572 0.03084 108 640:  63%|██████▎   | 12/19 [00:03<00:01,  4.54it/s]9/99 7.26G 0.08928 0.04572 0.03084 108 640:  68%|██████▊   | 13/19 [00:03<00:01,  4.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08973 0.04562 0.03083 74 640:  68%|██████▊   | 13/19 [00:12<00:01,  4.55it/s] 9/99 7.26G 0.08973 0.04562 0.03083 74 640:  74%|███████▎  | 14/19 [00:12<00:15,  3.00s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09006 0.04641 0.03067 105 640:  74%|███████▎  | 14/19 [00:13<00:15,  3.00s/it]9/99 7.26G 0.09006 0.04641 0.03067 105 640:  79%|███████▉  | 15/19 [00:13<00:08,  2.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09024 0.04626 0.03049 81 640:  79%|███████▉  | 15/19 [00:14<00:08,  2.25s/it] 9/99 7.26G 0.09024 0.04626 0.03049 81 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.84s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.08998 0.04567 0.03032 54 640:  84%|████████▍ | 16/19 [00:15<00:05,  1.84s/it]9/99 7.26G 0.08998 0.04567 0.03032 54 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.70s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09004 0.04508 0.03029 58 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.70s/it]9/99 7.26G 0.09004 0.04508 0.03029 58 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.24s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
9/99 7.26G 0.09004 0.04501 0.03018 70 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.24s/it]9/99 7.26G 0.09004 0.04501 0.03018 70 640: 100%|██████████| 19/19 [00:16<00:00,  1.09it/s]9/99 7.26G 0.09004 0.04501 0.03018 70 640: 100%|██████████| 19/19 [00:16<00:00,  1.18it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.45s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
                   all         55        256    0.00922      0.398    0.00926    0.00224
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08835 0.04597 0.02949 65 640:   0%|          | 0/19 [00:00<?, ?it/s]10/99 7.28G 0.08835 0.04597 0.02949 65 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08837 0.04003 0.02795 53 640:   5%|▌         | 1/19 [00:00<00:03,  5.66it/s]10/99 7.28G 0.08837 0.04003 0.02795 53 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08817 0.03747 0.02846 51 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]10/99 7.28G 0.08817 0.03747 0.02846 51 640:  16%|█▌        | 3/19 [00:00<00:04,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08646 0.03838 0.0291 55 640:  16%|█▌        | 3/19 [00:01<00:04,  3.87it/s] 10/99 7.28G 0.08646 0.03838 0.0291 55 640:  21%|██        | 4/19 [00:01<00:04,  3.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08658 0.03959 0.02922 74 640:  21%|██        | 4/19 [00:01<00:04,  3.28it/s]10/99 7.28G 0.08658 0.03959 0.02922 74 640:  26%|██▋       | 5/19 [00:01<00:04,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08776 0.04019 0.02915 78 640:  26%|██▋       | 5/19 [00:01<00:04,  2.83it/s]10/99 7.28G 0.08776 0.04019 0.02915 78 640:  32%|███▏      | 6/19 [00:01<00:03,  3.28it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08821 0.04094 0.02898 76 640:  32%|███▏      | 6/19 [00:01<00:03,  3.28it/s]10/99 7.28G 0.08821 0.04094 0.02898 76 640:  37%|███▋      | 7/19 [00:01<00:03,  3.63it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08779 0.03961 0.02899 43 640:  37%|███▋      | 7/19 [00:02<00:03,  3.63it/s]10/99 7.28G 0.08779 0.03961 0.02899 43 640:  42%|████▏     | 8/19 [00:02<00:03,  3.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08786 0.03945 0.02964 61 640:  42%|████▏     | 8/19 [00:02<00:03,  3.58it/s]10/99 7.28G 0.08786 0.03945 0.02964 61 640:  47%|████▋     | 9/19 [00:02<00:03,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08823 0.03941 0.02964 63 640:  47%|████▋     | 9/19 [00:03<00:03,  2.96it/s]10/99 7.28G 0.08823 0.03941 0.02964 63 640:  53%|█████▎    | 10/19 [00:03<00:03,  2.88it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08871 0.03936 0.02974 58 640:  53%|█████▎    | 10/19 [00:07<00:03,  2.88it/s]10/99 7.28G 0.08871 0.03936 0.02974 58 640:  58%|█████▊    | 11/19 [00:07<00:12,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08959 0.04082 0.02959 148 640:  58%|█████▊    | 11/19 [00:08<00:12,  1.52s/it]10/99 7.28G 0.08959 0.04082 0.02959 148 640:  63%|██████▎   | 12/19 [00:08<00:09,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08898 0.041 0.02979 61 640:  63%|██████▎   | 12/19 [00:09<00:09,  1.35s/it]   10/99 7.28G 0.08898 0.041 0.02979 61 640:  68%|██████▊   | 13/19 [00:09<00:08,  1.47s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08928 0.04166 0.02978 94 640:  68%|██████▊   | 13/19 [00:10<00:08,  1.47s/it]10/99 7.28G 0.08928 0.04166 0.02978 94 640:  74%|███████▎  | 14/19 [00:10<00:06,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08879 0.04074 0.02987 38 640:  74%|███████▎  | 14/19 [00:10<00:06,  1.27s/it]10/99 7.28G 0.08879 0.04074 0.02987 38 640:  79%|███████▉  | 15/19 [00:10<00:03,  1.07it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08876 0.04028 0.03007 52 640:  79%|███████▉  | 15/19 [00:11<00:03,  1.07it/s]10/99 7.28G 0.08876 0.04028 0.03007 52 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.41it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08864 0.03977 0.03024 46 640:  84%|████████▍ | 16/19 [00:14<00:02,  1.41it/s]10/99 7.28G 0.08864 0.03977 0.03024 46 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08897 0.04028 0.03009 92 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.43s/it]10/99 7.28G 0.08897 0.04028 0.03009 92 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.13s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
10/99 7.28G 0.08912 0.0412 0.03007 107 640:  95%|█████████▍| 18/19 [00:22<00:01,  1.13s/it]10/99 7.28G 0.08912 0.0412 0.03007 107 640: 100%|██████████| 19/19 [00:22<00:00,  3.25s/it]10/99 7.28G 0.08912 0.0412 0.03007 107 640: 100%|██████████| 19/19 [00:22<00:00,  1.20s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]
                   all         55        256    0.00856      0.397     0.0267    0.00418
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.0855 0.03687 0.03187 52 640:   0%|          | 0/19 [00:01<?, ?it/s]11/99 7.3G 0.0855 0.03687 0.03187 52 640:   5%|▌         | 1/19 [00:01<00:30,  1.68s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08589 0.03473 0.02997 48 640:   5%|▌         | 1/19 [00:01<00:30,  1.68s/it]11/99 7.3G 0.08589 0.03473 0.02997 48 640:  11%|█         | 2/19 [00:01<00:13,  1.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08786 0.04155 0.03005 92 640:  11%|█         | 2/19 [00:02<00:13,  1.24it/s]11/99 7.3G 0.08786 0.04155 0.03005 92 640:  16%|█▌        | 3/19 [00:02<00:09,  1.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08853 0.03775 0.03 41 640:  16%|█▌        | 3/19 [00:02<00:09,  1.66it/s]   11/99 7.3G 0.08853 0.03775 0.03 41 640:  21%|██        | 4/19 [00:02<00:07,  2.03it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08868 0.03845 0.03042 66 640:  21%|██        | 4/19 [00:02<00:07,  2.03it/s]11/99 7.3G 0.08868 0.03845 0.03042 66 640:  26%|██▋       | 5/19 [00:02<00:05,  2.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.0891 0.04063 0.02999 91 640:  26%|██▋       | 5/19 [00:02<00:05,  2.64it/s] 11/99 7.3G 0.0891 0.04063 0.02999 91 640:  32%|███▏      | 6/19 [00:02<00:04,  3.04it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08937 0.04309 0.03015 95 640:  32%|███▏      | 6/19 [00:03<00:04,  3.04it/s]11/99 7.3G 0.08937 0.04309 0.03015 95 640:  37%|███▋      | 7/19 [00:03<00:03,  3.16it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08973 0.04495 0.02989 106 640:  37%|███▋      | 7/19 [00:03<00:03,  3.16it/s]11/99 7.3G 0.08973 0.04495 0.02989 106 640:  42%|████▏     | 8/19 [00:03<00:02,  3.68it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08975 0.04508 0.02978 73 640:  42%|████▏     | 8/19 [00:03<00:02,  3.68it/s] 11/99 7.3G 0.08975 0.04508 0.02978 73 640:  47%|████▋     | 9/19 [00:03<00:02,  4.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08971 0.04518 0.02981 71 640:  47%|████▋     | 9/19 [00:03<00:02,  4.14it/s]11/99 7.3G 0.08971 0.04518 0.02981 71 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08984 0.04652 0.02968 101 640:  53%|█████▎    | 10/19 [00:03<00:01,  4.54it/s]11/99 7.3G 0.08984 0.04652 0.02968 101 640:  58%|█████▊    | 11/19 [00:03<00:01,  4.82it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08981 0.04542 0.03007 49 640:  58%|█████▊    | 11/19 [00:04<00:01,  4.82it/s] 11/99 7.3G 0.08981 0.04542 0.03007 49 640:  63%|██████▎   | 12/19 [00:04<00:01,  5.06it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08981 0.04573 0.03025 77 640:  63%|██████▎   | 12/19 [00:04<00:01,  5.06it/s]11/99 7.3G 0.08981 0.04573 0.03025 77 640:  68%|██████▊   | 13/19 [00:04<00:01,  5.24it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08971 0.04591 0.03009 79 640:  68%|██████▊   | 13/19 [00:04<00:01,  5.24it/s]11/99 7.3G 0.08971 0.04591 0.03009 79 640:  74%|███████▎  | 14/19 [00:04<00:00,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08962 0.04476 0.02984 42 640:  74%|███████▎  | 14/19 [00:04<00:00,  5.35it/s]11/99 7.3G 0.08962 0.04476 0.02984 42 640:  79%|███████▉  | 15/19 [00:04<00:00,  4.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08975 0.04503 0.02974 91 640:  79%|███████▉  | 15/19 [00:14<00:00,  4.23it/s]11/99 7.3G 0.08975 0.04503 0.02974 91 640:  84%|████████▍ | 16/19 [00:14<00:09,  3.04s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08985 0.04521 0.02988 79 640:  84%|████████▍ | 16/19 [00:14<00:09,  3.04s/it]11/99 7.3G 0.08985 0.04521 0.02988 79 640:  89%|████████▉ | 17/19 [00:14<00:04,  2.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08956 0.04551 0.02989 80 640:  89%|████████▉ | 17/19 [00:16<00:04,  2.25s/it]11/99 7.3G 0.08956 0.04551 0.02989 80 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.99s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
11/99 7.3G 0.08972 0.04521 0.0298 75 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.99s/it] 11/99 7.3G 0.08972 0.04521 0.0298 75 640: 100%|██████████| 19/19 [00:16<00:00,  1.45s/it]11/99 7.3G 0.08972 0.04521 0.0298 75 640: 100%|██████████| 19/19 [00:16<00:00,  1.16it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.04s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.96s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
                   all         55        256    0.00704      0.362     0.0063    0.00168
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09152 0.05375 0.02897 100 640:   0%|          | 0/19 [00:00<?, ?it/s]12/99 7.3G 0.09152 0.05375 0.02897 100 640:   5%|▌         | 1/19 [00:00<00:06,  2.58it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09038 0.04939 0.0298 74 640:   5%|▌         | 1/19 [00:00<00:06,  2.58it/s]  12/99 7.3G 0.09038 0.04939 0.0298 74 640:  11%|█         | 2/19 [00:00<00:06,  2.54it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08863 0.05034 0.03047 76 640:  11%|█         | 2/19 [00:01<00:06,  2.54it/s]12/99 7.3G 0.08863 0.05034 0.03047 76 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08864 0.04678 0.02993 58 640:  16%|█▌        | 3/19 [00:01<00:05,  2.75it/s]12/99 7.3G 0.08864 0.04678 0.02993 58 640:  21%|██        | 4/19 [00:01<00:04,  3.30it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08886 0.04649 0.0293 75 640:  21%|██        | 4/19 [00:01<00:04,  3.30it/s] 12/99 7.3G 0.08886 0.04649 0.0293 75 640:  26%|██▋       | 5/19 [00:01<00:03,  3.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08828 0.04574 0.03 60 640:  26%|██▋       | 5/19 [00:01<00:03,  3.83it/s]  12/99 7.3G 0.08828 0.04574 0.03 60 640:  32%|███▏      | 6/19 [00:01<00:03,  4.31it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08898 0.04708 0.02989 93 640:  32%|███▏      | 6/19 [00:01<00:03,  4.31it/s]12/99 7.3G 0.08898 0.04708 0.02989 93 640:  37%|███▋      | 7/19 [00:01<00:02,  4.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09016 0.04877 0.02981 124 640:  37%|███▋      | 7/19 [00:02<00:02,  4.69it/s]12/99 7.3G 0.09016 0.04877 0.02981 124 640:  42%|████▏     | 8/19 [00:02<00:02,  4.98it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08912 0.04679 0.03012 41 640:  42%|████▏     | 8/19 [00:02<00:02,  4.98it/s] 12/99 7.3G 0.08912 0.04679 0.03012 41 640:  47%|████▋     | 9/19 [00:02<00:01,  5.18it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08915 0.04785 0.02989 93 640:  47%|████▋     | 9/19 [00:02<00:01,  5.18it/s]12/99 7.3G 0.08915 0.04785 0.02989 93 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.33it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.0896 0.05073 0.02984 159 640:  53%|█████▎    | 10/19 [00:02<00:01,  5.33it/s]12/99 7.3G 0.0896 0.05073 0.02984 159 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.45it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09012 0.05111 0.02977 105 640:  58%|█████▊    | 11/19 [00:02<00:01,  5.45it/s]12/99 7.3G 0.09012 0.05111 0.02977 105 640:  63%|██████▎   | 12/19 [00:02<00:01,  5.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08998 0.05118 0.02982 81 640:  63%|██████▎   | 12/19 [00:09<00:01,  5.53it/s] 12/99 7.3G 0.08998 0.05118 0.02982 81 640:  68%|██████▊   | 13/19 [00:09<00:12,  2.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08972 0.0503 0.02949 61 640:  68%|██████▊   | 13/19 [00:10<00:12,  2.05s/it] 12/99 7.3G 0.08972 0.0503 0.02949 61 640:  74%|███████▎  | 14/19 [00:10<00:08,  1.76s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09029 0.05141 0.02929 164 640:  74%|███████▎  | 14/19 [00:11<00:08,  1.76s/it]12/99 7.3G 0.09029 0.05141 0.02929 164 640:  79%|███████▉  | 15/19 [00:11<00:06,  1.62s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.09018 0.05092 0.02926 68 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.62s/it] 12/99 7.3G 0.09018 0.05092 0.02926 68 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.43s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.0899 0.05042 0.02939 66 640:  84%|████████▍ | 16/19 [00:14<00:04,  1.43s/it] 12/99 7.3G 0.0899 0.05042 0.02939 66 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08948 0.04978 0.02945 54 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.52s/it]12/99 7.3G 0.08948 0.04978 0.02945 54 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.12s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
12/99 7.3G 0.08952 0.05018 0.02938 94 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.12s/it]12/99 7.3G 0.08952 0.05018 0.02938 94 640: 100%|██████████| 19/19 [00:15<00:00,  1.18s/it]12/99 7.3G 0.08952 0.05018 0.02938 94 640: 100%|██████████| 19/19 [00:15<00:00,  1.21it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.90s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.50s/it]
                   all         55        256    0.00741      0.445    0.00729    0.00154
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.09038 0.04504 0.03301 76 640:   0%|          | 0/19 [00:00<?, ?it/s]13/99 7.3G 0.09038 0.04504 0.03301 76 640:   5%|▌         | 1/19 [00:00<00:03,  5.56it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.09139 0.05353 0.03075 113 640:   5%|▌         | 1/19 [00:00<00:03,  5.56it/s]13/99 7.3G 0.09139 0.05353 0.03075 113 640:  11%|█         | 2/19 [00:00<00:03,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.09218 0.05487 0.03044 111 640:  11%|█         | 2/19 [00:00<00:03,  5.60it/s]13/99 7.3G 0.09218 0.05487 0.03044 111 640:  16%|█▌        | 3/19 [00:00<00:02,  5.36it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.09064 0.04946 0.0297 51 640:  16%|█▌        | 3/19 [00:00<00:02,  5.36it/s]  13/99 7.3G 0.09064 0.04946 0.0297 51 640:  21%|██        | 4/19 [00:00<00:02,  5.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.0902 0.04893 0.02986 74 640:  21%|██        | 4/19 [00:00<00:02,  5.26it/s]13/99 7.3G 0.0902 0.04893 0.02986 74 640:  26%|██▋       | 5/19 [00:00<00:02,  5.37it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08923 0.04725 0.02941 56 640:  26%|██▋       | 5/19 [00:01<00:02,  5.37it/s]13/99 7.3G 0.08923 0.04725 0.02941 56 640:  32%|███▏      | 6/19 [00:01<00:02,  5.27it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08983 0.04673 0.02978 75 640:  32%|███▏      | 6/19 [00:01<00:02,  5.27it/s]13/99 7.3G 0.08983 0.04673 0.02978 75 640:  37%|███▋      | 7/19 [00:01<00:02,  5.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08938 0.04502 0.03041 47 640:  37%|███▋      | 7/19 [00:01<00:02,  5.26it/s]13/99 7.3G 0.08938 0.04502 0.03041 47 640:  42%|████▏     | 8/19 [00:01<00:02,  5.35it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08945 0.04359 0.0301 53 640:  42%|████▏     | 8/19 [00:01<00:02,  5.35it/s] 13/99 7.3G 0.08945 0.04359 0.0301 53 640:  47%|████▋     | 9/19 [00:01<00:01,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08892 0.04249 0.02979 48 640:  47%|████▋     | 9/19 [00:04<00:01,  5.02it/s]13/99 7.3G 0.08892 0.04249 0.02979 48 640:  53%|█████▎    | 10/19 [00:04<00:09,  1.05s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08935 0.04471 0.0296 121 640:  53%|█████▎    | 10/19 [00:06<00:09,  1.05s/it]13/99 7.3G 0.08935 0.04471 0.0296 121 640:  58%|█████▊    | 11/19 [00:06<00:10,  1.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08951 0.04462 0.02944 81 640:  58%|█████▊    | 11/19 [00:07<00:10,  1.29s/it]13/99 7.3G 0.08951 0.04462 0.02944 81 640:  63%|██████▎   | 12/19 [00:07<00:08,  1.18s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08938 0.0438 0.02955 52 640:  63%|██████▎   | 12/19 [00:10<00:08,  1.18s/it] 13/99 7.3G 0.08938 0.0438 0.02955 52 640:  68%|██████▊   | 13/19 [00:10<00:09,  1.66s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08902 0.04365 0.02959 64 640:  68%|██████▊   | 13/19 [00:10<00:09,  1.66s/it]13/99 7.3G 0.08902 0.04365 0.02959 64 640:  74%|███████▎  | 14/19 [00:10<00:06,  1.35s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.0893 0.04436 0.02962 100 640:  74%|███████▎  | 14/19 [00:11<00:06,  1.35s/it]13/99 7.3G 0.0893 0.04436 0.02962 100 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.02s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08943 0.0448 0.02958 83 640:  79%|███████▉  | 15/19 [00:11<00:04,  1.02s/it] 13/99 7.3G 0.08943 0.0448 0.02958 83 640:  84%|████████▍ | 16/19 [00:11<00:02,  1.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08894 0.04461 0.02962 58 640:  84%|████████▍ | 16/19 [00:14<00:02,  1.02it/s]13/99 7.3G 0.08894 0.04461 0.02962 58 640:  89%|████████▉ | 17/19 [00:14<00:02,  1.44s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08909 0.04553 0.02961 108 640:  89%|████████▉ | 17/19 [00:18<00:02,  1.44s/it]13/99 7.3G 0.08909 0.04553 0.02961 108 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.28s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
13/99 7.3G 0.08894 0.04603 0.02962 83 640:  95%|█████████▍| 18/19 [00:19<00:02,  2.28s/it] 13/99 7.3G 0.08894 0.04603 0.02962 83 640: 100%|██████████| 19/19 [00:19<00:00,  1.84s/it]13/99 7.3G 0.08894 0.04603 0.02962 83 640: 100%|██████████| 19/19 [00:19<00:00,  1.03s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.86s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.93s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
                   all         55        256    0.00763      0.403    0.00891    0.00207
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08583 0.04344 0.02874 68 640:   0%|          | 0/19 [00:00<?, ?it/s]14/99 7.3G 0.08583 0.04344 0.02874 68 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08435 0.04591 0.02823 75 640:   5%|▌         | 1/19 [00:00<00:03,  5.69it/s]14/99 7.3G 0.08435 0.04591 0.02823 75 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08376 0.04719 0.02854 71 640:  11%|█         | 2/19 [00:00<00:02,  5.72it/s]14/99 7.3G 0.08376 0.04719 0.02854 71 640:  16%|█▌        | 3/19 [00:00<00:02,  5.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08456 0.04556 0.02892 64 640:  16%|█▌        | 3/19 [00:00<00:02,  5.74it/s]14/99 7.3G 0.08456 0.04556 0.02892 64 640:  21%|██        | 4/19 [00:00<00:02,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08456 0.0443 0.02873 60 640:  21%|██        | 4/19 [00:00<00:02,  5.65it/s] 14/99 7.3G 0.08456 0.0443 0.02873 60 640:  26%|██▋       | 5/19 [00:00<00:02,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08525 0.04427 0.0285 75 640:  26%|██▋       | 5/19 [00:01<00:02,  5.65it/s]14/99 7.3G 0.08525 0.04427 0.0285 75 640:  32%|███▏      | 6/19 [00:01<00:02,  5.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08605 0.04444 0.02894 72 640:  32%|███▏      | 6/19 [00:01<00:02,  5.60it/s]14/99 7.3G 0.08605 0.04444 0.02894 72 640:  37%|███▋      | 7/19 [00:01<00:02,  5.65it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08664 0.04493 0.02928 81 640:  37%|███▋      | 7/19 [00:01<00:02,  5.65it/s]14/99 7.3G 0.08664 0.04493 0.02928 81 640:  42%|████▏     | 8/19 [00:01<00:02,  4.94it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08733 0.04401 0.0291 67 640:  42%|████▏     | 8/19 [00:01<00:02,  4.94it/s] 14/99 7.3G 0.08733 0.04401 0.0291 67 640:  47%|████▋     | 9/19 [00:01<00:02,  4.26it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08737 0.04386 0.02939 64 640:  47%|████▋     | 9/19 [00:02<00:02,  4.26it/s]14/99 7.3G 0.08737 0.04386 0.02939 64 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.87it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08748 0.04488 0.02944 93 640:  53%|█████▎    | 10/19 [00:02<00:02,  3.87it/s]14/99 7.3G 0.08748 0.04488 0.02944 93 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08782 0.04529 0.02947 89 640:  58%|█████▊    | 11/19 [00:02<00:02,  3.74it/s]14/99 7.3G 0.08782 0.04529 0.02947 89 640:  63%|██████▎   | 12/19 [00:02<00:01,  3.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08819 0.04492 0.02946 72 640:  63%|██████▎   | 12/19 [00:02<00:01,  3.78it/s]14/99 7.3G 0.08819 0.04492 0.02946 72 640:  68%|██████▊   | 13/19 [00:02<00:01,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08861 0.04648 0.02946 117 640:  68%|██████▊   | 13/19 [00:07<00:01,  4.22it/s]14/99 7.3G 0.08861 0.04648 0.02946 117 640:  74%|███████▎  | 14/19 [00:07<00:07,  1.48s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08828 0.04599 0.02948 60 640:  74%|███████▎  | 14/19 [00:11<00:07,  1.48s/it] 14/99 7.3G 0.08828 0.04599 0.02948 60 640:  79%|███████▉  | 15/19 [00:11<00:09,  2.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08819 0.04609 0.02944 73 640:  79%|███████▉  | 15/19 [00:12<00:09,  2.25s/it]14/99 7.3G 0.08819 0.04609 0.02944 73 640:  84%|████████▍ | 16/19 [00:12<00:05,  1.89s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08774 0.04533 0.02944 47 640:  84%|████████▍ | 16/19 [00:14<00:05,  1.89s/it]14/99 7.3G 0.08774 0.04533 0.02944 47 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.85s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08775 0.045 0.0293 65 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.85s/it]   14/99 7.3G 0.08775 0.045 0.0293 65 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.60s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
14/99 7.3G 0.08754 0.04424 0.02955 43 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.60s/it]14/99 7.3G 0.08754 0.04424 0.02955 43 640: 100%|██████████| 19/19 [00:15<00:00,  1.40s/it]14/99 7.3G 0.08754 0.04424 0.02955 43 640: 100%|██████████| 19/19 [00:15<00:00,  1.19it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:07<00:07,  7.07s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  2.98s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
                   all         55        256    0.00652      0.439    0.00959    0.00268
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08828 0.05092 0.02819 82 640:   0%|          | 0/19 [00:00<?, ?it/s]15/99 7.3G 0.08828 0.05092 0.02819 82 640:   5%|▌         | 1/19 [00:00<00:03,  5.02it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08637 0.04734 0.02729 67 640:   5%|▌         | 1/19 [00:00<00:03,  5.02it/s]15/99 7.3G 0.08637 0.04734 0.02729 67 640:  11%|█         | 2/19 [00:00<00:03,  5.40it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08702 0.05069 0.02791 90 640:  11%|█         | 2/19 [00:00<00:03,  5.40it/s]15/99 7.3G 0.08702 0.05069 0.02791 90 640:  16%|█▌        | 3/19 [00:00<00:02,  5.55it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08797 0.05314 0.02839 105 640:  16%|█▌        | 3/19 [00:00<00:02,  5.55it/s]15/99 7.3G 0.08797 0.05314 0.02839 105 640:  21%|██        | 4/19 [00:00<00:02,  5.62it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08897 0.05181 0.02808 87 640:  21%|██        | 4/19 [00:01<00:02,  5.62it/s] 15/99 7.3G 0.08897 0.05181 0.02808 87 640:  26%|██▋       | 5/19 [00:01<00:03,  4.15it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08823 0.04893 0.02785 55 640:  26%|██▋       | 5/19 [00:01<00:03,  4.15it/s]15/99 7.3G 0.08823 0.04893 0.02785 55 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08759 0.04695 0.02834 58 640:  32%|███▏      | 6/19 [00:01<00:02,  4.57it/s]15/99 7.3G 0.08759 0.04695 0.02834 58 640:  37%|███▋      | 7/19 [00:01<00:02,  4.90it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08675 0.04562 0.02871 51 640:  37%|███▋      | 7/19 [00:01<00:02,  4.90it/s]15/99 7.3G 0.08675 0.04562 0.02871 51 640:  42%|████▏     | 8/19 [00:01<00:02,  5.14it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08787 0.04633 0.02847 118 640:  42%|████▏     | 8/19 [00:01<00:02,  5.14it/s]15/99 7.3G 0.08787 0.04633 0.02847 118 640:  47%|████▋     | 9/19 [00:01<00:01,  5.32it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08772 0.04584 0.02854 64 640:  47%|████▋     | 9/19 [00:01<00:01,  5.32it/s] 15/99 7.3G 0.08772 0.04584 0.02854 64 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.44it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08791 0.04786 0.02858 113 640:  53%|█████▎    | 10/19 [00:03<00:01,  5.44it/s]15/99 7.3G 0.08791 0.04786 0.02858 113 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.78it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08743 0.04789 0.02892 70 640:  58%|█████▊    | 11/19 [00:09<00:04,  1.78it/s] 15/99 7.3G 0.08743 0.04789 0.02892 70 640:  63%|██████▎   | 12/19 [00:09<00:16,  2.29s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08818 0.04927 0.02877 139 640:  63%|██████▎   | 12/19 [00:10<00:16,  2.29s/it]15/99 7.3G 0.08818 0.04927 0.02877 139 640:  68%|██████▊   | 13/19 [00:10<00:10,  1.73s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08847 0.04908 0.02886 83 640:  68%|██████▊   | 13/19 [00:10<00:10,  1.73s/it] 15/99 7.3G 0.08847 0.04908 0.02886 83 640:  74%|███████▎  | 14/19 [00:10<00:06,  1.27s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08845 0.04926 0.0289 85 640:  74%|███████▎  | 14/19 [00:12<00:06,  1.27s/it] 15/99 7.3G 0.08845 0.04926 0.0289 85 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08811 0.04794 0.02884 39 640:  79%|███████▉  | 15/19 [00:12<00:06,  1.51s/it]15/99 7.3G 0.08811 0.04794 0.02884 39 640:  84%|████████▍ | 16/19 [00:12<00:03,  1.17s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08782 0.0481 0.02896 75 640:  84%|████████▍ | 16/19 [00:15<00:03,  1.17s/it] 15/99 7.3G 0.08782 0.0481 0.02896 75 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.55s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08762 0.0476 0.02898 58 640:  89%|████████▉ | 17/19 [00:15<00:03,  1.55s/it]15/99 7.3G 0.08762 0.0476 0.02898 58 640:  95%|█████████▍| 18/19 [00:15<00:01,  1.16s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
15/99 7.3G 0.08743 0.04622 0.02879 31 640:  95%|█████████▍| 18/19 [00:17<00:01,  1.16s/it]15/99 7.3G 0.08743 0.04622 0.02879 31 640: 100%|██████████| 19/19 [00:17<00:00,  1.32s/it]15/99 7.3G 0.08743 0.04622 0.02879 31 640: 100%|██████████| 19/19 [00:17<00:00,  1.11it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.47s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
                   all         55        256    0.00781      0.479      0.015    0.00394
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08543 0.0295 0.03048 42 640:   0%|          | 0/19 [00:00<?, ?it/s]16/99 7.3G 0.08543 0.0295 0.03048 42 640:   5%|▌         | 1/19 [00:00<00:03,  5.42it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.09003 0.04654 0.02958 125 640:   5%|▌         | 1/19 [00:00<00:03,  5.42it/s]16/99 7.3G 0.09003 0.04654 0.02958 125 640:  11%|█         | 2/19 [00:00<00:04,  4.22it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.09098 0.04672 0.02907 90 640:  11%|█         | 2/19 [00:00<00:04,  4.22it/s] 16/99 7.3G 0.09098 0.04672 0.02907 90 640:  16%|█▌        | 3/19 [00:00<00:05,  2.83it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.09076 0.04954 0.02891 103 640:  16%|█▌        | 3/19 [00:01<00:05,  2.83it/s]16/99 7.3G 0.09076 0.04954 0.02891 103 640:  21%|██        | 4/19 [00:01<00:06,  2.47it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08956 0.05243 0.02831 101 640:  21%|██        | 4/19 [00:01<00:06,  2.47it/s]16/99 7.3G 0.08956 0.05243 0.02831 101 640:  26%|██▋       | 5/19 [00:01<00:05,  2.73it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08853 0.05066 0.02912 58 640:  26%|██▋       | 5/19 [00:02<00:05,  2.73it/s] 16/99 7.3G 0.08853 0.05066 0.02912 58 640:  32%|███▏      | 6/19 [00:02<00:04,  2.60it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08926 0.05249 0.02902 117 640:  32%|███▏      | 6/19 [00:02<00:04,  2.60it/s]16/99 7.3G 0.08926 0.05249 0.02902 117 640:  37%|███▋      | 7/19 [00:02<00:04,  2.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08872 0.04975 0.0292 44 640:  37%|███▋      | 7/19 [00:02<00:04,  2.69it/s]  16/99 7.3G 0.08872 0.04975 0.0292 44 640:  42%|████▏     | 8/19 [00:02<00:03,  3.23it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08883 0.04878 0.02903 67 640:  42%|████▏     | 8/19 [00:02<00:03,  3.23it/s]16/99 7.3G 0.08883 0.04878 0.02903 67 640:  47%|████▋     | 9/19 [00:02<00:02,  3.74it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08858 0.04683 0.02894 46 640:  47%|████▋     | 9/19 [00:03<00:02,  3.74it/s]16/99 7.3G 0.08858 0.04683 0.02894 46 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.20it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08864 0.04952 0.02879 124 640:  53%|█████▎    | 10/19 [00:03<00:02,  4.20it/s]16/99 7.3G 0.08864 0.04952 0.02879 124 640:  58%|█████▊    | 11/19 [00:03<00:02,  2.96it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08882 0.04927 0.02851 85 640:  58%|█████▊    | 11/19 [00:05<00:02,  2.96it/s] 16/99 7.3G 0.08882 0.04927 0.02851 85 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.21it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08899 0.04882 0.02839 76 640:  63%|██████▎   | 12/19 [00:05<00:05,  1.21it/s]16/99 7.3G 0.08899 0.04882 0.02839 76 640:  68%|██████▊   | 13/19 [00:05<00:03,  1.53it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08915 0.04772 0.02843 59 640:  68%|██████▊   | 13/19 [00:10<00:03,  1.53it/s]16/99 7.3G 0.08915 0.04772 0.02843 59 640:  74%|███████▎  | 14/19 [00:10<00:09,  1.87s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08869 0.0468 0.02874 52 640:  74%|███████▎  | 14/19 [00:10<00:09,  1.87s/it] 16/99 7.3G 0.08869 0.0468 0.02874 52 640:  79%|███████▉  | 15/19 [00:10<00:05,  1.38s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08855 0.04681 0.0285 85 640:  79%|███████▉  | 15/19 [00:12<00:05,  1.38s/it]16/99 7.3G 0.08855 0.04681 0.0285 85 640:  84%|████████▍ | 16/19 [00:12<00:04,  1.51s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08856 0.04685 0.02866 78 640:  84%|████████▍ | 16/19 [00:18<00:04,  1.51s/it]16/99 7.3G 0.08856 0.04685 0.02866 78 640:  89%|████████▉ | 17/19 [00:18<00:05,  2.79s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08881 0.04735 0.02854 104 640:  89%|████████▉ | 17/19 [00:18<00:05,  2.79s/it]16/99 7.3G 0.08881 0.04735 0.02854 104 640:  95%|█████████▍| 18/19 [00:18<00:02,  2.10s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
16/99 7.3G 0.08914 0.04741 0.02845 93 640:  95%|█████████▍| 18/19 [00:19<00:02,  2.10s/it] 16/99 7.3G 0.08914 0.04741 0.02845 93 640: 100%|██████████| 19/19 [00:19<00:00,  1.61s/it]16/99 7.3G 0.08914 0.04741 0.02845 93 640: 100%|██████████| 19/19 [00:19<00:00,  1.01s/it]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:06<00:06,  6.80s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  2.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]
                   all         55        256    0.00784      0.472     0.0143    0.00329
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09018 0.04004 0.03042 71 640:   0%|          | 0/19 [00:00<?, ?it/s]17/99 7.3G 0.09018 0.04004 0.03042 71 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.08857 0.03614 0.0277 49 640:   5%|▌         | 1/19 [00:00<00:03,  5.64it/s] 17/99 7.3G 0.08857 0.03614 0.0277 49 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09061 0.04446 0.02684 132 640:  11%|█         | 2/19 [00:00<00:03,  5.66it/s]17/99 7.3G 0.09061 0.04446 0.02684 132 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09084 0.0488 0.02816 102 640:  16%|█▌        | 3/19 [00:00<00:02,  5.70it/s] 17/99 7.3G 0.09084 0.0488 0.02816 102 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09108 0.0485 0.02856 85 640:  21%|██        | 4/19 [00:00<00:02,  5.71it/s] 17/99 7.3G 0.09108 0.0485 0.02856 85 640:  26%|██▋       | 5/19 [00:00<00:02,  5.72it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09149 0.04892 0.02865 93 640:  26%|██▋       | 5/19 [00:01<00:02,  5.72it/s]17/99 7.3G 0.09149 0.04892 0.02865 93 640:  32%|███▏      | 6/19 [00:01<00:02,  5.67it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.092 0.04814 0.0285 80 640:  32%|███▏      | 6/19 [00:01<00:02,  5.67it/s]   17/99 7.3G 0.092 0.04814 0.0285 80 640:  37%|███▋      | 7/19 [00:01<00:02,  5.69it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09094 0.04803 0.02841 74 640:  37%|███▋      | 7/19 [00:01<00:02,  5.69it/s]17/99 7.3G 0.09094 0.04803 0.02841 74 640:  42%|████▏     | 8/19 [00:01<00:01,  5.70it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09048 0.0472 0.02843 62 640:  42%|████▏     | 8/19 [00:01<00:01,  5.70it/s] 17/99 7.3G 0.09048 0.0472 0.02843 62 640:  47%|████▋     | 9/19 [00:01<00:01,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09155 0.04786 0.0283 137 640:  47%|████▋     | 9/19 [00:01<00:01,  5.71it/s]17/99 7.3G 0.09155 0.04786 0.0283 137 640:  53%|█████▎    | 10/19 [00:01<00:01,  5.71it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09095 0.04617 0.02869 45 640:  53%|█████▎    | 10/19 [00:03<00:01,  5.71it/s]17/99 7.3G 0.09095 0.04617 0.02869 45 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09088 0.04618 0.02868 79 640:  58%|█████▊    | 11/19 [00:03<00:04,  1.61it/s]17/99 7.3G 0.09088 0.04618 0.02868 79 640:  63%|██████▎   | 12/19 [00:03<00:03,  2.05it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09078 0.04575 0.02862 70 640:  63%|██████▎   | 12/19 [00:05<00:03,  2.05it/s]17/99 7.3G 0.09078 0.04575 0.02862 70 640:  68%|██████▊   | 13/19 [00:05<00:06,  1.07s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09055 0.04544 0.02843 69 640:  68%|██████▊   | 13/19 [00:11<00:06,  1.07s/it]17/99 7.3G 0.09055 0.04544 0.02843 69 640:  74%|███████▎  | 14/19 [00:11<00:11,  2.37s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09118 0.04664 0.0284 142 640:  74%|███████▎  | 14/19 [00:11<00:11,  2.37s/it]17/99 7.3G 0.09118 0.04664 0.0284 142 640:  79%|███████▉  | 15/19 [00:11<00:06,  1.71s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09123 0.04658 0.02839 82 640:  79%|███████▉  | 15/19 [00:11<00:06,  1.71s/it]17/99 7.3G 0.09123 0.04658 0.02839 82 640:  84%|████████▍ | 16/19 [00:11<00:03,  1.25s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09072 0.04608 0.02833 58 640:  84%|████████▍ | 16/19 [00:13<00:03,  1.25s/it]17/99 7.3G 0.09072 0.04608 0.02833 58 640:  89%|████████▉ | 17/19 [00:13<00:03,  1.52s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09085 0.04694 0.02833 115 640:  89%|████████▉ | 17/19 [00:14<00:03,  1.52s/it]17/99 7.3G 0.09085 0.04694 0.02833 115 640:  95%|█████████▍| 18/19 [00:14<00:01,  1.19s/it]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
17/99 7.3G 0.09053 0.04667 0.02847 65 640:  95%|█████████▍| 18/19 [00:16<00:01,  1.19s/it] 17/99 7.3G 0.09053 0.04667 0.02847 65 640: 100%|██████████| 19/19 [00:16<00:00,  1.48s/it]17/99 7.3G 0.09053 0.04667 0.02847 65 640: 100%|██████████| 19/19 [00:16<00:00,  1.16it/s]
[34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s][34m[1mval: [0mScanning /mnt/bst/hxu10/hxu10/chanti/dataset/labels/val.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.33s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  3.87s/it]                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]
                   all         55        256    0.00769      0.499     0.0224    0.00589
  0%|          | 0/19 [00:00<?, ?it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.3G 0.08695 0.03165 0.02857 50 640:   0%|          | 0/19 [00:00<?, ?it/s]18/99 7.3G 0.08695 0.03165 0.02857 50 640:   5%|▌         | 1/19 [00:00<00:03,  5.61it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
18/99 7.3G 0.08893 0.03017 0.02858 48 640:   5%|▌         | 1/19 [00:00<00:03,  5.61it/s]18/99 7.3G 0.08893 0.03017 0.02858 48 640:  11%|█         | 2/19 [00:00<00:03,  4.89it/s]/mnt/bst/hxu10/hxu10/chanti/yolov5/train_dp.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=amp and not opt.dp):
/mnt/bst/hxu10/hxu10/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_ba